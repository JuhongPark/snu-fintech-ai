{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q35P2ummXim"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/JuhongPark/snu-fintech-ai/blob/main/Lending_Club_SSAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WyoEfeVoyh-"
   },
   "source": [
    "# 데이터 가져오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brnach 지정\n",
    "branch = 'develop'\n",
    "\n",
    "# 라이브러리 설치 (2분 가량 소요됨)\n",
    "#!pip install shap -q\n",
    "#!pip install pytorch_tabnet -q\n",
    "\n",
    "# 필요 파일 다운로드\n",
    "#!wget https://raw.githubusercontent.com/JuhongPark/snu-fintech-ai/{branch}/LC_Data_Cleaned_0829.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # seed 값 설정\n",
    "is_test = True  # 테스트런 설정\n",
    "\n",
    "# 입력 변수 파라미터\n",
    "target = 'loan_status_encoded'\n",
    "drop_list = ['id', 'int_rate', 'installment', 'sub_grade', 'grade', 'tbond_int', 'year', 'term', 'loan_status', 'zip_code']\n",
    "feature_list = ['loan_amnt', 'emp_length', 'revol_util', 'pub_rec', 'fico_range_high', 'fico_range_low', 'percent_bc_gt_75', 'annual_inc',\n",
    "                'dti', 'delinq_2yrs', 'open_acc', 'revol_bal', 'total_acc', 'inq_last_6mths'] # 'fico_range_low'제외\n",
    "cat_list = ['purpose', 'addr_state', 'initial_list_status', 'home_ownership']\n",
    "\n",
    "# 대출 이후 변수\n",
    "post_list = ['Funded_amnt', 'funded_amnt_inv', 'collection_recovery_fee', 'collections_12_mths_ex_med', 'last_credit_pull_d', 'last_pymnt_amnt', 'last_pymnt_d',\n",
    " 'mths_since_last_major_derog', 'next_pymnt_d', 'out_prncp', 'out_prncp_inv', 'recoveries', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee',\n",
    " 'total_rec_prncp', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date',\n",
    " 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount',\n",
    " 'hardship_last_payment_amount', 'debt_settlement_flag', 'last_fico_range_high', 'last_fico_range_low']\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "# For model\n",
    "learning_rate = 0.01\n",
    "\n",
    "# For training\n",
    "#n_epochs = 10000\n",
    "\n",
    "# For CV\n",
    "cv = 10\n",
    "\n",
    "# For SSAE\n",
    "n_epochs_ssae = 10000\n",
    "latent_size = 8\n",
    "\n",
    "# 테스트 런일 경우, 에포크 수를 줄임\n",
    "if is_test:\n",
    "    n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATZ3E7NEKApJ",
    "outputId": "653a45fc-17ab-47e4-b4ae-5360f61fcec2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed 설정\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed) # 파이썬 표준 라이브러리\n",
    "    np.random.seed(seed) # numpy의 random 모듈에서 사용하는 seed\n",
    "    torch.manual_seed(seed) # pytorch에서 사용하는 seed\n",
    "    if torch.cuda.is_available(): # GPU에서 실행되는 PyTorch 연산의 무작위성을 제어\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# seed 값 설정\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ans_U0Bqoyh-"
   },
   "source": [
    "## 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mn3BhYVohcPI"
   },
   "outputs": [],
   "source": [
    "# 모든 행이 화면에 표시되도록 설정합니다.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 파일 로드\n",
    "file_path = 'LC_Data_Cleaned_0829.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 테스트 런일 경우, 데이터 크기 줄이기\n",
    "if is_test:\n",
    "    df = df.sample(frac=0.01, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0DpvFHJoyh-"
   },
   "source": [
    "## 데이터 구조 훑어 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxjdTP4gKApM"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()\n",
    "\n",
    "df_raw['term_num']=df_raw.apply(lambda row: 36 if row['term'] == 0 else 60,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "L42IMY2hKSEO",
    "outputId": "c428f136-6ace-4ed8-f5fe-b90c4949fd50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2220, 59)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 df 에 있는 칼럼 중에서, drop_list 및 post_list의 칼럼 제거\n",
    "drop_list = list(set(df.columns) & set(drop_list + post_list))\n",
    "\n",
    "# 불필요한 변수 Drop\n",
    "df = df.drop(columns = drop_list)\n",
    "\n",
    "# 결측치 처리\n",
    "df = df.fillna(0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWIX34Exoyim"
   },
   "source": [
    "# 모델 선택과 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTRHX9M5oyiA"
   },
   "source": [
    "## 테스트 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PrhJ2zzMKApN"
   },
   "outputs": [],
   "source": [
    "# Torch 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if not is_test else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obj = X_test.join(df_raw[['total_pymnt','recoveries','collection_recovery_fee','tbond_int','term_num']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train distribution: loan_status_encoded\n",
      "0    1481\n",
      "1     295\n",
      "Name: count, dtype: int64\n",
      "1776 444 1776 444\n",
      "Resampled y_train distribution: loan_status_encoded\n",
      "0    295\n",
      "1    295\n",
      "Name: count, dtype: int64\n",
      "590 444 590 444\n"
     ]
    }
   ],
   "source": [
    "# 언더샘플링 객체 생성\n",
    "print(f\"Original y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# 트레인 데이터셋에 대해 언더샘플링 수행\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Resampled y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수 선정\n",
    "num_list = list(set(X_train.columns) ^ set(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder 생성 및 학습 데이터에 적합\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oneHotEncoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "oneHotEncoder.fit(X_train[cat_list])\n",
    "\n",
    "# 학습 데이터에 인코딩 적용\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_train[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# 테스트 데이터에 인코딩 적용\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_test[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 원래 데이터프레임과 병합\n",
    "X_train = X_train.drop(cat_list, axis=1).join(X_train_encoded)\n",
    "X_test = X_test.drop(cat_list, axis=1).join(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 수치형 변수 찾기\n",
    "X_train[num_list] = scaler.fit_transform(X_train[num_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test[num_list] = scaler.transform(X_test[num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 113)\n",
      "(590,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvlhFSTgKApN"
   },
   "source": [
    "## SSAE 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GhT0RT1sKApN"
   },
   "outputs": [],
   "source": [
    "# SSAE 모델 정의\n",
    "class DenoisingSSAE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DenoisingSSAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # input_dim이 실제 데이터의 feature 수와 일치해야 함\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)  # output_dim도 input_dim과 일치해야 함\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "# Noise 추가 함수\n",
    "def add_noise(data, noise_factor=0.2):\n",
    "    noise = noise_factor * np.random.randn(*data.shape)\n",
    "    noisy_data = data + noise\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.)\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Encoding 목표 설정\n",
    "encoding_target = list(set(X_train.columns) ^ set(feature_list))\n",
    "\n",
    "# 노이즈가 추가\n",
    "X_train_noisy_np = add_noise(X_train[encoding_target]).to_numpy()\n",
    "X_train_np = X_train[encoding_target].to_numpy()\n",
    "          \n",
    "input_dim = X_train_np.shape[1]  # X_train의 feature 수\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 0.4500\n",
      "Epoch [10/10000], Loss: 0.3828\n",
      "Epoch [20/10000], Loss: 0.3329\n",
      "Epoch [30/10000], Loss: 0.3133\n",
      "Epoch [40/10000], Loss: 0.2886\n",
      "Epoch [50/10000], Loss: 0.2734\n",
      "Epoch [60/10000], Loss: 0.2585\n",
      "Epoch [70/10000], Loss: 0.2448\n",
      "Epoch [80/10000], Loss: 0.2331\n",
      "Epoch [90/10000], Loss: 0.2246\n",
      "Epoch [100/10000], Loss: 0.2188\n",
      "Epoch [110/10000], Loss: 0.2130\n",
      "Epoch [120/10000], Loss: 0.2064\n",
      "Epoch [130/10000], Loss: 0.2030\n",
      "Epoch [140/10000], Loss: 0.1925\n",
      "Epoch [150/10000], Loss: 0.1866\n",
      "Epoch [160/10000], Loss: 0.1808\n",
      "Epoch [170/10000], Loss: 0.1758\n",
      "Epoch [180/10000], Loss: 0.1732\n",
      "Epoch [190/10000], Loss: 0.1706\n",
      "Epoch [200/10000], Loss: 0.1692\n",
      "Epoch [210/10000], Loss: 0.1644\n",
      "Epoch [220/10000], Loss: 0.1658\n",
      "Epoch [230/10000], Loss: 0.1596\n",
      "Epoch [240/10000], Loss: 0.1586\n",
      "Epoch [250/10000], Loss: 0.1580\n",
      "Epoch [260/10000], Loss: 0.1564\n",
      "Epoch [270/10000], Loss: 0.1545\n",
      "Epoch [280/10000], Loss: 0.1519\n",
      "Epoch [290/10000], Loss: 0.1532\n",
      "Epoch [300/10000], Loss: 0.1520\n",
      "Epoch [310/10000], Loss: 0.1478\n",
      "Epoch [320/10000], Loss: 0.1471\n",
      "Epoch [330/10000], Loss: 0.1445\n",
      "Epoch [340/10000], Loss: 0.1438\n",
      "Epoch [350/10000], Loss: 0.1426\n",
      "Epoch [360/10000], Loss: 0.1435\n",
      "Epoch [370/10000], Loss: 0.1399\n",
      "Epoch [380/10000], Loss: 0.1406\n",
      "Epoch [390/10000], Loss: 0.1382\n",
      "Epoch [400/10000], Loss: 0.1372\n",
      "Epoch [410/10000], Loss: 0.1363\n",
      "Epoch [420/10000], Loss: 0.1373\n",
      "Epoch [430/10000], Loss: 0.1342\n",
      "Epoch [440/10000], Loss: 0.1369\n",
      "Epoch [450/10000], Loss: 0.1328\n",
      "Epoch [460/10000], Loss: 0.1348\n",
      "Epoch [470/10000], Loss: 0.1324\n",
      "Epoch [480/10000], Loss: 0.1348\n",
      "Epoch [490/10000], Loss: 0.1336\n",
      "Epoch [500/10000], Loss: 0.1294\n",
      "Epoch [510/10000], Loss: 0.1301\n",
      "Epoch [520/10000], Loss: 0.1290\n",
      "Epoch [530/10000], Loss: 0.1312\n",
      "Epoch [540/10000], Loss: 0.1302\n",
      "Epoch [550/10000], Loss: 0.1297\n",
      "Epoch [560/10000], Loss: 0.1276\n",
      "Epoch [570/10000], Loss: 0.1291\n",
      "Epoch [580/10000], Loss: 0.1271\n",
      "Epoch [590/10000], Loss: 0.1264\n",
      "Epoch [600/10000], Loss: 0.1256\n",
      "Epoch [610/10000], Loss: 0.1259\n",
      "Epoch [620/10000], Loss: 0.1254\n",
      "Epoch [630/10000], Loss: 0.1230\n",
      "Epoch [640/10000], Loss: 0.1229\n",
      "Epoch [650/10000], Loss: 0.1229\n",
      "Epoch [660/10000], Loss: 0.1240\n",
      "Epoch [670/10000], Loss: 0.1237\n",
      "Epoch [680/10000], Loss: 0.1244\n",
      "Epoch [690/10000], Loss: 0.1218\n",
      "Epoch [700/10000], Loss: 0.1216\n",
      "Epoch [710/10000], Loss: 0.1239\n",
      "Epoch [720/10000], Loss: 0.1228\n",
      "Epoch [730/10000], Loss: 0.1223\n",
      "Epoch [740/10000], Loss: 0.1208\n",
      "Epoch [750/10000], Loss: 0.1212\n",
      "Epoch [760/10000], Loss: 0.1220\n",
      "Epoch [770/10000], Loss: 0.1218\n",
      "Epoch [780/10000], Loss: 0.1222\n",
      "Epoch [790/10000], Loss: 0.1198\n",
      "Epoch [800/10000], Loss: 0.1185\n",
      "Epoch [810/10000], Loss: 0.1208\n",
      "Epoch [820/10000], Loss: 0.1192\n",
      "Epoch [830/10000], Loss: 0.1186\n",
      "Epoch [840/10000], Loss: 0.1217\n",
      "Epoch [850/10000], Loss: 0.1199\n",
      "Epoch [860/10000], Loss: 0.1190\n",
      "Epoch [870/10000], Loss: 0.1187\n",
      "Epoch [880/10000], Loss: 0.1191\n",
      "Epoch [890/10000], Loss: 0.1187\n",
      "Epoch [900/10000], Loss: 0.1183\n",
      "Epoch [910/10000], Loss: 0.1203\n",
      "Epoch [920/10000], Loss: 0.1171\n",
      "Epoch [930/10000], Loss: 0.1189\n",
      "Epoch [940/10000], Loss: 0.1163\n",
      "Epoch [950/10000], Loss: 0.1173\n",
      "Epoch [960/10000], Loss: 0.1186\n",
      "Epoch [970/10000], Loss: 0.1177\n",
      "Epoch [980/10000], Loss: 0.1174\n",
      "Epoch [990/10000], Loss: 0.1159\n",
      "Epoch [1000/10000], Loss: 0.1152\n",
      "Epoch [1010/10000], Loss: 0.1153\n",
      "Epoch [1020/10000], Loss: 0.1156\n",
      "Epoch [1030/10000], Loss: 0.1174\n",
      "Epoch [1040/10000], Loss: 0.1151\n",
      "Epoch [1050/10000], Loss: 0.1168\n",
      "Epoch [1060/10000], Loss: 0.1156\n",
      "Epoch [1070/10000], Loss: 0.1163\n",
      "Epoch [1080/10000], Loss: 0.1163\n",
      "Epoch [1090/10000], Loss: 0.1139\n",
      "Epoch [1100/10000], Loss: 0.1161\n",
      "Epoch [1110/10000], Loss: 0.1141\n",
      "Epoch [1120/10000], Loss: 0.1148\n",
      "Epoch [1130/10000], Loss: 0.1155\n",
      "Epoch [1140/10000], Loss: 0.1144\n",
      "Epoch [1150/10000], Loss: 0.1141\n",
      "Epoch [1160/10000], Loss: 0.1134\n",
      "Epoch [1170/10000], Loss: 0.1135\n",
      "Epoch [1180/10000], Loss: 0.1169\n",
      "Epoch [1190/10000], Loss: 0.1131\n",
      "Epoch [1200/10000], Loss: 0.1129\n",
      "Epoch [1210/10000], Loss: 0.1125\n",
      "Epoch [1220/10000], Loss: 0.1137\n",
      "Epoch [1230/10000], Loss: 0.1142\n",
      "Epoch [1240/10000], Loss: 0.1125\n",
      "Epoch [1250/10000], Loss: 0.1143\n",
      "Epoch [1260/10000], Loss: 0.1125\n",
      "Epoch [1270/10000], Loss: 0.1113\n",
      "Epoch [1280/10000], Loss: 0.1124\n",
      "Epoch [1290/10000], Loss: 0.1128\n",
      "Epoch [1300/10000], Loss: 0.1137\n",
      "Epoch [1310/10000], Loss: 0.1126\n",
      "Epoch [1320/10000], Loss: 0.1134\n",
      "Epoch [1330/10000], Loss: 0.1120\n",
      "Epoch [1340/10000], Loss: 0.1127\n",
      "Epoch [1350/10000], Loss: 0.1123\n",
      "Epoch [1360/10000], Loss: 0.1117\n",
      "Epoch [1370/10000], Loss: 0.1133\n",
      "Epoch [1380/10000], Loss: 0.1121\n",
      "Epoch [1390/10000], Loss: 0.1115\n",
      "Epoch [1400/10000], Loss: 0.1111\n",
      "Epoch [1410/10000], Loss: 0.1167\n",
      "Epoch [1420/10000], Loss: 0.1095\n",
      "Epoch [1430/10000], Loss: 0.1108\n",
      "Epoch [1440/10000], Loss: 0.1106\n",
      "Epoch [1450/10000], Loss: 0.1121\n",
      "Epoch [1460/10000], Loss: 0.1103\n",
      "Epoch [1470/10000], Loss: 0.1128\n",
      "Epoch [1480/10000], Loss: 0.1114\n",
      "Epoch [1490/10000], Loss: 0.1105\n",
      "Epoch [1500/10000], Loss: 0.1095\n",
      "Epoch [1510/10000], Loss: 0.1103\n",
      "Epoch [1520/10000], Loss: 0.1129\n",
      "Epoch [1530/10000], Loss: 0.1130\n",
      "Epoch [1540/10000], Loss: 0.1095\n",
      "Epoch [1550/10000], Loss: 0.1112\n",
      "Epoch [1560/10000], Loss: 0.1101\n",
      "Epoch [1570/10000], Loss: 0.1115\n",
      "Epoch [1580/10000], Loss: 0.1094\n",
      "Epoch [1590/10000], Loss: 0.1098\n",
      "Epoch [1600/10000], Loss: 0.1086\n",
      "Epoch [1610/10000], Loss: 0.1099\n",
      "Epoch [1620/10000], Loss: 0.1122\n",
      "Epoch [1630/10000], Loss: 0.1087\n",
      "Epoch [1640/10000], Loss: 0.1104\n",
      "Epoch [1650/10000], Loss: 0.1083\n",
      "Epoch [1660/10000], Loss: 0.1083\n",
      "Epoch [1670/10000], Loss: 0.1115\n",
      "Epoch [1680/10000], Loss: 0.1098\n",
      "Epoch [1690/10000], Loss: 0.1091\n",
      "Epoch [1700/10000], Loss: 0.1102\n",
      "Epoch [1710/10000], Loss: 0.1099\n",
      "Epoch [1720/10000], Loss: 0.1082\n",
      "Epoch [1730/10000], Loss: 0.1091\n",
      "Epoch [1740/10000], Loss: 0.1082\n",
      "Epoch [1750/10000], Loss: 0.1081\n",
      "Epoch [1760/10000], Loss: 0.1100\n",
      "Epoch [1770/10000], Loss: 0.1093\n",
      "Epoch [1780/10000], Loss: 0.1073\n",
      "Epoch [1790/10000], Loss: 0.1086\n",
      "Epoch [1800/10000], Loss: 0.1076\n",
      "Epoch [1810/10000], Loss: 0.1098\n",
      "Epoch [1820/10000], Loss: 0.1086\n",
      "Epoch [1830/10000], Loss: 0.1119\n",
      "Epoch [1840/10000], Loss: 0.1077\n",
      "Epoch [1850/10000], Loss: 0.1072\n",
      "Epoch [1860/10000], Loss: 0.1095\n",
      "Epoch [1870/10000], Loss: 0.1067\n",
      "Epoch [1880/10000], Loss: 0.1100\n",
      "Epoch [1890/10000], Loss: 0.1055\n",
      "Epoch [1900/10000], Loss: 0.1070\n",
      "Epoch [1910/10000], Loss: 0.1074\n",
      "Epoch [1920/10000], Loss: 0.1076\n",
      "Epoch [1930/10000], Loss: 0.1083\n",
      "Epoch [1940/10000], Loss: 0.1081\n",
      "Epoch [1950/10000], Loss: 0.1069\n",
      "Epoch [1960/10000], Loss: 0.1050\n",
      "Epoch [1970/10000], Loss: 0.1081\n",
      "Epoch [1980/10000], Loss: 0.1056\n",
      "Epoch [1990/10000], Loss: 0.1085\n",
      "Epoch [2000/10000], Loss: 0.1063\n",
      "Epoch [2010/10000], Loss: 0.1069\n",
      "Epoch [2020/10000], Loss: 0.1054\n",
      "Epoch [2030/10000], Loss: 0.1064\n",
      "Epoch [2040/10000], Loss: 0.1062\n",
      "Epoch [2050/10000], Loss: 0.1048\n",
      "Epoch [2060/10000], Loss: 0.1065\n",
      "Epoch [2070/10000], Loss: 0.1073\n",
      "Epoch [2080/10000], Loss: 0.1054\n",
      "Epoch [2090/10000], Loss: 0.1088\n",
      "Epoch [2100/10000], Loss: 0.1064\n",
      "Epoch [2110/10000], Loss: 0.1084\n",
      "Epoch [2120/10000], Loss: 0.1063\n",
      "Epoch [2130/10000], Loss: 0.1073\n",
      "Epoch [2140/10000], Loss: 0.1061\n",
      "Epoch [2150/10000], Loss: 0.1118\n",
      "Epoch [2160/10000], Loss: 0.1066\n",
      "Epoch [2170/10000], Loss: 0.1066\n",
      "Epoch [2180/10000], Loss: 0.1065\n",
      "Epoch [2190/10000], Loss: 0.1052\n",
      "Epoch [2200/10000], Loss: 0.1059\n",
      "Epoch [2210/10000], Loss: 0.1052\n",
      "Epoch [2220/10000], Loss: 0.1055\n",
      "Epoch [2230/10000], Loss: 0.1055\n",
      "Epoch [2240/10000], Loss: 0.1054\n",
      "Epoch [2250/10000], Loss: 0.1067\n",
      "Epoch [2260/10000], Loss: 0.1048\n",
      "Epoch [2270/10000], Loss: 0.1048\n",
      "Epoch [2280/10000], Loss: 0.1037\n",
      "Epoch [2290/10000], Loss: 0.1079\n",
      "Epoch [2300/10000], Loss: 0.1053\n",
      "Epoch [2310/10000], Loss: 0.1054\n",
      "Epoch [2320/10000], Loss: 0.1059\n",
      "Epoch [2330/10000], Loss: 0.1050\n",
      "Epoch [2340/10000], Loss: 0.1078\n",
      "Epoch [2350/10000], Loss: 0.1071\n",
      "Epoch [2360/10000], Loss: 0.1050\n",
      "Epoch [2370/10000], Loss: 0.1054\n",
      "Epoch [2380/10000], Loss: 0.1062\n",
      "Epoch [2390/10000], Loss: 0.1050\n",
      "Epoch [2400/10000], Loss: 0.1046\n",
      "Epoch [2410/10000], Loss: 0.1090\n",
      "Epoch [2420/10000], Loss: 0.1039\n",
      "Epoch [2430/10000], Loss: 0.1040\n",
      "Epoch [2440/10000], Loss: 0.1038\n",
      "Epoch [2450/10000], Loss: 0.1047\n",
      "Epoch [2460/10000], Loss: 0.1054\n",
      "Epoch [2470/10000], Loss: 0.1047\n",
      "Epoch [2480/10000], Loss: 0.1059\n",
      "Epoch [2490/10000], Loss: 0.1058\n",
      "Epoch [2500/10000], Loss: 0.1042\n",
      "Epoch [2510/10000], Loss: 0.1051\n",
      "Epoch [2520/10000], Loss: 0.1040\n",
      "Epoch [2530/10000], Loss: 0.1076\n",
      "Epoch [2540/10000], Loss: 0.1044\n",
      "Epoch [2550/10000], Loss: 0.1039\n",
      "Epoch [2560/10000], Loss: 0.1040\n",
      "Epoch [2570/10000], Loss: 0.1080\n",
      "Epoch [2580/10000], Loss: 0.1049\n",
      "Epoch [2590/10000], Loss: 0.1035\n",
      "Epoch [2600/10000], Loss: 0.1050\n",
      "Epoch [2610/10000], Loss: 0.1061\n",
      "Epoch [2620/10000], Loss: 0.1058\n",
      "Epoch [2630/10000], Loss: 0.1040\n",
      "Epoch [2640/10000], Loss: 0.1046\n",
      "Epoch [2650/10000], Loss: 0.1031\n",
      "Epoch [2660/10000], Loss: 0.1044\n",
      "Epoch [2670/10000], Loss: 0.1031\n",
      "Epoch [2680/10000], Loss: 0.1059\n",
      "Epoch [2690/10000], Loss: 0.1039\n",
      "Epoch [2700/10000], Loss: 0.1030\n",
      "Epoch [2710/10000], Loss: 0.1025\n",
      "Epoch [2720/10000], Loss: 0.1071\n",
      "Epoch [2730/10000], Loss: 0.1048\n",
      "Epoch [2740/10000], Loss: 0.1019\n",
      "Epoch [2750/10000], Loss: 0.1032\n",
      "Epoch [2760/10000], Loss: 0.1033\n",
      "Epoch [2770/10000], Loss: 0.1026\n",
      "Epoch [2780/10000], Loss: 0.1033\n",
      "Epoch [2790/10000], Loss: 0.1031\n",
      "Epoch [2800/10000], Loss: 0.1031\n",
      "Epoch [2810/10000], Loss: 0.1032\n",
      "Epoch [2820/10000], Loss: 0.1031\n",
      "Epoch [2830/10000], Loss: 0.1044\n",
      "Epoch [2840/10000], Loss: 0.1029\n",
      "Epoch [2850/10000], Loss: 0.1032\n",
      "Epoch [2860/10000], Loss: 0.1030\n",
      "Epoch [2870/10000], Loss: 0.1054\n",
      "Epoch [2880/10000], Loss: 0.1028\n",
      "Epoch [2890/10000], Loss: 0.1061\n",
      "Epoch [2900/10000], Loss: 0.1034\n",
      "Epoch [2910/10000], Loss: 0.1036\n",
      "Epoch [2920/10000], Loss: 0.1039\n",
      "Epoch [2930/10000], Loss: 0.1026\n",
      "Epoch [2940/10000], Loss: 0.1029\n",
      "Epoch [2950/10000], Loss: 0.1019\n",
      "Epoch [2960/10000], Loss: 0.1059\n",
      "Epoch [2970/10000], Loss: 0.1023\n",
      "Epoch [2980/10000], Loss: 0.1023\n",
      "Epoch [2990/10000], Loss: 0.1013\n",
      "Epoch [3000/10000], Loss: 0.1066\n",
      "Epoch [3010/10000], Loss: 0.1033\n",
      "Epoch [3020/10000], Loss: 0.1114\n",
      "Epoch [3030/10000], Loss: 0.1027\n",
      "Epoch [3040/10000], Loss: 0.1015\n",
      "Epoch [3050/10000], Loss: 0.1009\n",
      "Epoch [3060/10000], Loss: 0.1015\n",
      "Epoch [3070/10000], Loss: 0.1025\n",
      "Epoch [3080/10000], Loss: 0.1033\n",
      "Epoch [3090/10000], Loss: 0.1024\n",
      "Epoch [3100/10000], Loss: 0.1019\n",
      "Epoch [3110/10000], Loss: 0.1022\n",
      "Epoch [3120/10000], Loss: 0.1028\n",
      "Epoch [3130/10000], Loss: 0.1010\n",
      "Epoch [3140/10000], Loss: 0.1046\n",
      "Epoch [3150/10000], Loss: 0.1016\n",
      "Epoch [3160/10000], Loss: 0.1024\n",
      "Epoch [3170/10000], Loss: 0.1009\n",
      "Epoch [3180/10000], Loss: 0.1027\n",
      "Epoch [3190/10000], Loss: 0.1019\n",
      "Epoch [3200/10000], Loss: 0.1045\n",
      "Epoch [3210/10000], Loss: 0.1041\n",
      "Epoch [3220/10000], Loss: 0.1017\n",
      "Epoch [3230/10000], Loss: 0.1020\n",
      "Epoch [3240/10000], Loss: 0.1028\n",
      "Epoch [3250/10000], Loss: 0.1010\n",
      "Epoch [3260/10000], Loss: 0.1033\n",
      "Epoch [3270/10000], Loss: 0.1014\n",
      "Epoch [3280/10000], Loss: 0.1022\n",
      "Epoch [3290/10000], Loss: 0.1040\n",
      "Epoch [3300/10000], Loss: 0.1008\n",
      "Epoch [3310/10000], Loss: 0.1023\n",
      "Epoch [3320/10000], Loss: 0.1021\n",
      "Epoch [3330/10000], Loss: 0.1058\n",
      "Epoch [3340/10000], Loss: 0.1024\n",
      "Epoch [3350/10000], Loss: 0.1017\n",
      "Epoch [3360/10000], Loss: 0.1002\n",
      "Epoch [3370/10000], Loss: 0.1032\n",
      "Epoch [3380/10000], Loss: 0.1012\n",
      "Epoch [3390/10000], Loss: 0.1017\n",
      "Epoch [3400/10000], Loss: 0.1009\n",
      "Epoch [3410/10000], Loss: 0.1014\n",
      "Epoch [3420/10000], Loss: 0.1052\n",
      "Epoch [3430/10000], Loss: 0.1008\n",
      "Epoch [3440/10000], Loss: 0.1013\n",
      "Epoch [3450/10000], Loss: 0.1011\n",
      "Epoch [3460/10000], Loss: 0.1036\n",
      "Epoch [3470/10000], Loss: 0.1009\n",
      "Epoch [3480/10000], Loss: 0.1015\n",
      "Epoch [3490/10000], Loss: 0.1023\n",
      "Epoch [3500/10000], Loss: 0.1045\n",
      "Epoch [3510/10000], Loss: 0.1005\n",
      "Epoch [3520/10000], Loss: 0.1014\n",
      "Epoch [3530/10000], Loss: 0.1003\n",
      "Epoch [3540/10000], Loss: 0.1016\n",
      "Epoch [3550/10000], Loss: 0.1007\n",
      "Epoch [3560/10000], Loss: 0.1009\n",
      "Epoch [3570/10000], Loss: 0.1001\n",
      "Epoch [3580/10000], Loss: 0.1041\n",
      "Epoch [3590/10000], Loss: 0.1011\n",
      "Epoch [3600/10000], Loss: 0.1007\n",
      "Epoch [3610/10000], Loss: 0.1000\n",
      "Epoch [3620/10000], Loss: 0.1009\n",
      "Epoch [3630/10000], Loss: 0.1004\n",
      "Epoch [3640/10000], Loss: 0.1033\n",
      "Epoch [3650/10000], Loss: 0.1013\n",
      "Epoch [3660/10000], Loss: 0.1003\n",
      "Epoch [3670/10000], Loss: 0.1003\n",
      "Epoch [3680/10000], Loss: 0.1009\n",
      "Epoch [3690/10000], Loss: 0.1001\n",
      "Epoch [3700/10000], Loss: 0.1011\n",
      "Epoch [3710/10000], Loss: 0.1003\n",
      "Epoch [3720/10000], Loss: 0.1011\n",
      "Epoch [3730/10000], Loss: 0.1024\n",
      "Epoch [3740/10000], Loss: 0.1015\n",
      "Epoch [3750/10000], Loss: 0.1000\n",
      "Epoch [3760/10000], Loss: 0.0999\n",
      "Epoch [3770/10000], Loss: 0.1010\n",
      "Epoch [3780/10000], Loss: 0.1000\n",
      "Epoch [3790/10000], Loss: 0.0995\n",
      "Epoch [3800/10000], Loss: 0.1009\n",
      "Epoch [3810/10000], Loss: 0.1068\n",
      "Epoch [3820/10000], Loss: 0.1015\n",
      "Epoch [3830/10000], Loss: 0.0993\n",
      "Epoch [3840/10000], Loss: 0.1013\n",
      "Epoch [3850/10000], Loss: 0.1010\n",
      "Epoch [3860/10000], Loss: 0.0995\n",
      "Epoch [3870/10000], Loss: 0.0987\n",
      "Epoch [3880/10000], Loss: 0.1010\n",
      "Epoch [3890/10000], Loss: 0.1012\n",
      "Epoch [3900/10000], Loss: 0.1031\n",
      "Epoch [3910/10000], Loss: 0.0993\n",
      "Epoch [3920/10000], Loss: 0.1012\n",
      "Epoch [3930/10000], Loss: 0.1020\n",
      "Epoch [3940/10000], Loss: 0.1001\n",
      "Epoch [3950/10000], Loss: 0.1011\n",
      "Epoch [3960/10000], Loss: 0.1001\n",
      "Epoch [3970/10000], Loss: 0.0994\n",
      "Epoch [3980/10000], Loss: 0.1020\n",
      "Epoch [3990/10000], Loss: 0.0990\n",
      "Epoch [4000/10000], Loss: 0.1065\n",
      "Epoch [4010/10000], Loss: 0.1010\n",
      "Epoch [4020/10000], Loss: 0.0986\n",
      "Epoch [4030/10000], Loss: 0.1027\n",
      "Epoch [4040/10000], Loss: 0.0999\n",
      "Epoch [4050/10000], Loss: 0.0988\n",
      "Epoch [4060/10000], Loss: 0.1006\n",
      "Epoch [4070/10000], Loss: 0.1002\n",
      "Epoch [4080/10000], Loss: 0.1031\n",
      "Epoch [4090/10000], Loss: 0.0996\n",
      "Epoch [4100/10000], Loss: 0.0986\n",
      "Epoch [4110/10000], Loss: 0.0999\n",
      "Epoch [4120/10000], Loss: 0.1015\n",
      "Epoch [4130/10000], Loss: 0.0992\n",
      "Epoch [4140/10000], Loss: 0.0984\n",
      "Epoch [4150/10000], Loss: 0.1044\n",
      "Epoch [4160/10000], Loss: 0.1002\n",
      "Epoch [4170/10000], Loss: 0.0998\n",
      "Epoch [4180/10000], Loss: 0.0990\n",
      "Epoch [4190/10000], Loss: 0.0989\n",
      "Epoch [4200/10000], Loss: 0.1001\n",
      "Epoch [4210/10000], Loss: 0.0976\n",
      "Epoch [4220/10000], Loss: 0.1023\n",
      "Epoch [4230/10000], Loss: 0.0984\n",
      "Epoch [4240/10000], Loss: 0.0982\n",
      "Epoch [4250/10000], Loss: 0.0991\n",
      "Epoch [4260/10000], Loss: 0.1007\n",
      "Epoch [4270/10000], Loss: 0.1015\n",
      "Epoch [4280/10000], Loss: 0.1001\n",
      "Epoch [4290/10000], Loss: 0.0991\n",
      "Epoch [4300/10000], Loss: 0.0995\n",
      "Epoch [4310/10000], Loss: 0.1004\n",
      "Epoch [4320/10000], Loss: 0.1006\n",
      "Epoch [4330/10000], Loss: 0.0987\n",
      "Epoch [4340/10000], Loss: 0.1002\n",
      "Epoch [4350/10000], Loss: 0.0985\n",
      "Epoch [4360/10000], Loss: 0.1005\n",
      "Epoch [4370/10000], Loss: 0.1005\n",
      "Epoch [4380/10000], Loss: 0.0989\n",
      "Epoch [4390/10000], Loss: 0.1010\n",
      "Epoch [4400/10000], Loss: 0.0986\n",
      "Epoch [4410/10000], Loss: 0.1013\n",
      "Epoch [4420/10000], Loss: 0.0988\n",
      "Epoch [4430/10000], Loss: 0.0999\n",
      "Epoch [4440/10000], Loss: 0.0977\n",
      "Epoch [4450/10000], Loss: 0.0999\n",
      "Epoch [4460/10000], Loss: 0.0999\n",
      "Epoch [4470/10000], Loss: 0.0980\n",
      "Epoch [4480/10000], Loss: 0.0998\n",
      "Epoch [4490/10000], Loss: 0.0983\n",
      "Epoch [4500/10000], Loss: 0.0993\n",
      "Epoch [4510/10000], Loss: 0.0988\n",
      "Epoch [4520/10000], Loss: 0.1010\n",
      "Epoch [4530/10000], Loss: 0.0990\n",
      "Epoch [4540/10000], Loss: 0.0981\n",
      "Epoch [4550/10000], Loss: 0.0999\n",
      "Epoch [4560/10000], Loss: 0.0983\n",
      "Epoch [4570/10000], Loss: 0.0976\n",
      "Epoch [4580/10000], Loss: 0.0982\n",
      "Epoch [4590/10000], Loss: 0.1006\n",
      "Epoch [4600/10000], Loss: 0.0990\n",
      "Epoch [4610/10000], Loss: 0.1005\n",
      "Epoch [4620/10000], Loss: 0.0995\n",
      "Epoch [4630/10000], Loss: 0.0980\n",
      "Epoch [4640/10000], Loss: 0.0992\n",
      "Epoch [4650/10000], Loss: 0.0973\n",
      "Epoch [4660/10000], Loss: 0.0994\n",
      "Epoch [4670/10000], Loss: 0.1011\n",
      "Epoch [4680/10000], Loss: 0.0994\n",
      "Epoch [4690/10000], Loss: 0.0981\n",
      "Epoch [4700/10000], Loss: 0.0983\n",
      "Epoch [4710/10000], Loss: 0.0982\n",
      "Epoch [4720/10000], Loss: 0.0993\n",
      "Epoch [4730/10000], Loss: 0.0979\n",
      "Epoch [4740/10000], Loss: 0.0987\n",
      "Epoch [4750/10000], Loss: 0.0995\n",
      "Epoch [4760/10000], Loss: 0.0974\n",
      "Epoch [4770/10000], Loss: 0.1067\n",
      "Epoch [4780/10000], Loss: 0.1016\n",
      "Epoch [4790/10000], Loss: 0.0979\n",
      "Epoch [4800/10000], Loss: 0.0985\n",
      "Epoch [4810/10000], Loss: 0.1001\n",
      "Epoch [4820/10000], Loss: 0.0975\n",
      "Epoch [4830/10000], Loss: 0.0970\n",
      "Epoch [4840/10000], Loss: 0.1000\n",
      "Epoch [4850/10000], Loss: 0.0983\n",
      "Epoch [4860/10000], Loss: 0.0969\n",
      "Epoch [4870/10000], Loss: 0.0981\n",
      "Epoch [4880/10000], Loss: 0.1012\n",
      "Epoch [4890/10000], Loss: 0.0980\n",
      "Epoch [4900/10000], Loss: 0.0983\n",
      "Epoch [4910/10000], Loss: 0.0972\n",
      "Epoch [4920/10000], Loss: 0.0992\n",
      "Epoch [4930/10000], Loss: 0.0999\n",
      "Epoch [4940/10000], Loss: 0.0982\n",
      "Epoch [4950/10000], Loss: 0.0973\n",
      "Epoch [4960/10000], Loss: 0.0992\n",
      "Epoch [4970/10000], Loss: 0.1036\n",
      "Epoch [4980/10000], Loss: 0.0984\n",
      "Epoch [4990/10000], Loss: 0.0974\n",
      "Epoch [5000/10000], Loss: 0.0983\n",
      "Epoch [5010/10000], Loss: 0.0975\n",
      "Epoch [5020/10000], Loss: 0.0983\n",
      "Epoch [5030/10000], Loss: 0.0983\n",
      "Epoch [5040/10000], Loss: 0.0981\n",
      "Epoch [5050/10000], Loss: 0.0979\n",
      "Epoch [5060/10000], Loss: 0.1000\n",
      "Epoch [5070/10000], Loss: 0.0984\n",
      "Epoch [5080/10000], Loss: 0.0975\n",
      "Epoch [5090/10000], Loss: 0.0968\n",
      "Epoch [5100/10000], Loss: 0.0995\n",
      "Epoch [5110/10000], Loss: 0.0984\n",
      "Epoch [5120/10000], Loss: 0.0966\n",
      "Epoch [5130/10000], Loss: 0.0993\n",
      "Epoch [5140/10000], Loss: 0.0990\n",
      "Epoch [5150/10000], Loss: 0.0991\n",
      "Epoch [5160/10000], Loss: 0.0973\n",
      "Epoch [5170/10000], Loss: 0.1018\n",
      "Epoch [5180/10000], Loss: 0.0983\n",
      "Epoch [5190/10000], Loss: 0.0971\n",
      "Epoch [5200/10000], Loss: 0.0981\n",
      "Epoch [5210/10000], Loss: 0.0984\n",
      "Epoch [5220/10000], Loss: 0.0979\n",
      "Epoch [5230/10000], Loss: 0.0972\n",
      "Epoch [5240/10000], Loss: 0.0990\n",
      "Epoch [5250/10000], Loss: 0.0968\n",
      "Epoch [5260/10000], Loss: 0.0978\n",
      "Epoch [5270/10000], Loss: 0.0971\n",
      "Epoch [5280/10000], Loss: 0.0995\n",
      "Epoch [5290/10000], Loss: 0.0998\n",
      "Epoch [5300/10000], Loss: 0.0977\n",
      "Epoch [5310/10000], Loss: 0.0974\n",
      "Epoch [5320/10000], Loss: 0.0976\n",
      "Epoch [5330/10000], Loss: 0.0960\n",
      "Epoch [5340/10000], Loss: 0.0986\n",
      "Epoch [5350/10000], Loss: 0.0989\n",
      "Epoch [5360/10000], Loss: 0.0963\n",
      "Epoch [5370/10000], Loss: 0.0962\n",
      "Epoch [5380/10000], Loss: 0.0976\n",
      "Epoch [5390/10000], Loss: 0.0973\n",
      "Epoch [5400/10000], Loss: 0.1006\n",
      "Epoch [5410/10000], Loss: 0.0995\n",
      "Epoch [5420/10000], Loss: 0.0962\n",
      "Epoch [5430/10000], Loss: 0.0985\n",
      "Epoch [5440/10000], Loss: 0.0978\n",
      "Epoch [5450/10000], Loss: 0.0963\n",
      "Epoch [5460/10000], Loss: 0.0985\n",
      "Epoch [5470/10000], Loss: 0.0988\n",
      "Epoch [5480/10000], Loss: 0.0987\n",
      "Epoch [5490/10000], Loss: 0.0972\n",
      "Epoch [5500/10000], Loss: 0.0966\n",
      "Epoch [5510/10000], Loss: 0.0979\n",
      "Epoch [5520/10000], Loss: 0.0961\n",
      "Epoch [5530/10000], Loss: 0.0963\n",
      "Epoch [5540/10000], Loss: 0.0993\n",
      "Epoch [5550/10000], Loss: 0.0982\n",
      "Epoch [5560/10000], Loss: 0.0970\n",
      "Epoch [5570/10000], Loss: 0.0967\n",
      "Epoch [5580/10000], Loss: 0.0975\n",
      "Epoch [5590/10000], Loss: 0.0976\n",
      "Epoch [5600/10000], Loss: 0.0970\n",
      "Epoch [5610/10000], Loss: 0.0967\n",
      "Epoch [5620/10000], Loss: 0.0978\n",
      "Epoch [5630/10000], Loss: 0.0971\n",
      "Epoch [5640/10000], Loss: 0.0968\n",
      "Epoch [5650/10000], Loss: 0.0994\n",
      "Epoch [5660/10000], Loss: 0.0985\n",
      "Epoch [5670/10000], Loss: 0.0964\n",
      "Epoch [5680/10000], Loss: 0.0998\n",
      "Epoch [5690/10000], Loss: 0.0979\n",
      "Epoch [5700/10000], Loss: 0.0987\n",
      "Epoch [5710/10000], Loss: 0.0967\n",
      "Epoch [5720/10000], Loss: 0.0973\n",
      "Epoch [5730/10000], Loss: 0.0980\n",
      "Epoch [5740/10000], Loss: 0.0987\n",
      "Epoch [5750/10000], Loss: 0.0993\n",
      "Epoch [5760/10000], Loss: 0.0965\n",
      "Epoch [5770/10000], Loss: 0.0964\n",
      "Epoch [5780/10000], Loss: 0.0973\n",
      "Epoch [5790/10000], Loss: 0.0975\n",
      "Epoch [5800/10000], Loss: 0.0980\n",
      "Epoch [5810/10000], Loss: 0.1007\n",
      "Epoch [5820/10000], Loss: 0.0974\n",
      "Epoch [5830/10000], Loss: 0.0982\n",
      "Epoch [5840/10000], Loss: 0.0954\n",
      "Epoch [5850/10000], Loss: 0.0976\n",
      "Epoch [5860/10000], Loss: 0.0962\n",
      "Epoch [5870/10000], Loss: 0.0971\n",
      "Epoch [5880/10000], Loss: 0.0962\n",
      "Epoch [5890/10000], Loss: 0.1000\n",
      "Epoch [5900/10000], Loss: 0.0971\n",
      "Epoch [5910/10000], Loss: 0.0968\n",
      "Epoch [5920/10000], Loss: 0.0961\n",
      "Epoch [5930/10000], Loss: 0.1016\n",
      "Epoch [5940/10000], Loss: 0.0984\n",
      "Epoch [5950/10000], Loss: 0.0968\n",
      "Epoch [5960/10000], Loss: 0.0956\n",
      "Epoch [5970/10000], Loss: 0.0976\n",
      "Epoch [5980/10000], Loss: 0.0964\n",
      "Epoch [5990/10000], Loss: 0.0961\n",
      "Epoch [6000/10000], Loss: 0.1028\n",
      "Epoch [6010/10000], Loss: 0.0962\n",
      "Epoch [6020/10000], Loss: 0.0950\n",
      "Epoch [6030/10000], Loss: 0.0998\n",
      "Epoch [6040/10000], Loss: 0.0972\n",
      "Epoch [6050/10000], Loss: 0.0967\n",
      "Epoch [6060/10000], Loss: 0.0985\n",
      "Epoch [6070/10000], Loss: 0.0973\n",
      "Epoch [6080/10000], Loss: 0.0972\n",
      "Epoch [6090/10000], Loss: 0.0982\n",
      "Epoch [6100/10000], Loss: 0.0971\n",
      "Epoch [6110/10000], Loss: 0.0961\n",
      "Epoch [6120/10000], Loss: 0.0968\n",
      "Epoch [6130/10000], Loss: 0.0957\n",
      "Epoch [6140/10000], Loss: 0.0966\n",
      "Epoch [6150/10000], Loss: 0.0991\n",
      "Epoch [6160/10000], Loss: 0.0966\n",
      "Epoch [6170/10000], Loss: 0.0957\n",
      "Epoch [6180/10000], Loss: 0.0965\n",
      "Epoch [6190/10000], Loss: 0.0964\n",
      "Epoch [6200/10000], Loss: 0.0972\n",
      "Epoch [6210/10000], Loss: 0.0962\n",
      "Epoch [6220/10000], Loss: 0.0969\n",
      "Epoch [6230/10000], Loss: 0.0981\n",
      "Epoch [6240/10000], Loss: 0.0969\n",
      "Epoch [6250/10000], Loss: 0.0967\n",
      "Epoch [6260/10000], Loss: 0.0958\n",
      "Epoch [6270/10000], Loss: 0.0959\n",
      "Epoch [6280/10000], Loss: 0.0990\n",
      "Epoch [6290/10000], Loss: 0.0964\n",
      "Epoch [6300/10000], Loss: 0.0974\n",
      "Epoch [6310/10000], Loss: 0.0960\n",
      "Epoch [6320/10000], Loss: 0.0974\n",
      "Epoch [6330/10000], Loss: 0.0968\n",
      "Epoch [6340/10000], Loss: 0.0949\n",
      "Epoch [6350/10000], Loss: 0.0963\n",
      "Epoch [6360/10000], Loss: 0.0967\n",
      "Epoch [6370/10000], Loss: 0.0956\n",
      "Epoch [6380/10000], Loss: 0.0958\n",
      "Epoch [6390/10000], Loss: 0.0975\n",
      "Epoch [6400/10000], Loss: 0.0973\n",
      "Epoch [6410/10000], Loss: 0.0967\n",
      "Epoch [6420/10000], Loss: 0.0967\n",
      "Epoch [6430/10000], Loss: 0.0949\n",
      "Epoch [6440/10000], Loss: 0.0970\n",
      "Epoch [6450/10000], Loss: 0.0969\n",
      "Epoch [6460/10000], Loss: 0.0949\n",
      "Epoch [6470/10000], Loss: 0.0967\n",
      "Epoch [6480/10000], Loss: 0.1014\n",
      "Epoch [6490/10000], Loss: 0.0981\n",
      "Epoch [6500/10000], Loss: 0.0959\n",
      "Epoch [6510/10000], Loss: 0.0964\n",
      "Epoch [6520/10000], Loss: 0.0971\n",
      "Epoch [6530/10000], Loss: 0.0964\n",
      "Epoch [6540/10000], Loss: 0.0963\n",
      "Epoch [6550/10000], Loss: 0.0942\n",
      "Epoch [6560/10000], Loss: 0.0958\n",
      "Epoch [6570/10000], Loss: 0.0976\n",
      "Epoch [6580/10000], Loss: 0.0958\n",
      "Epoch [6590/10000], Loss: 0.0954\n",
      "Epoch [6600/10000], Loss: 0.0954\n",
      "Epoch [6610/10000], Loss: 0.0972\n",
      "Epoch [6620/10000], Loss: 0.0964\n",
      "Epoch [6630/10000], Loss: 0.0967\n",
      "Epoch [6640/10000], Loss: 0.0944\n",
      "Epoch [6650/10000], Loss: 0.0963\n",
      "Epoch [6660/10000], Loss: 0.0945\n",
      "Epoch [6670/10000], Loss: 0.0968\n",
      "Epoch [6680/10000], Loss: 0.0958\n",
      "Epoch [6690/10000], Loss: 0.0969\n",
      "Epoch [6700/10000], Loss: 0.0957\n",
      "Epoch [6710/10000], Loss: 0.0951\n",
      "Epoch [6720/10000], Loss: 0.0963\n",
      "Epoch [6730/10000], Loss: 0.0969\n",
      "Epoch [6740/10000], Loss: 0.0968\n",
      "Epoch [6750/10000], Loss: 0.0975\n",
      "Epoch [6760/10000], Loss: 0.0968\n",
      "Epoch [6770/10000], Loss: 0.0971\n",
      "Epoch [6780/10000], Loss: 0.0966\n",
      "Epoch [6790/10000], Loss: 0.0971\n",
      "Epoch [6800/10000], Loss: 0.0958\n",
      "Epoch [6810/10000], Loss: 0.0974\n",
      "Epoch [6820/10000], Loss: 0.0963\n",
      "Epoch [6830/10000], Loss: 0.0957\n",
      "Epoch [6840/10000], Loss: 0.0944\n",
      "Epoch [6850/10000], Loss: 0.0948\n",
      "Epoch [6860/10000], Loss: 0.0953\n",
      "Epoch [6870/10000], Loss: 0.1024\n",
      "Epoch [6880/10000], Loss: 0.0963\n",
      "Epoch [6890/10000], Loss: 0.0946\n",
      "Epoch [6900/10000], Loss: 0.0969\n",
      "Epoch [6910/10000], Loss: 0.0956\n",
      "Epoch [6920/10000], Loss: 0.0952\n",
      "Epoch [6930/10000], Loss: 0.0945\n",
      "Epoch [6940/10000], Loss: 0.0987\n",
      "Epoch [6950/10000], Loss: 0.0969\n",
      "Epoch [6960/10000], Loss: 0.0949\n",
      "Epoch [6970/10000], Loss: 0.0966\n",
      "Epoch [6980/10000], Loss: 0.0954\n",
      "Epoch [6990/10000], Loss: 0.0979\n",
      "Epoch [7000/10000], Loss: 0.0953\n",
      "Epoch [7010/10000], Loss: 0.0952\n",
      "Epoch [7020/10000], Loss: 0.0955\n",
      "Epoch [7030/10000], Loss: 0.0957\n",
      "Epoch [7040/10000], Loss: 0.0947\n",
      "Epoch [7050/10000], Loss: 0.0987\n",
      "Epoch [7060/10000], Loss: 0.0960\n",
      "Epoch [7070/10000], Loss: 0.0955\n",
      "Epoch [7080/10000], Loss: 0.0958\n",
      "Epoch [7090/10000], Loss: 0.0949\n",
      "Epoch [7100/10000], Loss: 0.0985\n",
      "Epoch [7110/10000], Loss: 0.0994\n",
      "Epoch [7120/10000], Loss: 0.0949\n",
      "Epoch [7130/10000], Loss: 0.0959\n",
      "Epoch [7140/10000], Loss: 0.0956\n",
      "Epoch [7150/10000], Loss: 0.0975\n",
      "Epoch [7160/10000], Loss: 0.0942\n",
      "Epoch [7170/10000], Loss: 0.0938\n",
      "Epoch [7180/10000], Loss: 0.0959\n",
      "Epoch [7190/10000], Loss: 0.0992\n",
      "Epoch [7200/10000], Loss: 0.0940\n",
      "Epoch [7210/10000], Loss: 0.0963\n",
      "Epoch [7220/10000], Loss: 0.0940\n",
      "Epoch [7230/10000], Loss: 0.0956\n",
      "Epoch [7240/10000], Loss: 0.0997\n",
      "Epoch [7250/10000], Loss: 0.0950\n",
      "Epoch [7260/10000], Loss: 0.0958\n",
      "Epoch [7270/10000], Loss: 0.0951\n",
      "Epoch [7280/10000], Loss: 0.0937\n",
      "Epoch [7290/10000], Loss: 0.1002\n",
      "Epoch [7300/10000], Loss: 0.0972\n",
      "Epoch [7310/10000], Loss: 0.0946\n",
      "Epoch [7320/10000], Loss: 0.0957\n",
      "Epoch [7330/10000], Loss: 0.0955\n",
      "Epoch [7340/10000], Loss: 0.0959\n",
      "Epoch [7350/10000], Loss: 0.0958\n",
      "Epoch [7360/10000], Loss: 0.0946\n",
      "Epoch [7370/10000], Loss: 0.0952\n",
      "Epoch [7380/10000], Loss: 0.0964\n",
      "Epoch [7390/10000], Loss: 0.0945\n",
      "Epoch [7400/10000], Loss: 0.0971\n",
      "Epoch [7410/10000], Loss: 0.0966\n",
      "Epoch [7420/10000], Loss: 0.0944\n",
      "Epoch [7430/10000], Loss: 0.0954\n",
      "Epoch [7440/10000], Loss: 0.0951\n",
      "Epoch [7450/10000], Loss: 0.0954\n",
      "Epoch [7460/10000], Loss: 0.0959\n",
      "Epoch [7470/10000], Loss: 0.0955\n",
      "Epoch [7480/10000], Loss: 0.0947\n",
      "Epoch [7490/10000], Loss: 0.0942\n",
      "Epoch [7500/10000], Loss: 0.0965\n",
      "Epoch [7510/10000], Loss: 0.0946\n",
      "Epoch [7520/10000], Loss: 0.0972\n",
      "Epoch [7530/10000], Loss: 0.0949\n",
      "Epoch [7540/10000], Loss: 0.0946\n",
      "Epoch [7550/10000], Loss: 0.0961\n",
      "Epoch [7560/10000], Loss: 0.0953\n",
      "Epoch [7570/10000], Loss: 0.0992\n",
      "Epoch [7580/10000], Loss: 0.0951\n",
      "Epoch [7590/10000], Loss: 0.0943\n",
      "Epoch [7600/10000], Loss: 0.0938\n",
      "Epoch [7610/10000], Loss: 0.0940\n",
      "Epoch [7620/10000], Loss: 0.0943\n",
      "Epoch [7630/10000], Loss: 0.0963\n",
      "Epoch [7640/10000], Loss: 0.0944\n",
      "Epoch [7650/10000], Loss: 0.0985\n",
      "Epoch [7660/10000], Loss: 0.0944\n",
      "Epoch [7670/10000], Loss: 0.0939\n",
      "Epoch [7680/10000], Loss: 0.0961\n",
      "Epoch [7690/10000], Loss: 0.0941\n",
      "Epoch [7700/10000], Loss: 0.1018\n",
      "Epoch [7710/10000], Loss: 0.0953\n",
      "Epoch [7720/10000], Loss: 0.0937\n",
      "Epoch [7730/10000], Loss: 0.0957\n",
      "Epoch [7740/10000], Loss: 0.0943\n",
      "Epoch [7750/10000], Loss: 0.0958\n",
      "Epoch [7760/10000], Loss: 0.0950\n",
      "Epoch [7770/10000], Loss: 0.0936\n",
      "Epoch [7780/10000], Loss: 0.0978\n",
      "Epoch [7790/10000], Loss: 0.0945\n",
      "Epoch [7800/10000], Loss: 0.0937\n",
      "Epoch [7810/10000], Loss: 0.0960\n",
      "Epoch [7820/10000], Loss: 0.0949\n",
      "Epoch [7830/10000], Loss: 0.0942\n",
      "Epoch [7840/10000], Loss: 0.0956\n",
      "Epoch [7850/10000], Loss: 0.0981\n",
      "Epoch [7860/10000], Loss: 0.0962\n",
      "Epoch [7870/10000], Loss: 0.0940\n",
      "Epoch [7880/10000], Loss: 0.0963\n",
      "Epoch [7890/10000], Loss: 0.0973\n",
      "Epoch [7900/10000], Loss: 0.0956\n",
      "Epoch [7910/10000], Loss: 0.0948\n",
      "Epoch [7920/10000], Loss: 0.0958\n",
      "Epoch [7930/10000], Loss: 0.0949\n",
      "Epoch [7940/10000], Loss: 0.0943\n",
      "Epoch [7950/10000], Loss: 0.0954\n",
      "Epoch [7960/10000], Loss: 0.0956\n",
      "Epoch [7970/10000], Loss: 0.0951\n",
      "Epoch [7980/10000], Loss: 0.0946\n",
      "Epoch [7990/10000], Loss: 0.0944\n",
      "Epoch [8000/10000], Loss: 0.0970\n",
      "Epoch [8010/10000], Loss: 0.0958\n",
      "Epoch [8020/10000], Loss: 0.0945\n",
      "Epoch [8030/10000], Loss: 0.0995\n",
      "Epoch [8040/10000], Loss: 0.0936\n",
      "Epoch [8050/10000], Loss: 0.0940\n",
      "Epoch [8060/10000], Loss: 0.0980\n",
      "Epoch [8070/10000], Loss: 0.0958\n",
      "Epoch [8080/10000], Loss: 0.0957\n",
      "Epoch [8090/10000], Loss: 0.0953\n",
      "Epoch [8100/10000], Loss: 0.0969\n",
      "Epoch [8110/10000], Loss: 0.0940\n",
      "Epoch [8120/10000], Loss: 0.0956\n",
      "Epoch [8130/10000], Loss: 0.0937\n",
      "Epoch [8140/10000], Loss: 0.0940\n",
      "Epoch [8150/10000], Loss: 0.0968\n",
      "Epoch [8160/10000], Loss: 0.0939\n",
      "Epoch [8170/10000], Loss: 0.0954\n",
      "Epoch [8180/10000], Loss: 0.0955\n",
      "Epoch [8190/10000], Loss: 0.0934\n",
      "Epoch [8200/10000], Loss: 0.0955\n",
      "Epoch [8210/10000], Loss: 0.0944\n",
      "Epoch [8220/10000], Loss: 0.0933\n",
      "Epoch [8230/10000], Loss: 0.0953\n",
      "Epoch [8240/10000], Loss: 0.0958\n",
      "Epoch [8250/10000], Loss: 0.0944\n",
      "Epoch [8260/10000], Loss: 0.0963\n",
      "Epoch [8270/10000], Loss: 0.0938\n",
      "Epoch [8280/10000], Loss: 0.0950\n",
      "Epoch [8290/10000], Loss: 0.0980\n",
      "Epoch [8300/10000], Loss: 0.0963\n",
      "Epoch [8310/10000], Loss: 0.0950\n",
      "Epoch [8320/10000], Loss: 0.0945\n",
      "Epoch [8330/10000], Loss: 0.0939\n",
      "Epoch [8340/10000], Loss: 0.0949\n",
      "Epoch [8350/10000], Loss: 0.0953\n",
      "Epoch [8360/10000], Loss: 0.0964\n",
      "Epoch [8370/10000], Loss: 0.0955\n",
      "Epoch [8380/10000], Loss: 0.0944\n",
      "Epoch [8390/10000], Loss: 0.0953\n",
      "Epoch [8400/10000], Loss: 0.0955\n",
      "Epoch [8410/10000], Loss: 0.0936\n",
      "Epoch [8420/10000], Loss: 0.0949\n",
      "Epoch [8430/10000], Loss: 0.0934\n",
      "Epoch [8440/10000], Loss: 0.0949\n",
      "Epoch [8450/10000], Loss: 0.0945\n",
      "Epoch [8460/10000], Loss: 0.0927\n",
      "Epoch [8470/10000], Loss: 0.0960\n",
      "Epoch [8480/10000], Loss: 0.0963\n",
      "Epoch [8490/10000], Loss: 0.0934\n",
      "Epoch [8500/10000], Loss: 0.0946\n",
      "Epoch [8510/10000], Loss: 0.0968\n",
      "Epoch [8520/10000], Loss: 0.0937\n",
      "Epoch [8530/10000], Loss: 0.0945\n",
      "Epoch [8540/10000], Loss: 0.0924\n",
      "Epoch [8550/10000], Loss: 0.0957\n",
      "Epoch [8560/10000], Loss: 0.0966\n",
      "Epoch [8570/10000], Loss: 0.0934\n",
      "Epoch [8580/10000], Loss: 0.0944\n",
      "Epoch [8590/10000], Loss: 0.0935\n",
      "Epoch [8600/10000], Loss: 0.0941\n",
      "Epoch [8610/10000], Loss: 0.0940\n",
      "Epoch [8620/10000], Loss: 0.0943\n",
      "Epoch [8630/10000], Loss: 0.0985\n",
      "Epoch [8640/10000], Loss: 0.0946\n",
      "Epoch [8650/10000], Loss: 0.0933\n",
      "Epoch [8660/10000], Loss: 0.0949\n",
      "Epoch [8670/10000], Loss: 0.0956\n",
      "Epoch [8680/10000], Loss: 0.0943\n",
      "Epoch [8690/10000], Loss: 0.0936\n",
      "Epoch [8700/10000], Loss: 0.0958\n",
      "Epoch [8710/10000], Loss: 0.0939\n",
      "Epoch [8720/10000], Loss: 0.0946\n",
      "Epoch [8730/10000], Loss: 0.0950\n",
      "Epoch [8740/10000], Loss: 0.0933\n",
      "Epoch [8750/10000], Loss: 0.0940\n",
      "Epoch [8760/10000], Loss: 0.0972\n",
      "Epoch [8770/10000], Loss: 0.0946\n",
      "Epoch [8780/10000], Loss: 0.0940\n",
      "Epoch [8790/10000], Loss: 0.0948\n",
      "Epoch [8800/10000], Loss: 0.0930\n",
      "Epoch [8810/10000], Loss: 0.1052\n",
      "Epoch [8820/10000], Loss: 0.0962\n",
      "Epoch [8830/10000], Loss: 0.0935\n",
      "Epoch [8840/10000], Loss: 0.0938\n",
      "Epoch [8850/10000], Loss: 0.0928\n",
      "Epoch [8860/10000], Loss: 0.0936\n",
      "Epoch [8870/10000], Loss: 0.0948\n",
      "Epoch [8880/10000], Loss: 0.0941\n",
      "Epoch [8890/10000], Loss: 0.0922\n",
      "Epoch [8900/10000], Loss: 0.0932\n",
      "Epoch [8910/10000], Loss: 0.0979\n",
      "Epoch [8920/10000], Loss: 0.0936\n",
      "Epoch [8930/10000], Loss: 0.0932\n",
      "Epoch [8940/10000], Loss: 0.0932\n",
      "Epoch [8950/10000], Loss: 0.0948\n",
      "Epoch [8960/10000], Loss: 0.0953\n",
      "Epoch [8970/10000], Loss: 0.0930\n",
      "Epoch [8980/10000], Loss: 0.0927\n",
      "Epoch [8990/10000], Loss: 0.0932\n",
      "Epoch [9000/10000], Loss: 0.0922\n",
      "Epoch [9010/10000], Loss: 0.0999\n",
      "Epoch [9020/10000], Loss: 0.0945\n",
      "Epoch [9030/10000], Loss: 0.0934\n",
      "Epoch [9040/10000], Loss: 0.0932\n",
      "Epoch [9050/10000], Loss: 0.0935\n",
      "Epoch [9060/10000], Loss: 0.0967\n",
      "Epoch [9070/10000], Loss: 0.0957\n",
      "Epoch [9080/10000], Loss: 0.0940\n",
      "Epoch [9090/10000], Loss: 0.0924\n",
      "Epoch [9100/10000], Loss: 0.0951\n",
      "Epoch [9110/10000], Loss: 0.0936\n",
      "Epoch [9120/10000], Loss: 0.0935\n",
      "Epoch [9130/10000], Loss: 0.0919\n",
      "Epoch [9140/10000], Loss: 0.0955\n",
      "Epoch [9150/10000], Loss: 0.0951\n",
      "Epoch [9160/10000], Loss: 0.0937\n",
      "Epoch [9170/10000], Loss: 0.0942\n",
      "Epoch [9180/10000], Loss: 0.0970\n",
      "Epoch [9190/10000], Loss: 0.0937\n",
      "Epoch [9200/10000], Loss: 0.0927\n",
      "Epoch [9210/10000], Loss: 0.0927\n",
      "Epoch [9220/10000], Loss: 0.0952\n",
      "Epoch [9230/10000], Loss: 0.0930\n",
      "Epoch [9240/10000], Loss: 0.0944\n",
      "Epoch [9250/10000], Loss: 0.0948\n",
      "Epoch [9260/10000], Loss: 0.0929\n",
      "Epoch [9270/10000], Loss: 0.0943\n",
      "Epoch [9280/10000], Loss: 0.0928\n",
      "Epoch [9290/10000], Loss: 0.0932\n",
      "Epoch [9300/10000], Loss: 0.0971\n",
      "Epoch [9310/10000], Loss: 0.0927\n",
      "Epoch [9320/10000], Loss: 0.0921\n",
      "Epoch [9330/10000], Loss: 0.0967\n",
      "Epoch [9340/10000], Loss: 0.0943\n",
      "Epoch [9350/10000], Loss: 0.0929\n",
      "Epoch [9360/10000], Loss: 0.0935\n",
      "Epoch [9370/10000], Loss: 0.0963\n",
      "Epoch [9380/10000], Loss: 0.0975\n",
      "Epoch [9390/10000], Loss: 0.0928\n",
      "Epoch [9400/10000], Loss: 0.0929\n",
      "Epoch [9410/10000], Loss: 0.0928\n",
      "Epoch [9420/10000], Loss: 0.0923\n",
      "Epoch [9430/10000], Loss: 0.0945\n",
      "Epoch [9440/10000], Loss: 0.0938\n",
      "Epoch [9450/10000], Loss: 0.0972\n",
      "Epoch [9460/10000], Loss: 0.0947\n",
      "Epoch [9470/10000], Loss: 0.0946\n",
      "Epoch [9480/10000], Loss: 0.0932\n",
      "Epoch [9490/10000], Loss: 0.0938\n",
      "Epoch [9500/10000], Loss: 0.0943\n",
      "Epoch [9510/10000], Loss: 0.0935\n",
      "Epoch [9520/10000], Loss: 0.0964\n",
      "Epoch [9530/10000], Loss: 0.0942\n",
      "Epoch [9540/10000], Loss: 0.0947\n",
      "Epoch [9550/10000], Loss: 0.0933\n",
      "Epoch [9560/10000], Loss: 0.0924\n",
      "Epoch [9570/10000], Loss: 0.0937\n",
      "Epoch [9580/10000], Loss: 0.1006\n",
      "Epoch [9590/10000], Loss: 0.0954\n",
      "Epoch [9600/10000], Loss: 0.0928\n",
      "Epoch [9610/10000], Loss: 0.0929\n",
      "Epoch [9620/10000], Loss: 0.0930\n",
      "Epoch [9630/10000], Loss: 0.0941\n",
      "Epoch [9640/10000], Loss: 0.0937\n",
      "Epoch [9650/10000], Loss: 0.0942\n",
      "Epoch [9660/10000], Loss: 0.0927\n",
      "Epoch [9670/10000], Loss: 0.0942\n",
      "Epoch [9680/10000], Loss: 0.0936\n",
      "Epoch [9690/10000], Loss: 0.0932\n",
      "Epoch [9700/10000], Loss: 0.0933\n",
      "Epoch [9710/10000], Loss: 0.0954\n",
      "Epoch [9720/10000], Loss: 0.0933\n",
      "Epoch [9730/10000], Loss: 0.0944\n",
      "Epoch [9740/10000], Loss: 0.0948\n",
      "Epoch [9750/10000], Loss: 0.0924\n",
      "Epoch [9760/10000], Loss: 0.0938\n",
      "Epoch [9770/10000], Loss: 0.0943\n",
      "Epoch [9780/10000], Loss: 0.0943\n",
      "Epoch [9790/10000], Loss: 0.0937\n",
      "Epoch [9800/10000], Loss: 0.0927\n",
      "Epoch [9810/10000], Loss: 0.0937\n",
      "Epoch [9820/10000], Loss: 0.0939\n",
      "Epoch [9830/10000], Loss: 0.0939\n",
      "Epoch [9840/10000], Loss: 0.0924\n",
      "Epoch [9850/10000], Loss: 0.0937\n",
      "Epoch [9860/10000], Loss: 0.0943\n",
      "Epoch [9870/10000], Loss: 0.0935\n",
      "Epoch [9880/10000], Loss: 0.0925\n",
      "Epoch [9890/10000], Loss: 0.0948\n",
      "Epoch [9900/10000], Loss: 0.0935\n",
      "Epoch [9910/10000], Loss: 0.0930\n",
      "Epoch [9920/10000], Loss: 0.0955\n",
      "Epoch [9930/10000], Loss: 0.0941\n",
      "Epoch [9940/10000], Loss: 0.0918\n",
      "Epoch [9950/10000], Loss: 0.0948\n",
      "Epoch [9960/10000], Loss: 0.0929\n",
      "Epoch [9970/10000], Loss: 0.0956\n",
      "Epoch [9980/10000], Loss: 0.0933\n",
      "Epoch [9990/10000], Loss: 0.0921\n",
      "Epoch [10000/10000], Loss: 0.0971\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성 및 학습 설정\n",
    "ssae_model = DenoisingSSAE(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ssae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(n_epochs_ssae):\n",
    "    ssae_model.train()\n",
    "    inputs = torch.FloatTensor(X_train_noisy_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "    targets = torch.FloatTensor(X_train_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = ssae_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs_ssae}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNxDAlFBKApN"
   },
   "source": [
    "## 모델 평가함수 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yXpRxDP8W6Zo"
   },
   "outputs": [],
   "source": [
    "# # 평가함수 정의\n",
    "# def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "        \n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#     print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "#     disp.plot()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "81YDhprUKApO"
   },
   "outputs": [],
   "source": [
    "# Shapley Value 계산, 시각화 함수 정의\n",
    "def evaluate_models_shap1(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    masker = shap.maskers.Independent(X_train_encoded_ssae)\n",
    "    explainer = shap.LinearExplainer(model, masker=masker)\n",
    "    shap_values = explainer(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n",
    "\n",
    "def evaluate_models_shap2(model, X_test_encoded_ssae):\n",
    "    # TreeExplainer를 사용한 Shapley Value 계산\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "D6GIT1bg23Ny"
   },
   "outputs": [],
   "source": [
    "# ShapleyValue 계산, 시각화함수 정의 (TabNet 전용)\n",
    "def evaluate_models_shap_tabnet(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    # DeepExplainer를 사용한 Shapley Value 계산 (TabNet 전용)\n",
    "    explainer = DeepExplainer(model, X_train_encoded_ssae)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증(CV)을 위한 함수 정의\n",
    "def cross_validate_model(model, X, y, cv=cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목적함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 정의\n",
    "class LoanAnalysis:\n",
    "    \"\"\"\n",
    "    1. 라벨링 수행 TN, FN, TP, FP\n",
    "    2. 각 라벨별 annualized return(ar) 계산 수행: annualized return = revenue^(1/term)-1\n",
    "    3. 포트폴리오 전체 ar 계산 수행(money-weighted sum)\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.copy()\n",
    "        self.label_results()\n",
    "\n",
    "    def label_results(self):\n",
    "        self.df['result'] = self.df.apply(lambda row: \n",
    "            'True Negative' if row['predicted'] == 0 and row['actual'] == 0 else \n",
    "            'False Negative' if row['predicted'] == 0 and row['actual'] == 1 else \n",
    "            'True Positive' if row['predicted'] == 1 and row['actual'] == 1 else \n",
    "            'False Positive' if row['predicted'] == 1 and row['actual'] == 0 else \n",
    "            'Other', axis=1)\n",
    "\n",
    "    def calculate_true_negative_ar(self):\n",
    "        \"\"\"\n",
    "        True Negative 레코드를 필터링하고 return 열을 계산합니다.\n",
    "        \"\"\"\n",
    "        true_negative_df = self.df[self.df['result'] == 'True Negative'].copy()\n",
    "        true_negative_df['revenue'] = true_negative_df['total_pymnt'] # 대출이자수익(+원금) # total_pymnt = loan_amnt + int_rate * loan_amnt\n",
    "        true_negative_df['annualized_return'] = (true_negative_df['revenue']/true_negative_df['loan_amnt'])**(1/(true_negative_df['term_num']/12)) - 1\n",
    "\n",
    "        return true_negative_df\n",
    "\n",
    "    def calculate_false_negative_ar(self):\n",
    "        \"\"\"\n",
    "        False Negative 레코드를 필터링하고 return 열을 계산합니다.\n",
    "        \"\"\"\n",
    "        false_negative_df = self.df[self.df['result'] == 'False Negative'].copy()\n",
    "        false_negative_df['revenue'] = false_negative_df['total_pymnt'] + false_negative_df['recoveries'] - false_negative_df['collection_recovery_fee'] # default로 인한 손실\n",
    "        false_negative_df['annualized_return'] = (false_negative_df['revenue']/false_negative_df['loan_amnt'])**(1/(false_negative_df['term_num']/12)) - 1\n",
    "\n",
    "        return false_negative_df\n",
    "\n",
    "    def calculate_positives_ar(self):\n",
    "        \"\"\"\n",
    "        True Positive와 False Positive 레코드를 필터링하고 return 열을 계산합니다.\n",
    "        \"\"\"\n",
    "        positive_df = self.df[self.df['result'].isin(['True Positive', 'False Positive'])].copy()\n",
    "        positive_df['annualized_return'] = positive_df['tbond_int'] # 국채수익률 = 이미 annualized 되어 있음\n",
    "        return positive_df\n",
    "\n",
    "    def process_all(self):\n",
    "        \"\"\"\n",
    "        모든 레코드에 대한 return 열을 계산합니다.\n",
    "        \"\"\"\n",
    "        tn_df = self.calculate_true_negative_ar()\n",
    "        fn_df = self.calculate_false_negative_ar()\n",
    "        pos_df = self.calculate_positives_ar()\n",
    "\n",
    "        # 각 데이터프레임을 결합하여 반환\n",
    "        final_df = pd.concat([tn_df, fn_df, pos_df])\n",
    "        return final_df\n",
    "    \n",
    "    def calculate_portfolio_ar(self):\n",
    "        final_df = self.process_all()\n",
    "        # 각 개별 loan amount의 비중을 계산하고, 가중치와 해당 annualized return을 곱한 값을 합산\n",
    "        final_df['weight'] = final_df['loan_amnt'] / final_df['loan_amnt'].sum()\n",
    "        final_df['weighted_ar'] = final_df['weight'] * final_df['annualized_return']\n",
    "    \n",
    "        portfolio_annualized_return = final_df['weighted_ar'].sum()\n",
    "        return portfolio_annualized_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_list = [f'e{i}' for i in range(latent_size)]\n",
    "X_train_encoded = pd.concat(\n",
    "                    [X_train[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_train[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_train.index, columns=encoded_list)],\n",
    "                    axis=1)\n",
    "\n",
    "X_test_encoded = pd.concat(\n",
    "                    [X_test[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_test[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_test.index, columns=encoded_list)],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scalerenc = StandardScaler()\n",
    "X_train_encoded[encoded_list] = scalerenc.fit_transform(X_train_encoded[encoded_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test_encoded[encoded_list] = scalerenc.transform(X_test_encoded[encoded_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Logistic Regression model:\n",
      "  F1 Score: 0.3018\n",
      "  Accuracy: 0.5518\n",
      "  Precision: 0.2038\n",
      "  Recall: 0.5811\n",
      "  Annualized return: 1.6953%\n",
      "  Best Parameters: {'C': 0.46415888336127775, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression  # 수정된 부분\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 모든 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Stopped training because there are no more leaves\")\n",
    "\n",
    "# Train and evaluate the model with a given parameter set\n",
    "def evaluate_model(model_name, model, param_set):\n",
    "    try:\n",
    "        model.set_params(**param_set)\n",
    "        model.fit(X_train_encoded, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_encoded)\n",
    "        y_pred = list(y_pred)\n",
    "        X_obj[\"predicted\"]= y_pred\n",
    "        X_obj['actual'] = y_test\n",
    "        analysis = LoanAnalysis(X_obj)\n",
    "        portfolio_annualized_return = analysis.calculate_portfolio_ar()\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracy, precision, recall, f1, portfolio_annualized_return, confusion_matrix(y_test, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name} with params: {param_set}. Error: {str(e)}\")\n",
    "        return None, None, None, None, None , None\n",
    "\n",
    "# Define parameter grids for each model\n",
    "logistic_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Regularization strength with 10 values\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],  # Fast solver for logistic regression with L2 regularization\n",
    "    'max_iter': [1000]  # Very high iteration count\n",
    "}\n",
    "\n",
    "# Common parameter grid for XGBoost, LightGBM, and RandomForest\n",
    "common_param_grid = {\n",
    "    'learning_rate': np.logspace(-4, 0, 10),  # Learning rate with 10 values\n",
    "    'reg_alpha': np.logspace(-3, 1, 10),  # L1 regularization with 10 values\n",
    "    'reg_lambda': np.logspace(-3, 1, 10),  # L2 regularization with 10 values\n",
    "    'n_estimators': [1000]  # Very high number of trees\n",
    "}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Regularization strength with 10 values\n",
    "    'kernel': ['rbf'],  # RBF kernel is commonly used\n",
    "    'gamma': ['scale'],  # Standard option for gamma\n",
    "    'max_iter': [1000]  # Very high iteration count for convergence\n",
    "}\n",
    "\n",
    "# Evaluate models with parameter grids\n",
    "logistic_model = LogisticRegression(n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(n_jobs=-1, verbosity=0)  # Disable XGBoost output\n",
    "lgb_model = lgb.LGBMClassifier(n_jobs=-1, verbose=-1)  # Disable LightGBM output\n",
    "rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "svm_model = SVC()\n",
    "\n",
    "# Dictionary to store best results for each model\n",
    "best_results = {}\n",
    "\n",
    "# Logistic Regression with portfolio_annualized_return optimization\n",
    "for params in ParameterGrid(logistic_param_grid):\n",
    "    accuracy, precision, recall, f1, portfolio_annualized_return, cm = evaluate_model('Logistic Regression', logistic_model, params)\n",
    "    if accuracy is not None and ('Logistic Regression' not in best_results or portfolio_annualized_return > best_results['Logistic Regression']['portfolio_annualized_return']):\n",
    "        best_results['Logistic Regression'] = {\n",
    "            'model': logistic_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'portfolio_annualized_return': portfolio_annualized_return,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest Logistic Regression model:\")\n",
    "if 'Logistic Regression' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['Logistic Regression']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['Logistic Regression']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['Logistic Regression']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['Logistic Regression']['recall']:.4f}\")\n",
    "    print(f\"  Annualized return: {best_results['Logistic Regression']['portfolio_annualized_return']*100:.4f}%\") # 목적함수 추가\n",
    "    print(f\"  Best Parameters: {best_results['Logistic Regression']['params']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with F1 optimization\n",
    "# for params in ParameterGrid(common_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('XGBoost Classifier', xgb_model, params)\n",
    "#     if accuracy is not None and ('XGBoost Classifier' not in best_results or f1 > best_results['XGBoost Classifier']['f1_score']):\n",
    "#         best_results['XGBoost Classifier'] = {\n",
    "#             'model': xgb_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest XGBoost model:\")\n",
    "# if 'XGBoost Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['XGBoost Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['XGBoost Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['XGBoost Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['XGBoost Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['XGBoost Classifier']['params']}\\n\")\n",
    "\n",
    "# LightGBM with portfolio_annualized_return optimization\n",
    "for params in ParameterGrid(common_param_grid):\n",
    "    accuracy, precision, recall, f1,portfolio_annualized_return, cm = evaluate_model('LightGBM Classifier', lgb_model, params)\n",
    "    if accuracy is not None and ('LightGBM Classifier' not in best_results or portfolio_annualized_return > best_results['LightGBM Classifier']['portfolio_annualized_return']):\n",
    "        best_results['LightGBM Classifier'] = {\n",
    "            'model': lgb_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'portfolio_annualized_return': portfolio_annualized_return,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest LightGBM model:\")\n",
    "if 'LightGBM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['LightGBM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['LightGBM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['LightGBM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['LightGBM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Annualized return: {best_results['LightGBM Classifier']['portfolio_annualized_return']*100:.4f}%\") # 목적함수 추가\n",
    "    print(f\"  Best Parameters: {best_results['LightGBM Classifier']['params']}\\n\")\n",
    "\n",
    "# RandomForest with F1 optimization\n",
    "# for params in ParameterGrid(common_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('RandomForest Classifier', rf_model, params)\n",
    "#     if accuracy is not None and ('RandomForest Classifier' not in best_results or f1 > best_results['RandomForest Classifier']['f1_score']):\n",
    "#         best_results['RandomForest Classifier'] = {\n",
    "#             'model': rf_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest RandomForest model:\")\n",
    "# if 'RandomForest Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['RandomForest Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['RandomForest Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['RandomForest Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['RandomForest Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['RandomForest Classifier']['params']}\\n\")\n",
    "\n",
    "# # SVM with F1 optimization\n",
    "# for params in ParameterGrid(svm_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('SVM Classifier', svm_model, params)\n",
    "#     if accuracy is not None and ('SVM Classifier' not in best_results or f1 > best_results['SVM Classifier']['f1_score']):\n",
    "#         best_results['SVM Classifier'] = {\n",
    "#             'model': svm_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest SVM model:\")\n",
    "# if 'SVM Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['SVM Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['SVM Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['SVM Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['SVM Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['SVM Classifier']['params']}\\n\")\n",
    "\n",
    "# # Find the best overall model based on F1 score\n",
    "# best_model_name = max(best_results, key=lambda x: best_results[x]['f1_score'])\n",
    "# best_model_results = best_results[best_model_name]\n",
    "\n",
    "# # Print the best overall model\n",
    "# print(f\"\\nBest overall model: {best_model_name}\")\n",
    "# print(f\"  F1 Score: {best_model_results['f1_score']:.4f}\")\n",
    "# print(f\"  Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "# print(f\"  Precision: {best_model_results['precision']:.4f}\")\n",
    "# print(f\"  Recall: {best_model_results['recall']:.4f}\")\n",
    "# print(f\"  Best Parameters: {best_model_results['params']}\")\n",
    "\n",
    "# # Plot confusion matrix for the best overall model\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=best_model_results['confusion_matrix'])\n",
    "# disp.plot()\n",
    "# plt.title(f\"Confusion Matrix for {best_model_name}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best LightGBM model:\n",
      "  F1 Score: 0.1325\n",
      "  Accuracy: 0.7050\n",
      "  Precision: 0.1299\n",
      "  Recall: 0.1351\n",
      "  Annualized return: 2.3849%\n",
      "  Best Parameters: {'learning_rate': 0.0001, 'n_estimators': 1000, 'reg_alpha': 10.0, 'reg_lambda': 0.001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest LightGBM model:\")\n",
    "if 'LightGBM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['LightGBM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['LightGBM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['LightGBM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['LightGBM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Annualized return: {best_results['LightGBM Classifier']['portfolio_annualized_return']*100:.4f}%\") # 목적함수 추가\n",
    "    print(f\"  Best Parameters: {best_results['LightGBM Classifier']['params']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
