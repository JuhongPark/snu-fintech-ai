{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q35P2ummXim"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/JuhongPark/snu-fintech-ai/blob/main/Lending_Club_SSAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WyoEfeVoyh-"
   },
   "source": [
    "# 데이터 가져오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brnach 지정\n",
    "branch = 'develop'\n",
    "\n",
    "# 라이브러리 설치 (2분 가량 소요됨)\n",
    "#!pip install shap -q\n",
    "#!pip install pytorch_tabnet -q\n",
    "\n",
    "# 필요 파일 다운로드\n",
    "#!wget https://raw.githubusercontent.com/JuhongPark/snu-fintech-ai/{branch}/LC_Data_Cleaned_0829.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # seed 값 설정\n",
    "is_test = False  # 테스트런 설정\n",
    "\n",
    "# 입력 변수 파라미터\n",
    "target = 'loan_status_encoded'\n",
    "drop_list = ['id', 'int_rate', 'installment', 'sub_grade', 'grade', 'tbond_int', 'year', 'term', 'loan_status', 'zip_code']\n",
    "feature_list = ['loan_amnt', 'emp_length', 'revol_util', 'pub_rec', 'fico_range_high', 'fico_range_low', 'percent_bc_gt_75', 'annual_inc',\n",
    "                'dti', 'delinq_2yrs', 'open_acc', 'revol_bal', 'total_acc', 'inq_last_6mths'] # 'fico_range_low'제외\n",
    "cat_list = ['purpose', 'addr_state', 'initial_list_status', 'home_ownership', 'application_type', 'verification_status_joint', 'issue_d']\n",
    "\n",
    "# 대출 이후 변수\n",
    "post_list = ['Funded_amnt', 'funded_amnt_inv', 'collection_recovery_fee', 'collections_12_mths_ex_med', 'last_credit_pull_d', 'last_pymnt_amnt', 'last_pymnt_d',\n",
    " 'mths_since_last_major_derog', 'next_pymnt_d', 'out_prncp', 'out_prncp_inv', 'recoveries', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee',\n",
    " 'total_rec_prncp', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date',\n",
    " 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount',\n",
    " 'hardship_last_payment_amount', 'debt_settlement_flag', 'last_fico_range_high', 'last_fico_range_low']\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "# For model\n",
    "learning_rate = 0.01\n",
    "\n",
    "# For training\n",
    "#n_epochs = 10000\n",
    "\n",
    "# For CV\n",
    "cv = 10\n",
    "\n",
    "# For SSAE\n",
    "n_epochs_ssae = 10000\n",
    "latent_size = 8\n",
    "\n",
    "# 테스트 런일 경우, 에포크 수를 줄임\n",
    "if is_test:\n",
    "    n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATZ3E7NEKApJ",
    "outputId": "653a45fc-17ab-47e4-b4ae-5360f61fcec2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed 설정\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed) # 파이썬 표준 라이브러리\n",
    "    np.random.seed(seed) # numpy의 random 모듈에서 사용하는 seed\n",
    "    torch.manual_seed(seed) # pytorch에서 사용하는 seed\n",
    "    if torch.cuda.is_available(): # GPU에서 실행되는 PyTorch 연산의 무작위성을 제어\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# seed 값 설정\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ans_U0Bqoyh-"
   },
   "source": [
    "## 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mn3BhYVohcPI"
   },
   "outputs": [],
   "source": [
    "# 모든 행이 화면에 표시되도록 설정합니다.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 파일 로드\n",
    "file_path = 'LC_Data_Cleaned_0829.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 테스트 런일 경우, 데이터 크기 줄이기\n",
    "if is_test:\n",
    "    df = df.sample(frac=0.01, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0DpvFHJoyh-"
   },
   "source": [
    "## 데이터 구조 훑어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "eIAt4-Y_oyh-",
    "outputId": "537e79ec-50cb-4a7f-d6f3-e25d5f9b6667"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>loan_status_encoded</th>\n",
       "      <th>year</th>\n",
       "      <th>tbond_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>36.28</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82804.0</td>\n",
       "      <td>61559.0</td>\n",
       "      <td>22800.0</td>\n",
       "      <td>51304.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1199</td>\n",
       "      <td>166.05</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55620.0</td>\n",
       "      <td>18537.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>12955.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22975.0</td>\n",
       "      <td>22975.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>489.85</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203958.0</td>\n",
       "      <td>72241.0</td>\n",
       "      <td>21200.0</td>\n",
       "      <td>79120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.016416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14825.0</td>\n",
       "      <td>14825.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>380.91</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63937.0</td>\n",
       "      <td>26644.0</td>\n",
       "      <td>15300.0</td>\n",
       "      <td>34437.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.011721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0762</td>\n",
       "      <td>311.62</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>85000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350965.0</td>\n",
       "      <td>107907.0</td>\n",
       "      <td>47797.0</td>\n",
       "      <td>100380.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  funded_amnt  term  int_rate  installment grade sub_grade  \\\n",
       "0     1000.0       1000.0     0    0.1824        36.28     D        D5   \n",
       "1     5000.0       5000.0     0    0.1199       166.05     B        B3   \n",
       "2    22975.0      22975.0     1    0.1015       489.85     B        B2   \n",
       "3    14825.0      14825.0     1    0.1855       380.91     D        D2   \n",
       "4    10000.0      10000.0     0    0.0762       311.62     A        A3   \n",
       "\n",
       "   emp_length  home_ownership  annual_inc  ...  percent_bc_gt_75  \\\n",
       "0        10.0               1     70000.0  ...             100.0   \n",
       "1        10.0               0     37000.0  ...              80.0   \n",
       "2         7.0               0     81000.0  ...              75.0   \n",
       "3        10.0               1     37000.0  ...              40.0   \n",
       "4        10.0               0     85000.0  ...              25.0   \n",
       "\n",
       "  pub_rec_bankruptcies tax_liens tot_hi_cred_lim total_bal_ex_mort  \\\n",
       "0                  0.0       1.0         82804.0           61559.0   \n",
       "1                  0.0       0.0         55620.0           18537.0   \n",
       "2                  0.0       0.0        203958.0           72241.0   \n",
       "3                  0.0       0.0         63937.0           26644.0   \n",
       "4                  0.0       0.0        350965.0          107907.0   \n",
       "\n",
       "   total_bc_limit  total_il_high_credit_limit  loan_status_encoded  year  \\\n",
       "0         22800.0                     51304.0                    0  2014   \n",
       "1          8000.0                     12955.0                    0  2014   \n",
       "2         21200.0                     79120.0                    0  2014   \n",
       "3         15300.0                     34437.0                    1  2013   \n",
       "4         47797.0                    100380.0                    1  2013   \n",
       "\n",
       "   tbond_int  \n",
       "0   0.001211  \n",
       "1   0.001211  \n",
       "2   0.016416  \n",
       "3   0.011721  \n",
       "4   0.001312  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxjdTP4gKApM"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "L42IMY2hKSEO",
    "outputId": "c428f136-6ace-4ed8-f5fe-b90c4949fd50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(222028, 59)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 df 에 있는 칼럼 중에서, drop_list 및 post_list의 칼럼 제거\n",
    "drop_list = list(set(df.columns) & set(drop_list + post_list))\n",
    "\n",
    "# 불필요한 변수 Drop\n",
    "df = df.drop(columns = drop_list)\n",
    "\n",
    "# 결측치 처리\n",
    "df = df.fillna(0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWIX34Exoyim"
   },
   "source": [
    "# 모델 선택과 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTRHX9M5oyiA"
   },
   "source": [
    "## 테스트 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PrhJ2zzMKApN"
   },
   "outputs": [],
   "source": [
    "# Torch 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if not is_test else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train distribution: loan_status_encoded\n",
      "0    147586\n",
      "1     30036\n",
      "Name: count, dtype: int64\n",
      "177622 44406 177622 44406\n",
      "Resampled y_train distribution: loan_status_encoded\n",
      "0    30036\n",
      "1    30036\n",
      "Name: count, dtype: int64\n",
      "60072 44406 60072 44406\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 언더샘플링 객체 생성\n",
    "print(f\"Original y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# 트레인 데이터셋에 대해 언더샘플링 수행\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Resampled y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수 선정\n",
    "num_list = list(set(X_train.columns) ^ set(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder 생성 및 학습 데이터에 적합\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oneHotEncoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "oneHotEncoder.fit(X_train[cat_list])\n",
    "\n",
    "# 학습 데이터에 인코딩 적용\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_train[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# 테스트 데이터에 인코딩 적용\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_test[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 원래 데이터프레임과 병합\n",
    "X_train = X_train.drop(cat_list, axis=1).join(X_train_encoded)\n",
    "X_test = X_test.drop(cat_list, axis=1).join(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 수치형 변수 찾기\n",
    "X_train[num_list] = scaler.fit_transform(X_train[num_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test[num_list] = scaler.transform(X_test[num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60072, 119)\n",
      "(60072,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvlhFSTgKApN"
   },
   "source": [
    "## SSAE 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GhT0RT1sKApN"
   },
   "outputs": [],
   "source": [
    "# SSAE 모델 정의\n",
    "class DenoisingSSAE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DenoisingSSAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # input_dim이 실제 데이터의 feature 수와 일치해야 함\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)  # output_dim도 input_dim과 일치해야 함\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "# Noise 추가 함수\n",
    "def add_noise(data, noise_factor=0.2):\n",
    "    noise = noise_factor * np.random.randn(*data.shape)\n",
    "    noisy_data = data + noise\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.)\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "# Encoding 목표 설정\n",
    "encoding_target = list(set(X_train.columns) ^ set(feature_list))\n",
    "\n",
    "# 노이즈가 추가\n",
    "X_train_noisy_np = add_noise(X_train[encoding_target]).to_numpy()\n",
    "X_train_np = X_train[encoding_target].to_numpy()\n",
    "          \n",
    "input_dim = X_train_np.shape[1]  # X_train의 feature 수\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 0.4314\n",
      "Epoch [10/10000], Loss: 0.3744\n",
      "Epoch [20/10000], Loss: 0.3456\n",
      "Epoch [30/10000], Loss: 0.3189\n",
      "Epoch [40/10000], Loss: 0.3069\n",
      "Epoch [50/10000], Loss: 0.2979\n",
      "Epoch [60/10000], Loss: 0.2796\n",
      "Epoch [70/10000], Loss: 0.2762\n",
      "Epoch [80/10000], Loss: 0.2678\n",
      "Epoch [90/10000], Loss: 0.2612\n",
      "Epoch [100/10000], Loss: 0.2579\n",
      "Epoch [110/10000], Loss: 0.2530\n",
      "Epoch [120/10000], Loss: 0.2451\n",
      "Epoch [130/10000], Loss: 0.2404\n",
      "Epoch [140/10000], Loss: 0.2363\n",
      "Epoch [150/10000], Loss: 0.2327\n",
      "Epoch [160/10000], Loss: 0.2321\n",
      "Epoch [170/10000], Loss: 0.2283\n",
      "Epoch [180/10000], Loss: 0.2263\n",
      "Epoch [190/10000], Loss: 0.2267\n",
      "Epoch [200/10000], Loss: 0.2227\n",
      "Epoch [210/10000], Loss: 0.2210\n",
      "Epoch [220/10000], Loss: 0.2195\n",
      "Epoch [230/10000], Loss: 0.2208\n",
      "Epoch [240/10000], Loss: 0.2163\n",
      "Epoch [250/10000], Loss: 0.2146\n",
      "Epoch [260/10000], Loss: 0.2137\n",
      "Epoch [270/10000], Loss: 0.2086\n",
      "Epoch [280/10000], Loss: 0.2162\n",
      "Epoch [290/10000], Loss: 0.2066\n",
      "Epoch [300/10000], Loss: 0.2047\n",
      "Epoch [310/10000], Loss: 0.2032\n",
      "Epoch [320/10000], Loss: 0.2068\n",
      "Epoch [330/10000], Loss: 0.2058\n",
      "Epoch [340/10000], Loss: 0.2036\n",
      "Epoch [350/10000], Loss: 0.2012\n",
      "Epoch [360/10000], Loss: 0.2007\n",
      "Epoch [370/10000], Loss: 0.2009\n",
      "Epoch [380/10000], Loss: 0.2001\n",
      "Epoch [390/10000], Loss: 0.1997\n",
      "Epoch [400/10000], Loss: 0.1988\n",
      "Epoch [410/10000], Loss: 0.1983\n",
      "Epoch [420/10000], Loss: 0.1982\n",
      "Epoch [430/10000], Loss: 0.1977\n",
      "Epoch [440/10000], Loss: 0.1969\n",
      "Epoch [450/10000], Loss: 0.1964\n",
      "Epoch [460/10000], Loss: 0.1966\n",
      "Epoch [470/10000], Loss: 0.1958\n",
      "Epoch [480/10000], Loss: 0.1977\n",
      "Epoch [490/10000], Loss: 0.1952\n",
      "Epoch [500/10000], Loss: 0.1941\n",
      "Epoch [510/10000], Loss: 0.1939\n",
      "Epoch [520/10000], Loss: 0.1948\n",
      "Epoch [530/10000], Loss: 0.1941\n",
      "Epoch [540/10000], Loss: 0.1932\n",
      "Epoch [550/10000], Loss: 0.1930\n",
      "Epoch [560/10000], Loss: 0.1948\n",
      "Epoch [570/10000], Loss: 0.1932\n",
      "Epoch [580/10000], Loss: 0.1927\n",
      "Epoch [590/10000], Loss: 0.1921\n",
      "Epoch [600/10000], Loss: 0.1931\n",
      "Epoch [610/10000], Loss: 0.1925\n",
      "Epoch [620/10000], Loss: 0.1915\n",
      "Epoch [630/10000], Loss: 0.1927\n",
      "Epoch [640/10000], Loss: 0.1915\n",
      "Epoch [650/10000], Loss: 0.1914\n",
      "Epoch [660/10000], Loss: 0.1922\n",
      "Epoch [670/10000], Loss: 0.1911\n",
      "Epoch [680/10000], Loss: 0.1924\n",
      "Epoch [690/10000], Loss: 0.1910\n",
      "Epoch [700/10000], Loss: 0.1906\n",
      "Epoch [710/10000], Loss: 0.1904\n",
      "Epoch [720/10000], Loss: 0.1922\n",
      "Epoch [730/10000], Loss: 0.1904\n",
      "Epoch [740/10000], Loss: 0.1903\n",
      "Epoch [750/10000], Loss: 0.1912\n",
      "Epoch [760/10000], Loss: 0.1905\n",
      "Epoch [770/10000], Loss: 0.1897\n",
      "Epoch [780/10000], Loss: 0.1893\n",
      "Epoch [790/10000], Loss: 0.1910\n",
      "Epoch [800/10000], Loss: 0.1899\n",
      "Epoch [810/10000], Loss: 0.1892\n",
      "Epoch [820/10000], Loss: 0.1892\n",
      "Epoch [830/10000], Loss: 0.1894\n",
      "Epoch [840/10000], Loss: 0.1892\n",
      "Epoch [850/10000], Loss: 0.1884\n",
      "Epoch [860/10000], Loss: 0.1885\n",
      "Epoch [870/10000], Loss: 0.1892\n",
      "Epoch [880/10000], Loss: 0.1882\n",
      "Epoch [890/10000], Loss: 0.1879\n",
      "Epoch [900/10000], Loss: 0.1893\n",
      "Epoch [910/10000], Loss: 0.1884\n",
      "Epoch [920/10000], Loss: 0.1871\n",
      "Epoch [930/10000], Loss: 0.1873\n",
      "Epoch [940/10000], Loss: 0.1875\n",
      "Epoch [950/10000], Loss: 0.1871\n",
      "Epoch [960/10000], Loss: 0.1871\n",
      "Epoch [970/10000], Loss: 0.1870\n",
      "Epoch [980/10000], Loss: 0.1870\n",
      "Epoch [990/10000], Loss: 0.1868\n",
      "Epoch [1000/10000], Loss: 0.1867\n",
      "Epoch [1010/10000], Loss: 0.1859\n",
      "Epoch [1020/10000], Loss: 0.1860\n",
      "Epoch [1030/10000], Loss: 0.1866\n",
      "Epoch [1040/10000], Loss: 0.1855\n",
      "Epoch [1050/10000], Loss: 0.1856\n",
      "Epoch [1060/10000], Loss: 0.1857\n",
      "Epoch [1070/10000], Loss: 0.1849\n",
      "Epoch [1080/10000], Loss: 0.1842\n",
      "Epoch [1090/10000], Loss: 0.1870\n",
      "Epoch [1100/10000], Loss: 0.1852\n",
      "Epoch [1110/10000], Loss: 0.1843\n",
      "Epoch [1120/10000], Loss: 0.1836\n",
      "Epoch [1130/10000], Loss: 0.1871\n",
      "Epoch [1140/10000], Loss: 0.1844\n",
      "Epoch [1150/10000], Loss: 0.1841\n",
      "Epoch [1160/10000], Loss: 0.1848\n",
      "Epoch [1170/10000], Loss: 0.1836\n",
      "Epoch [1180/10000], Loss: 0.1832\n",
      "Epoch [1190/10000], Loss: 0.1839\n",
      "Epoch [1200/10000], Loss: 0.1831\n",
      "Epoch [1210/10000], Loss: 0.1832\n",
      "Epoch [1220/10000], Loss: 0.1830\n",
      "Epoch [1230/10000], Loss: 0.1843\n",
      "Epoch [1240/10000], Loss: 0.1837\n",
      "Epoch [1250/10000], Loss: 0.1832\n",
      "Epoch [1260/10000], Loss: 0.1823\n",
      "Epoch [1270/10000], Loss: 0.1822\n",
      "Epoch [1280/10000], Loss: 0.1831\n",
      "Epoch [1290/10000], Loss: 0.1840\n",
      "Epoch [1300/10000], Loss: 0.1826\n",
      "Epoch [1310/10000], Loss: 0.1821\n",
      "Epoch [1320/10000], Loss: 0.1824\n",
      "Epoch [1330/10000], Loss: 0.1818\n",
      "Epoch [1340/10000], Loss: 0.1819\n",
      "Epoch [1350/10000], Loss: 0.1857\n",
      "Epoch [1360/10000], Loss: 0.1835\n",
      "Epoch [1370/10000], Loss: 0.1826\n",
      "Epoch [1380/10000], Loss: 0.1819\n",
      "Epoch [1390/10000], Loss: 0.1830\n",
      "Epoch [1400/10000], Loss: 0.1813\n",
      "Epoch [1410/10000], Loss: 0.1812\n",
      "Epoch [1420/10000], Loss: 0.1814\n",
      "Epoch [1430/10000], Loss: 0.1815\n",
      "Epoch [1440/10000], Loss: 0.1814\n",
      "Epoch [1450/10000], Loss: 0.1814\n",
      "Epoch [1460/10000], Loss: 0.1818\n",
      "Epoch [1470/10000], Loss: 0.1816\n",
      "Epoch [1480/10000], Loss: 0.1809\n",
      "Epoch [1490/10000], Loss: 0.1806\n",
      "Epoch [1500/10000], Loss: 0.1815\n",
      "Epoch [1510/10000], Loss: 0.1806\n",
      "Epoch [1520/10000], Loss: 0.1813\n",
      "Epoch [1530/10000], Loss: 0.1805\n",
      "Epoch [1540/10000], Loss: 0.1803\n",
      "Epoch [1550/10000], Loss: 0.1820\n",
      "Epoch [1560/10000], Loss: 0.1807\n",
      "Epoch [1570/10000], Loss: 0.1808\n",
      "Epoch [1580/10000], Loss: 0.1815\n",
      "Epoch [1590/10000], Loss: 0.1807\n",
      "Epoch [1600/10000], Loss: 0.1805\n",
      "Epoch [1610/10000], Loss: 0.1803\n",
      "Epoch [1620/10000], Loss: 0.1804\n",
      "Epoch [1630/10000], Loss: 0.1799\n",
      "Epoch [1640/10000], Loss: 0.1795\n",
      "Epoch [1650/10000], Loss: 0.1795\n",
      "Epoch [1660/10000], Loss: 0.1804\n",
      "Epoch [1670/10000], Loss: 0.1802\n",
      "Epoch [1680/10000], Loss: 0.1793\n",
      "Epoch [1690/10000], Loss: 0.1809\n",
      "Epoch [1700/10000], Loss: 0.1810\n",
      "Epoch [1710/10000], Loss: 0.1797\n",
      "Epoch [1720/10000], Loss: 0.1790\n",
      "Epoch [1730/10000], Loss: 0.1791\n",
      "Epoch [1740/10000], Loss: 0.1809\n",
      "Epoch [1750/10000], Loss: 0.1796\n",
      "Epoch [1760/10000], Loss: 0.1792\n",
      "Epoch [1770/10000], Loss: 0.1798\n",
      "Epoch [1780/10000], Loss: 0.1785\n",
      "Epoch [1790/10000], Loss: 0.1789\n",
      "Epoch [1800/10000], Loss: 0.1809\n",
      "Epoch [1810/10000], Loss: 0.1786\n",
      "Epoch [1820/10000], Loss: 0.1786\n",
      "Epoch [1830/10000], Loss: 0.1784\n",
      "Epoch [1840/10000], Loss: 0.1795\n",
      "Epoch [1850/10000], Loss: 0.1795\n",
      "Epoch [1860/10000], Loss: 0.1794\n",
      "Epoch [1870/10000], Loss: 0.1780\n",
      "Epoch [1880/10000], Loss: 0.1780\n",
      "Epoch [1890/10000], Loss: 0.1787\n",
      "Epoch [1900/10000], Loss: 0.1779\n",
      "Epoch [1910/10000], Loss: 0.1779\n",
      "Epoch [1920/10000], Loss: 0.1775\n",
      "Epoch [1930/10000], Loss: 0.1778\n",
      "Epoch [1940/10000], Loss: 0.1777\n",
      "Epoch [1950/10000], Loss: 0.1782\n",
      "Epoch [1960/10000], Loss: 0.1770\n",
      "Epoch [1970/10000], Loss: 0.1781\n",
      "Epoch [1980/10000], Loss: 0.1773\n",
      "Epoch [1990/10000], Loss: 0.1784\n",
      "Epoch [2000/10000], Loss: 0.1779\n",
      "Epoch [2010/10000], Loss: 0.1769\n",
      "Epoch [2020/10000], Loss: 0.1780\n",
      "Epoch [2030/10000], Loss: 0.1780\n",
      "Epoch [2040/10000], Loss: 0.1772\n",
      "Epoch [2050/10000], Loss: 0.1767\n",
      "Epoch [2060/10000], Loss: 0.1774\n",
      "Epoch [2070/10000], Loss: 0.1775\n",
      "Epoch [2080/10000], Loss: 0.1771\n",
      "Epoch [2090/10000], Loss: 0.1774\n",
      "Epoch [2100/10000], Loss: 0.1777\n",
      "Epoch [2110/10000], Loss: 0.1760\n",
      "Epoch [2120/10000], Loss: 0.1763\n",
      "Epoch [2130/10000], Loss: 0.1763\n",
      "Epoch [2140/10000], Loss: 0.1770\n",
      "Epoch [2150/10000], Loss: 0.1757\n",
      "Epoch [2160/10000], Loss: 0.1756\n",
      "Epoch [2170/10000], Loss: 0.1762\n",
      "Epoch [2180/10000], Loss: 0.1769\n",
      "Epoch [2190/10000], Loss: 0.1760\n",
      "Epoch [2200/10000], Loss: 0.1757\n",
      "Epoch [2210/10000], Loss: 0.1757\n",
      "Epoch [2220/10000], Loss: 0.1761\n",
      "Epoch [2230/10000], Loss: 0.1755\n",
      "Epoch [2240/10000], Loss: 0.1753\n",
      "Epoch [2250/10000], Loss: 0.1758\n",
      "Epoch [2260/10000], Loss: 0.1741\n",
      "Epoch [2270/10000], Loss: 0.1759\n",
      "Epoch [2280/10000], Loss: 0.1755\n",
      "Epoch [2290/10000], Loss: 0.1741\n",
      "Epoch [2300/10000], Loss: 0.1741\n",
      "Epoch [2310/10000], Loss: 0.1781\n",
      "Epoch [2320/10000], Loss: 0.1748\n",
      "Epoch [2330/10000], Loss: 0.1745\n",
      "Epoch [2340/10000], Loss: 0.1742\n",
      "Epoch [2350/10000], Loss: 0.1752\n",
      "Epoch [2360/10000], Loss: 0.1734\n",
      "Epoch [2370/10000], Loss: 0.1739\n",
      "Epoch [2380/10000], Loss: 0.1756\n",
      "Epoch [2390/10000], Loss: 0.1740\n",
      "Epoch [2400/10000], Loss: 0.1734\n",
      "Epoch [2410/10000], Loss: 0.1738\n",
      "Epoch [2420/10000], Loss: 0.1754\n",
      "Epoch [2430/10000], Loss: 0.1752\n",
      "Epoch [2440/10000], Loss: 0.1746\n",
      "Epoch [2450/10000], Loss: 0.1745\n",
      "Epoch [2460/10000], Loss: 0.1741\n",
      "Epoch [2470/10000], Loss: 0.1725\n",
      "Epoch [2480/10000], Loss: 0.1731\n",
      "Epoch [2490/10000], Loss: 0.1736\n",
      "Epoch [2500/10000], Loss: 0.1732\n",
      "Epoch [2510/10000], Loss: 0.1731\n",
      "Epoch [2520/10000], Loss: 0.1741\n",
      "Epoch [2530/10000], Loss: 0.1731\n",
      "Epoch [2540/10000], Loss: 0.1746\n",
      "Epoch [2550/10000], Loss: 0.1724\n",
      "Epoch [2560/10000], Loss: 0.1725\n",
      "Epoch [2570/10000], Loss: 0.1722\n",
      "Epoch [2580/10000], Loss: 0.1735\n",
      "Epoch [2590/10000], Loss: 0.1720\n",
      "Epoch [2600/10000], Loss: 0.1732\n",
      "Epoch [2610/10000], Loss: 0.1734\n",
      "Epoch [2620/10000], Loss: 0.1718\n",
      "Epoch [2630/10000], Loss: 0.1718\n",
      "Epoch [2640/10000], Loss: 0.1714\n",
      "Epoch [2650/10000], Loss: 0.1721\n",
      "Epoch [2660/10000], Loss: 0.1712\n",
      "Epoch [2670/10000], Loss: 0.1719\n",
      "Epoch [2680/10000], Loss: 0.1712\n",
      "Epoch [2690/10000], Loss: 0.1714\n",
      "Epoch [2700/10000], Loss: 0.1704\n",
      "Epoch [2710/10000], Loss: 0.1730\n",
      "Epoch [2720/10000], Loss: 0.1716\n",
      "Epoch [2730/10000], Loss: 0.1713\n",
      "Epoch [2740/10000], Loss: 0.1711\n",
      "Epoch [2750/10000], Loss: 0.1716\n",
      "Epoch [2760/10000], Loss: 0.1702\n",
      "Epoch [2770/10000], Loss: 0.1726\n",
      "Epoch [2780/10000], Loss: 0.1700\n",
      "Epoch [2790/10000], Loss: 0.1717\n",
      "Epoch [2800/10000], Loss: 0.1704\n",
      "Epoch [2810/10000], Loss: 0.1710\n",
      "Epoch [2820/10000], Loss: 0.1705\n",
      "Epoch [2830/10000], Loss: 0.1701\n",
      "Epoch [2840/10000], Loss: 0.1709\n",
      "Epoch [2850/10000], Loss: 0.1723\n",
      "Epoch [2860/10000], Loss: 0.1704\n",
      "Epoch [2870/10000], Loss: 0.1701\n",
      "Epoch [2880/10000], Loss: 0.1707\n",
      "Epoch [2890/10000], Loss: 0.1714\n",
      "Epoch [2900/10000], Loss: 0.1699\n",
      "Epoch [2910/10000], Loss: 0.1721\n",
      "Epoch [2920/10000], Loss: 0.1722\n",
      "Epoch [2930/10000], Loss: 0.1703\n",
      "Epoch [2940/10000], Loss: 0.1698\n",
      "Epoch [2950/10000], Loss: 0.1698\n",
      "Epoch [2960/10000], Loss: 0.1700\n",
      "Epoch [2970/10000], Loss: 0.1699\n",
      "Epoch [2980/10000], Loss: 0.1694\n",
      "Epoch [2990/10000], Loss: 0.1710\n",
      "Epoch [3000/10000], Loss: 0.1704\n",
      "Epoch [3010/10000], Loss: 0.1698\n",
      "Epoch [3020/10000], Loss: 0.1708\n",
      "Epoch [3030/10000], Loss: 0.1698\n",
      "Epoch [3040/10000], Loss: 0.1695\n",
      "Epoch [3050/10000], Loss: 0.1702\n",
      "Epoch [3060/10000], Loss: 0.1715\n",
      "Epoch [3070/10000], Loss: 0.1710\n",
      "Epoch [3080/10000], Loss: 0.1691\n",
      "Epoch [3090/10000], Loss: 0.1693\n",
      "Epoch [3100/10000], Loss: 0.1690\n",
      "Epoch [3110/10000], Loss: 0.1694\n",
      "Epoch [3120/10000], Loss: 0.1707\n",
      "Epoch [3130/10000], Loss: 0.1686\n",
      "Epoch [3140/10000], Loss: 0.1698\n",
      "Epoch [3150/10000], Loss: 0.1685\n",
      "Epoch [3160/10000], Loss: 0.1690\n",
      "Epoch [3170/10000], Loss: 0.1692\n",
      "Epoch [3180/10000], Loss: 0.1727\n",
      "Epoch [3190/10000], Loss: 0.1732\n",
      "Epoch [3200/10000], Loss: 0.1690\n",
      "Epoch [3210/10000], Loss: 0.1686\n",
      "Epoch [3220/10000], Loss: 0.1683\n",
      "Epoch [3230/10000], Loss: 0.1678\n",
      "Epoch [3240/10000], Loss: 0.1688\n",
      "Epoch [3250/10000], Loss: 0.1704\n",
      "Epoch [3260/10000], Loss: 0.1718\n",
      "Epoch [3270/10000], Loss: 0.1696\n",
      "Epoch [3280/10000], Loss: 0.1685\n",
      "Epoch [3290/10000], Loss: 0.1693\n",
      "Epoch [3300/10000], Loss: 0.1690\n",
      "Epoch [3310/10000], Loss: 0.1692\n",
      "Epoch [3320/10000], Loss: 0.1711\n",
      "Epoch [3330/10000], Loss: 0.1690\n",
      "Epoch [3340/10000], Loss: 0.1684\n",
      "Epoch [3350/10000], Loss: 0.1678\n",
      "Epoch [3360/10000], Loss: 0.1682\n",
      "Epoch [3370/10000], Loss: 0.1673\n",
      "Epoch [3380/10000], Loss: 0.1674\n",
      "Epoch [3390/10000], Loss: 0.1668\n",
      "Epoch [3400/10000], Loss: 0.1670\n",
      "Epoch [3410/10000], Loss: 0.1693\n",
      "Epoch [3420/10000], Loss: 0.1708\n",
      "Epoch [3430/10000], Loss: 0.1687\n",
      "Epoch [3440/10000], Loss: 0.1675\n",
      "Epoch [3450/10000], Loss: 0.1668\n",
      "Epoch [3460/10000], Loss: 0.1668\n",
      "Epoch [3470/10000], Loss: 0.1685\n",
      "Epoch [3480/10000], Loss: 0.1665\n",
      "Epoch [3490/10000], Loss: 0.1676\n",
      "Epoch [3500/10000], Loss: 0.1672\n",
      "Epoch [3510/10000], Loss: 0.1694\n",
      "Epoch [3520/10000], Loss: 0.1681\n",
      "Epoch [3530/10000], Loss: 0.1683\n",
      "Epoch [3540/10000], Loss: 0.1682\n",
      "Epoch [3550/10000], Loss: 0.1684\n",
      "Epoch [3560/10000], Loss: 0.1693\n",
      "Epoch [3570/10000], Loss: 0.1689\n",
      "Epoch [3580/10000], Loss: 0.1671\n",
      "Epoch [3590/10000], Loss: 0.1681\n",
      "Epoch [3600/10000], Loss: 0.1671\n",
      "Epoch [3610/10000], Loss: 0.1674\n",
      "Epoch [3620/10000], Loss: 0.1665\n",
      "Epoch [3630/10000], Loss: 0.1677\n",
      "Epoch [3640/10000], Loss: 0.1675\n",
      "Epoch [3650/10000], Loss: 0.1666\n",
      "Epoch [3660/10000], Loss: 0.1663\n",
      "Epoch [3670/10000], Loss: 0.1665\n",
      "Epoch [3680/10000], Loss: 0.1663\n",
      "Epoch [3690/10000], Loss: 0.1669\n",
      "Epoch [3700/10000], Loss: 0.1666\n",
      "Epoch [3710/10000], Loss: 0.1686\n",
      "Epoch [3720/10000], Loss: 0.1679\n",
      "Epoch [3730/10000], Loss: 0.1665\n",
      "Epoch [3740/10000], Loss: 0.1672\n",
      "Epoch [3750/10000], Loss: 0.1677\n",
      "Epoch [3760/10000], Loss: 0.1671\n",
      "Epoch [3770/10000], Loss: 0.1670\n",
      "Epoch [3780/10000], Loss: 0.1660\n",
      "Epoch [3790/10000], Loss: 0.1668\n",
      "Epoch [3800/10000], Loss: 0.1664\n",
      "Epoch [3810/10000], Loss: 0.1673\n",
      "Epoch [3820/10000], Loss: 0.1693\n",
      "Epoch [3830/10000], Loss: 0.1682\n",
      "Epoch [3840/10000], Loss: 0.1666\n",
      "Epoch [3850/10000], Loss: 0.1660\n",
      "Epoch [3860/10000], Loss: 0.1666\n",
      "Epoch [3870/10000], Loss: 0.1669\n",
      "Epoch [3880/10000], Loss: 0.1663\n",
      "Epoch [3890/10000], Loss: 0.1658\n",
      "Epoch [3900/10000], Loss: 0.1663\n",
      "Epoch [3910/10000], Loss: 0.1662\n",
      "Epoch [3920/10000], Loss: 0.1672\n",
      "Epoch [3930/10000], Loss: 0.1668\n",
      "Epoch [3940/10000], Loss: 0.1660\n",
      "Epoch [3950/10000], Loss: 0.1663\n",
      "Epoch [3960/10000], Loss: 0.1664\n",
      "Epoch [3970/10000], Loss: 0.1660\n",
      "Epoch [3980/10000], Loss: 0.1662\n",
      "Epoch [3990/10000], Loss: 0.1654\n",
      "Epoch [4000/10000], Loss: 0.1653\n",
      "Epoch [4010/10000], Loss: 0.1651\n",
      "Epoch [4020/10000], Loss: 0.1656\n",
      "Epoch [4030/10000], Loss: 0.1664\n",
      "Epoch [4040/10000], Loss: 0.1648\n",
      "Epoch [4050/10000], Loss: 0.1656\n",
      "Epoch [4060/10000], Loss: 0.1667\n",
      "Epoch [4070/10000], Loss: 0.1661\n",
      "Epoch [4080/10000], Loss: 0.1657\n",
      "Epoch [4090/10000], Loss: 0.1647\n",
      "Epoch [4100/10000], Loss: 0.1642\n",
      "Epoch [4110/10000], Loss: 0.1650\n",
      "Epoch [4120/10000], Loss: 0.1667\n",
      "Epoch [4130/10000], Loss: 0.1661\n",
      "Epoch [4140/10000], Loss: 0.1668\n",
      "Epoch [4150/10000], Loss: 0.1654\n",
      "Epoch [4160/10000], Loss: 0.1648\n",
      "Epoch [4170/10000], Loss: 0.1642\n",
      "Epoch [4180/10000], Loss: 0.1651\n",
      "Epoch [4190/10000], Loss: 0.1651\n",
      "Epoch [4200/10000], Loss: 0.1677\n",
      "Epoch [4210/10000], Loss: 0.1712\n",
      "Epoch [4220/10000], Loss: 0.1719\n",
      "Epoch [4230/10000], Loss: 0.1682\n",
      "Epoch [4240/10000], Loss: 0.1662\n",
      "Epoch [4250/10000], Loss: 0.1652\n",
      "Epoch [4260/10000], Loss: 0.1650\n",
      "Epoch [4270/10000], Loss: 0.1655\n",
      "Epoch [4280/10000], Loss: 0.1661\n",
      "Epoch [4290/10000], Loss: 0.1650\n",
      "Epoch [4300/10000], Loss: 0.1654\n",
      "Epoch [4310/10000], Loss: 0.1656\n",
      "Epoch [4320/10000], Loss: 0.1667\n",
      "Epoch [4330/10000], Loss: 0.1653\n",
      "Epoch [4340/10000], Loss: 0.1648\n",
      "Epoch [4350/10000], Loss: 0.1647\n",
      "Epoch [4360/10000], Loss: 0.1663\n",
      "Epoch [4370/10000], Loss: 0.1655\n",
      "Epoch [4380/10000], Loss: 0.1649\n",
      "Epoch [4390/10000], Loss: 0.1651\n",
      "Epoch [4400/10000], Loss: 0.1654\n",
      "Epoch [4410/10000], Loss: 0.1657\n",
      "Epoch [4420/10000], Loss: 0.1656\n",
      "Epoch [4430/10000], Loss: 0.1655\n",
      "Epoch [4440/10000], Loss: 0.1649\n",
      "Epoch [4450/10000], Loss: 0.1647\n",
      "Epoch [4460/10000], Loss: 0.1649\n",
      "Epoch [4470/10000], Loss: 0.1650\n",
      "Epoch [4480/10000], Loss: 0.1668\n",
      "Epoch [4490/10000], Loss: 0.1672\n",
      "Epoch [4500/10000], Loss: 0.1642\n",
      "Epoch [4510/10000], Loss: 0.1646\n",
      "Epoch [4520/10000], Loss: 0.1650\n",
      "Epoch [4530/10000], Loss: 0.1666\n",
      "Epoch [4540/10000], Loss: 0.1644\n",
      "Epoch [4550/10000], Loss: 0.1650\n",
      "Epoch [4560/10000], Loss: 0.1657\n",
      "Epoch [4570/10000], Loss: 0.1644\n",
      "Epoch [4580/10000], Loss: 0.1655\n",
      "Epoch [4590/10000], Loss: 0.1648\n",
      "Epoch [4600/10000], Loss: 0.1643\n",
      "Epoch [4610/10000], Loss: 0.1638\n",
      "Epoch [4620/10000], Loss: 0.1654\n",
      "Epoch [4630/10000], Loss: 0.1643\n",
      "Epoch [4640/10000], Loss: 0.1652\n",
      "Epoch [4650/10000], Loss: 0.1647\n",
      "Epoch [4660/10000], Loss: 0.1647\n",
      "Epoch [4670/10000], Loss: 0.1677\n",
      "Epoch [4680/10000], Loss: 0.1641\n",
      "Epoch [4690/10000], Loss: 0.1635\n",
      "Epoch [4700/10000], Loss: 0.1638\n",
      "Epoch [4710/10000], Loss: 0.1638\n",
      "Epoch [4720/10000], Loss: 0.1637\n",
      "Epoch [4730/10000], Loss: 0.1671\n",
      "Epoch [4740/10000], Loss: 0.1650\n",
      "Epoch [4750/10000], Loss: 0.1651\n",
      "Epoch [4760/10000], Loss: 0.1657\n",
      "Epoch [4770/10000], Loss: 0.1644\n",
      "Epoch [4780/10000], Loss: 0.1637\n",
      "Epoch [4790/10000], Loss: 0.1629\n",
      "Epoch [4800/10000], Loss: 0.1635\n",
      "Epoch [4810/10000], Loss: 0.1636\n",
      "Epoch [4820/10000], Loss: 0.1657\n",
      "Epoch [4830/10000], Loss: 0.1659\n",
      "Epoch [4840/10000], Loss: 0.1635\n",
      "Epoch [4850/10000], Loss: 0.1632\n",
      "Epoch [4860/10000], Loss: 0.1632\n",
      "Epoch [4870/10000], Loss: 0.1639\n",
      "Epoch [4880/10000], Loss: 0.1639\n",
      "Epoch [4890/10000], Loss: 0.1647\n",
      "Epoch [4900/10000], Loss: 0.1641\n",
      "Epoch [4910/10000], Loss: 0.1642\n",
      "Epoch [4920/10000], Loss: 0.1635\n",
      "Epoch [4930/10000], Loss: 0.1646\n",
      "Epoch [4940/10000], Loss: 0.1649\n",
      "Epoch [4950/10000], Loss: 0.1643\n",
      "Epoch [4960/10000], Loss: 0.1635\n",
      "Epoch [4970/10000], Loss: 0.1708\n",
      "Epoch [4980/10000], Loss: 0.1671\n",
      "Epoch [4990/10000], Loss: 0.1662\n",
      "Epoch [5000/10000], Loss: 0.1642\n",
      "Epoch [5010/10000], Loss: 0.1633\n",
      "Epoch [5020/10000], Loss: 0.1628\n",
      "Epoch [5030/10000], Loss: 0.1624\n",
      "Epoch [5040/10000], Loss: 0.1623\n",
      "Epoch [5050/10000], Loss: 0.1626\n",
      "Epoch [5060/10000], Loss: 0.1637\n",
      "Epoch [5070/10000], Loss: 0.1633\n",
      "Epoch [5080/10000], Loss: 0.1629\n",
      "Epoch [5090/10000], Loss: 0.1630\n",
      "Epoch [5100/10000], Loss: 0.1644\n",
      "Epoch [5110/10000], Loss: 0.1633\n",
      "Epoch [5120/10000], Loss: 0.1628\n",
      "Epoch [5130/10000], Loss: 0.1626\n",
      "Epoch [5140/10000], Loss: 0.1640\n",
      "Epoch [5150/10000], Loss: 0.1630\n",
      "Epoch [5160/10000], Loss: 0.1631\n",
      "Epoch [5170/10000], Loss: 0.1629\n",
      "Epoch [5180/10000], Loss: 0.1628\n",
      "Epoch [5190/10000], Loss: 0.1627\n",
      "Epoch [5200/10000], Loss: 0.1643\n",
      "Epoch [5210/10000], Loss: 0.1635\n",
      "Epoch [5220/10000], Loss: 0.1626\n",
      "Epoch [5230/10000], Loss: 0.1634\n",
      "Epoch [5240/10000], Loss: 0.1639\n",
      "Epoch [5250/10000], Loss: 0.1648\n",
      "Epoch [5260/10000], Loss: 0.1638\n",
      "Epoch [5270/10000], Loss: 0.1631\n",
      "Epoch [5280/10000], Loss: 0.1625\n",
      "Epoch [5290/10000], Loss: 0.1622\n",
      "Epoch [5300/10000], Loss: 0.1615\n",
      "Epoch [5310/10000], Loss: 0.1621\n",
      "Epoch [5320/10000], Loss: 0.1637\n",
      "Epoch [5330/10000], Loss: 0.1634\n",
      "Epoch [5340/10000], Loss: 0.1630\n",
      "Epoch [5350/10000], Loss: 0.1623\n",
      "Epoch [5360/10000], Loss: 0.1632\n",
      "Epoch [5370/10000], Loss: 0.1640\n",
      "Epoch [5380/10000], Loss: 0.1623\n",
      "Epoch [5390/10000], Loss: 0.1626\n",
      "Epoch [5400/10000], Loss: 0.1625\n",
      "Epoch [5410/10000], Loss: 0.1636\n",
      "Epoch [5420/10000], Loss: 0.1621\n",
      "Epoch [5430/10000], Loss: 0.1629\n",
      "Epoch [5440/10000], Loss: 0.1620\n",
      "Epoch [5450/10000], Loss: 0.1633\n",
      "Epoch [5460/10000], Loss: 0.1623\n",
      "Epoch [5470/10000], Loss: 0.1626\n",
      "Epoch [5480/10000], Loss: 0.1620\n",
      "Epoch [5490/10000], Loss: 0.1633\n",
      "Epoch [5500/10000], Loss: 0.1626\n",
      "Epoch [5510/10000], Loss: 0.1617\n",
      "Epoch [5520/10000], Loss: 0.1614\n",
      "Epoch [5530/10000], Loss: 0.1622\n",
      "Epoch [5540/10000], Loss: 0.1627\n",
      "Epoch [5550/10000], Loss: 0.1624\n",
      "Epoch [5560/10000], Loss: 0.1630\n",
      "Epoch [5570/10000], Loss: 0.1626\n",
      "Epoch [5580/10000], Loss: 0.1621\n",
      "Epoch [5590/10000], Loss: 0.1618\n",
      "Epoch [5600/10000], Loss: 0.1628\n",
      "Epoch [5610/10000], Loss: 0.1632\n",
      "Epoch [5620/10000], Loss: 0.1626\n",
      "Epoch [5630/10000], Loss: 0.1628\n",
      "Epoch [5640/10000], Loss: 0.1624\n",
      "Epoch [5650/10000], Loss: 0.1619\n",
      "Epoch [5660/10000], Loss: 0.1614\n",
      "Epoch [5670/10000], Loss: 0.1615\n",
      "Epoch [5680/10000], Loss: 0.1631\n",
      "Epoch [5690/10000], Loss: 0.1621\n",
      "Epoch [5700/10000], Loss: 0.1624\n",
      "Epoch [5710/10000], Loss: 0.1628\n",
      "Epoch [5720/10000], Loss: 0.1610\n",
      "Epoch [5730/10000], Loss: 0.1613\n",
      "Epoch [5740/10000], Loss: 0.1648\n",
      "Epoch [5750/10000], Loss: 0.1617\n",
      "Epoch [5760/10000], Loss: 0.1619\n",
      "Epoch [5770/10000], Loss: 0.1634\n",
      "Epoch [5780/10000], Loss: 0.1624\n",
      "Epoch [5790/10000], Loss: 0.1616\n",
      "Epoch [5800/10000], Loss: 0.1617\n",
      "Epoch [5810/10000], Loss: 0.1624\n",
      "Epoch [5820/10000], Loss: 0.1621\n",
      "Epoch [5830/10000], Loss: 0.1624\n",
      "Epoch [5840/10000], Loss: 0.1629\n",
      "Epoch [5850/10000], Loss: 0.1617\n",
      "Epoch [5860/10000], Loss: 0.1624\n",
      "Epoch [5870/10000], Loss: 0.1623\n",
      "Epoch [5880/10000], Loss: 0.1617\n",
      "Epoch [5890/10000], Loss: 0.1620\n",
      "Epoch [5900/10000], Loss: 0.1624\n",
      "Epoch [5910/10000], Loss: 0.1620\n",
      "Epoch [5920/10000], Loss: 0.1612\n",
      "Epoch [5930/10000], Loss: 0.1624\n",
      "Epoch [5940/10000], Loss: 0.1615\n",
      "Epoch [5950/10000], Loss: 0.1613\n",
      "Epoch [5960/10000], Loss: 0.1614\n",
      "Epoch [5970/10000], Loss: 0.1618\n",
      "Epoch [5980/10000], Loss: 0.1615\n",
      "Epoch [5990/10000], Loss: 0.1606\n",
      "Epoch [6000/10000], Loss: 0.1614\n",
      "Epoch [6010/10000], Loss: 0.1630\n",
      "Epoch [6020/10000], Loss: 0.1625\n",
      "Epoch [6030/10000], Loss: 0.1614\n",
      "Epoch [6040/10000], Loss: 0.1613\n",
      "Epoch [6050/10000], Loss: 0.1633\n",
      "Epoch [6060/10000], Loss: 0.1617\n",
      "Epoch [6070/10000], Loss: 0.1610\n",
      "Epoch [6080/10000], Loss: 0.1622\n",
      "Epoch [6090/10000], Loss: 0.1612\n",
      "Epoch [6100/10000], Loss: 0.1629\n",
      "Epoch [6110/10000], Loss: 0.1620\n",
      "Epoch [6120/10000], Loss: 0.1619\n",
      "Epoch [6130/10000], Loss: 0.1610\n",
      "Epoch [6140/10000], Loss: 0.1611\n",
      "Epoch [6150/10000], Loss: 0.1609\n",
      "Epoch [6160/10000], Loss: 0.1613\n",
      "Epoch [6170/10000], Loss: 0.1609\n",
      "Epoch [6180/10000], Loss: 0.1615\n",
      "Epoch [6190/10000], Loss: 0.1609\n",
      "Epoch [6200/10000], Loss: 0.1592\n",
      "Epoch [6210/10000], Loss: 0.1586\n",
      "Epoch [6220/10000], Loss: 0.1592\n",
      "Epoch [6230/10000], Loss: 0.1584\n",
      "Epoch [6240/10000], Loss: 0.1595\n",
      "Epoch [6250/10000], Loss: 0.1590\n",
      "Epoch [6260/10000], Loss: 0.1584\n",
      "Epoch [6270/10000], Loss: 0.1578\n",
      "Epoch [6280/10000], Loss: 0.1576\n",
      "Epoch [6290/10000], Loss: 0.1578\n",
      "Epoch [6300/10000], Loss: 0.1590\n",
      "Epoch [6310/10000], Loss: 0.1588\n",
      "Epoch [6320/10000], Loss: 0.1589\n",
      "Epoch [6330/10000], Loss: 0.1583\n",
      "Epoch [6340/10000], Loss: 0.1579\n",
      "Epoch [6350/10000], Loss: 0.1590\n",
      "Epoch [6360/10000], Loss: 0.1578\n",
      "Epoch [6370/10000], Loss: 0.1584\n",
      "Epoch [6380/10000], Loss: 0.1601\n",
      "Epoch [6390/10000], Loss: 0.1593\n",
      "Epoch [6400/10000], Loss: 0.1588\n",
      "Epoch [6410/10000], Loss: 0.1587\n",
      "Epoch [6420/10000], Loss: 0.1581\n",
      "Epoch [6430/10000], Loss: 0.1582\n",
      "Epoch [6440/10000], Loss: 0.1602\n",
      "Epoch [6450/10000], Loss: 0.1578\n",
      "Epoch [6460/10000], Loss: 0.1576\n",
      "Epoch [6470/10000], Loss: 0.1581\n",
      "Epoch [6480/10000], Loss: 0.1582\n",
      "Epoch [6490/10000], Loss: 0.1576\n",
      "Epoch [6500/10000], Loss: 0.1582\n",
      "Epoch [6510/10000], Loss: 0.1580\n",
      "Epoch [6520/10000], Loss: 0.1580\n",
      "Epoch [6530/10000], Loss: 0.1591\n",
      "Epoch [6540/10000], Loss: 0.1597\n",
      "Epoch [6550/10000], Loss: 0.1588\n",
      "Epoch [6560/10000], Loss: 0.1576\n",
      "Epoch [6570/10000], Loss: 0.1574\n",
      "Epoch [6580/10000], Loss: 0.1578\n",
      "Epoch [6590/10000], Loss: 0.1571\n",
      "Epoch [6600/10000], Loss: 0.1587\n",
      "Epoch [6610/10000], Loss: 0.1579\n",
      "Epoch [6620/10000], Loss: 0.1586\n",
      "Epoch [6630/10000], Loss: 0.1596\n",
      "Epoch [6640/10000], Loss: 0.1576\n",
      "Epoch [6650/10000], Loss: 0.1583\n",
      "Epoch [6660/10000], Loss: 0.1605\n",
      "Epoch [6670/10000], Loss: 0.1597\n",
      "Epoch [6680/10000], Loss: 0.1576\n",
      "Epoch [6690/10000], Loss: 0.1581\n",
      "Epoch [6700/10000], Loss: 0.1571\n",
      "Epoch [6710/10000], Loss: 0.1583\n",
      "Epoch [6720/10000], Loss: 0.1576\n",
      "Epoch [6730/10000], Loss: 0.1583\n",
      "Epoch [6740/10000], Loss: 0.1595\n",
      "Epoch [6750/10000], Loss: 0.1585\n",
      "Epoch [6760/10000], Loss: 0.1574\n",
      "Epoch [6770/10000], Loss: 0.1602\n",
      "Epoch [6780/10000], Loss: 0.1581\n",
      "Epoch [6790/10000], Loss: 0.1588\n",
      "Epoch [6800/10000], Loss: 0.1586\n",
      "Epoch [6810/10000], Loss: 0.1574\n",
      "Epoch [6820/10000], Loss: 0.1586\n",
      "Epoch [6830/10000], Loss: 0.1593\n",
      "Epoch [6840/10000], Loss: 0.1575\n",
      "Epoch [6850/10000], Loss: 0.1581\n",
      "Epoch [6860/10000], Loss: 0.1574\n",
      "Epoch [6870/10000], Loss: 0.1571\n",
      "Epoch [6880/10000], Loss: 0.1565\n",
      "Epoch [6890/10000], Loss: 0.1578\n",
      "Epoch [6900/10000], Loss: 0.1590\n",
      "Epoch [6910/10000], Loss: 0.1590\n",
      "Epoch [6920/10000], Loss: 0.1572\n",
      "Epoch [6930/10000], Loss: 0.1575\n",
      "Epoch [6940/10000], Loss: 0.1573\n",
      "Epoch [6950/10000], Loss: 0.1580\n",
      "Epoch [6960/10000], Loss: 0.1570\n",
      "Epoch [6970/10000], Loss: 0.1576\n",
      "Epoch [6980/10000], Loss: 0.1579\n",
      "Epoch [6990/10000], Loss: 0.1580\n",
      "Epoch [7000/10000], Loss: 0.1569\n",
      "Epoch [7010/10000], Loss: 0.1583\n",
      "Epoch [7020/10000], Loss: 0.1590\n",
      "Epoch [7030/10000], Loss: 0.1574\n",
      "Epoch [7040/10000], Loss: 0.1571\n",
      "Epoch [7050/10000], Loss: 0.1582\n",
      "Epoch [7060/10000], Loss: 0.1566\n",
      "Epoch [7070/10000], Loss: 0.1571\n",
      "Epoch [7080/10000], Loss: 0.1572\n",
      "Epoch [7090/10000], Loss: 0.1574\n",
      "Epoch [7100/10000], Loss: 0.1571\n",
      "Epoch [7110/10000], Loss: 0.1576\n",
      "Epoch [7120/10000], Loss: 0.1572\n",
      "Epoch [7130/10000], Loss: 0.1589\n",
      "Epoch [7140/10000], Loss: 0.1572\n",
      "Epoch [7150/10000], Loss: 0.1565\n",
      "Epoch [7160/10000], Loss: 0.1571\n",
      "Epoch [7170/10000], Loss: 0.1576\n",
      "Epoch [7180/10000], Loss: 0.1571\n",
      "Epoch [7190/10000], Loss: 0.1580\n",
      "Epoch [7200/10000], Loss: 0.1573\n",
      "Epoch [7210/10000], Loss: 0.1572\n",
      "Epoch [7220/10000], Loss: 0.1578\n",
      "Epoch [7230/10000], Loss: 0.1568\n",
      "Epoch [7240/10000], Loss: 0.1575\n",
      "Epoch [7250/10000], Loss: 0.1583\n",
      "Epoch [7260/10000], Loss: 0.1576\n",
      "Epoch [7270/10000], Loss: 0.1595\n",
      "Epoch [7280/10000], Loss: 0.1587\n",
      "Epoch [7290/10000], Loss: 0.1575\n",
      "Epoch [7300/10000], Loss: 0.1590\n",
      "Epoch [7310/10000], Loss: 0.1582\n",
      "Epoch [7320/10000], Loss: 0.1564\n",
      "Epoch [7330/10000], Loss: 0.1570\n",
      "Epoch [7340/10000], Loss: 0.1580\n",
      "Epoch [7350/10000], Loss: 0.1575\n",
      "Epoch [7360/10000], Loss: 0.1575\n",
      "Epoch [7370/10000], Loss: 0.1565\n",
      "Epoch [7380/10000], Loss: 0.1563\n",
      "Epoch [7390/10000], Loss: 0.1593\n",
      "Epoch [7400/10000], Loss: 0.1594\n",
      "Epoch [7410/10000], Loss: 0.1584\n",
      "Epoch [7420/10000], Loss: 0.1580\n",
      "Epoch [7430/10000], Loss: 0.1571\n",
      "Epoch [7440/10000], Loss: 0.1578\n",
      "Epoch [7450/10000], Loss: 0.1578\n",
      "Epoch [7460/10000], Loss: 0.1564\n",
      "Epoch [7470/10000], Loss: 0.1565\n",
      "Epoch [7480/10000], Loss: 0.1595\n",
      "Epoch [7490/10000], Loss: 0.1586\n",
      "Epoch [7500/10000], Loss: 0.1590\n",
      "Epoch [7510/10000], Loss: 0.1587\n",
      "Epoch [7520/10000], Loss: 0.1575\n",
      "Epoch [7530/10000], Loss: 0.1577\n",
      "Epoch [7540/10000], Loss: 0.1574\n",
      "Epoch [7550/10000], Loss: 0.1574\n",
      "Epoch [7560/10000], Loss: 0.1572\n",
      "Epoch [7570/10000], Loss: 0.1568\n",
      "Epoch [7580/10000], Loss: 0.1571\n",
      "Epoch [7590/10000], Loss: 0.1573\n",
      "Epoch [7600/10000], Loss: 0.1573\n",
      "Epoch [7610/10000], Loss: 0.1591\n",
      "Epoch [7620/10000], Loss: 0.1577\n",
      "Epoch [7630/10000], Loss: 0.1575\n",
      "Epoch [7640/10000], Loss: 0.1565\n",
      "Epoch [7650/10000], Loss: 0.1564\n",
      "Epoch [7660/10000], Loss: 0.1576\n",
      "Epoch [7670/10000], Loss: 0.1563\n",
      "Epoch [7680/10000], Loss: 0.1563\n",
      "Epoch [7690/10000], Loss: 0.1568\n",
      "Epoch [7700/10000], Loss: 0.1585\n",
      "Epoch [7710/10000], Loss: 0.1577\n",
      "Epoch [7720/10000], Loss: 0.1573\n",
      "Epoch [7730/10000], Loss: 0.1565\n",
      "Epoch [7740/10000], Loss: 0.1570\n",
      "Epoch [7750/10000], Loss: 0.1573\n",
      "Epoch [7760/10000], Loss: 0.1573\n",
      "Epoch [7770/10000], Loss: 0.1581\n",
      "Epoch [7780/10000], Loss: 0.1569\n",
      "Epoch [7790/10000], Loss: 0.1564\n",
      "Epoch [7800/10000], Loss: 0.1571\n",
      "Epoch [7810/10000], Loss: 0.1570\n",
      "Epoch [7820/10000], Loss: 0.1563\n",
      "Epoch [7830/10000], Loss: 0.1572\n",
      "Epoch [7840/10000], Loss: 0.1586\n",
      "Epoch [7850/10000], Loss: 0.1583\n",
      "Epoch [7860/10000], Loss: 0.1581\n",
      "Epoch [7870/10000], Loss: 0.1572\n",
      "Epoch [7880/10000], Loss: 0.1562\n",
      "Epoch [7890/10000], Loss: 0.1565\n",
      "Epoch [7900/10000], Loss: 0.1572\n",
      "Epoch [7910/10000], Loss: 0.1565\n",
      "Epoch [7920/10000], Loss: 0.1559\n",
      "Epoch [7930/10000], Loss: 0.1575\n",
      "Epoch [7940/10000], Loss: 0.1566\n",
      "Epoch [7950/10000], Loss: 0.1582\n",
      "Epoch [7960/10000], Loss: 0.1563\n",
      "Epoch [7970/10000], Loss: 0.1562\n",
      "Epoch [7980/10000], Loss: 0.1573\n",
      "Epoch [7990/10000], Loss: 0.1571\n",
      "Epoch [8000/10000], Loss: 0.1569\n",
      "Epoch [8010/10000], Loss: 0.1574\n",
      "Epoch [8020/10000], Loss: 0.1571\n",
      "Epoch [8030/10000], Loss: 0.1559\n",
      "Epoch [8040/10000], Loss: 0.1571\n",
      "Epoch [8050/10000], Loss: 0.1567\n",
      "Epoch [8060/10000], Loss: 0.1571\n",
      "Epoch [8070/10000], Loss: 0.1579\n",
      "Epoch [8080/10000], Loss: 0.1570\n",
      "Epoch [8090/10000], Loss: 0.1571\n",
      "Epoch [8100/10000], Loss: 0.1571\n",
      "Epoch [8110/10000], Loss: 0.1605\n",
      "Epoch [8120/10000], Loss: 0.1572\n",
      "Epoch [8130/10000], Loss: 0.1573\n",
      "Epoch [8140/10000], Loss: 0.1562\n",
      "Epoch [8150/10000], Loss: 0.1574\n",
      "Epoch [8160/10000], Loss: 0.1577\n",
      "Epoch [8170/10000], Loss: 0.1566\n",
      "Epoch [8180/10000], Loss: 0.1565\n",
      "Epoch [8190/10000], Loss: 0.1566\n",
      "Epoch [8200/10000], Loss: 0.1572\n",
      "Epoch [8210/10000], Loss: 0.1564\n",
      "Epoch [8220/10000], Loss: 0.1582\n",
      "Epoch [8230/10000], Loss: 0.1569\n",
      "Epoch [8240/10000], Loss: 0.1566\n",
      "Epoch [8250/10000], Loss: 0.1567\n",
      "Epoch [8260/10000], Loss: 0.1562\n",
      "Epoch [8270/10000], Loss: 0.1571\n",
      "Epoch [8280/10000], Loss: 0.1578\n",
      "Epoch [8290/10000], Loss: 0.1569\n",
      "Epoch [8300/10000], Loss: 0.1575\n",
      "Epoch [8310/10000], Loss: 0.1562\n",
      "Epoch [8320/10000], Loss: 0.1567\n",
      "Epoch [8330/10000], Loss: 0.1563\n",
      "Epoch [8340/10000], Loss: 0.1564\n",
      "Epoch [8350/10000], Loss: 0.1581\n",
      "Epoch [8360/10000], Loss: 0.1565\n",
      "Epoch [8370/10000], Loss: 0.1562\n",
      "Epoch [8380/10000], Loss: 0.1582\n",
      "Epoch [8390/10000], Loss: 0.1579\n",
      "Epoch [8400/10000], Loss: 0.1559\n",
      "Epoch [8410/10000], Loss: 0.1565\n",
      "Epoch [8420/10000], Loss: 0.1569\n",
      "Epoch [8430/10000], Loss: 0.1563\n",
      "Epoch [8440/10000], Loss: 0.1565\n",
      "Epoch [8450/10000], Loss: 0.1570\n",
      "Epoch [8460/10000], Loss: 0.1571\n",
      "Epoch [8470/10000], Loss: 0.1562\n",
      "Epoch [8480/10000], Loss: 0.1561\n",
      "Epoch [8490/10000], Loss: 0.1573\n",
      "Epoch [8500/10000], Loss: 0.1582\n",
      "Epoch [8510/10000], Loss: 0.1567\n",
      "Epoch [8520/10000], Loss: 0.1567\n",
      "Epoch [8530/10000], Loss: 0.1562\n",
      "Epoch [8540/10000], Loss: 0.1560\n",
      "Epoch [8550/10000], Loss: 0.1583\n",
      "Epoch [8560/10000], Loss: 0.1565\n",
      "Epoch [8570/10000], Loss: 0.1578\n",
      "Epoch [8580/10000], Loss: 0.1557\n",
      "Epoch [8590/10000], Loss: 0.1561\n",
      "Epoch [8600/10000], Loss: 0.1568\n",
      "Epoch [8610/10000], Loss: 0.1576\n",
      "Epoch [8620/10000], Loss: 0.1559\n",
      "Epoch [8630/10000], Loss: 0.1564\n",
      "Epoch [8640/10000], Loss: 0.1558\n",
      "Epoch [8650/10000], Loss: 0.1565\n",
      "Epoch [8660/10000], Loss: 0.1566\n",
      "Epoch [8670/10000], Loss: 0.1572\n",
      "Epoch [8680/10000], Loss: 0.1560\n",
      "Epoch [8690/10000], Loss: 0.1567\n",
      "Epoch [8700/10000], Loss: 0.1565\n",
      "Epoch [8710/10000], Loss: 0.1570\n",
      "Epoch [8720/10000], Loss: 0.1562\n",
      "Epoch [8730/10000], Loss: 0.1560\n",
      "Epoch [8740/10000], Loss: 0.1555\n",
      "Epoch [8750/10000], Loss: 0.1574\n",
      "Epoch [8760/10000], Loss: 0.1586\n",
      "Epoch [8770/10000], Loss: 0.1569\n",
      "Epoch [8780/10000], Loss: 0.1573\n",
      "Epoch [8790/10000], Loss: 0.1561\n",
      "Epoch [8800/10000], Loss: 0.1561\n",
      "Epoch [8810/10000], Loss: 0.1560\n",
      "Epoch [8820/10000], Loss: 0.1559\n",
      "Epoch [8830/10000], Loss: 0.1554\n",
      "Epoch [8840/10000], Loss: 0.1561\n",
      "Epoch [8850/10000], Loss: 0.1569\n",
      "Epoch [8860/10000], Loss: 0.1560\n",
      "Epoch [8870/10000], Loss: 0.1564\n",
      "Epoch [8880/10000], Loss: 0.1587\n",
      "Epoch [8890/10000], Loss: 0.1567\n",
      "Epoch [8900/10000], Loss: 0.1576\n",
      "Epoch [8910/10000], Loss: 0.1562\n",
      "Epoch [8920/10000], Loss: 0.1562\n",
      "Epoch [8930/10000], Loss: 0.1565\n",
      "Epoch [8940/10000], Loss: 0.1585\n",
      "Epoch [8950/10000], Loss: 0.1564\n",
      "Epoch [8960/10000], Loss: 0.1559\n",
      "Epoch [8970/10000], Loss: 0.1562\n",
      "Epoch [8980/10000], Loss: 0.1553\n",
      "Epoch [8990/10000], Loss: 0.1560\n",
      "Epoch [9000/10000], Loss: 0.1565\n",
      "Epoch [9010/10000], Loss: 0.1569\n",
      "Epoch [9020/10000], Loss: 0.1563\n",
      "Epoch [9030/10000], Loss: 0.1574\n",
      "Epoch [9040/10000], Loss: 0.1573\n",
      "Epoch [9050/10000], Loss: 0.1560\n",
      "Epoch [9060/10000], Loss: 0.1557\n",
      "Epoch [9070/10000], Loss: 0.1557\n",
      "Epoch [9080/10000], Loss: 0.1571\n",
      "Epoch [9090/10000], Loss: 0.1563\n",
      "Epoch [9100/10000], Loss: 0.1563\n",
      "Epoch [9110/10000], Loss: 0.1556\n",
      "Epoch [9120/10000], Loss: 0.1562\n",
      "Epoch [9130/10000], Loss: 0.1562\n",
      "Epoch [9140/10000], Loss: 0.1569\n",
      "Epoch [9150/10000], Loss: 0.1557\n",
      "Epoch [9160/10000], Loss: 0.1557\n",
      "Epoch [9170/10000], Loss: 0.1593\n",
      "Epoch [9180/10000], Loss: 0.1592\n",
      "Epoch [9190/10000], Loss: 0.1569\n",
      "Epoch [9200/10000], Loss: 0.1557\n",
      "Epoch [9210/10000], Loss: 0.1566\n",
      "Epoch [9220/10000], Loss: 0.1563\n",
      "Epoch [9230/10000], Loss: 0.1558\n",
      "Epoch [9240/10000], Loss: 0.1566\n",
      "Epoch [9250/10000], Loss: 0.1570\n",
      "Epoch [9260/10000], Loss: 0.1551\n",
      "Epoch [9270/10000], Loss: 0.1549\n",
      "Epoch [9280/10000], Loss: 0.1571\n",
      "Epoch [9290/10000], Loss: 0.1550\n",
      "Epoch [9300/10000], Loss: 0.1555\n",
      "Epoch [9310/10000], Loss: 0.1559\n",
      "Epoch [9320/10000], Loss: 0.1549\n",
      "Epoch [9330/10000], Loss: 0.1560\n",
      "Epoch [9340/10000], Loss: 0.1553\n",
      "Epoch [9350/10000], Loss: 0.1562\n",
      "Epoch [9360/10000], Loss: 0.1572\n",
      "Epoch [9370/10000], Loss: 0.1563\n",
      "Epoch [9380/10000], Loss: 0.1572\n",
      "Epoch [9390/10000], Loss: 0.1546\n",
      "Epoch [9400/10000], Loss: 0.1555\n",
      "Epoch [9410/10000], Loss: 0.1549\n",
      "Epoch [9420/10000], Loss: 0.1556\n",
      "Epoch [9430/10000], Loss: 0.1552\n",
      "Epoch [9440/10000], Loss: 0.1553\n",
      "Epoch [9450/10000], Loss: 0.1552\n",
      "Epoch [9460/10000], Loss: 0.1554\n",
      "Epoch [9470/10000], Loss: 0.1558\n",
      "Epoch [9480/10000], Loss: 0.1553\n",
      "Epoch [9490/10000], Loss: 0.1566\n",
      "Epoch [9500/10000], Loss: 0.1564\n",
      "Epoch [9510/10000], Loss: 0.1557\n",
      "Epoch [9520/10000], Loss: 0.1547\n",
      "Epoch [9530/10000], Loss: 0.1559\n",
      "Epoch [9540/10000], Loss: 0.1568\n",
      "Epoch [9550/10000], Loss: 0.1565\n",
      "Epoch [9560/10000], Loss: 0.1565\n",
      "Epoch [9570/10000], Loss: 0.1556\n",
      "Epoch [9580/10000], Loss: 0.1557\n",
      "Epoch [9590/10000], Loss: 0.1563\n",
      "Epoch [9600/10000], Loss: 0.1548\n",
      "Epoch [9610/10000], Loss: 0.1552\n",
      "Epoch [9620/10000], Loss: 0.1565\n",
      "Epoch [9630/10000], Loss: 0.1564\n",
      "Epoch [9640/10000], Loss: 0.1563\n",
      "Epoch [9650/10000], Loss: 0.1555\n",
      "Epoch [9660/10000], Loss: 0.1554\n",
      "Epoch [9670/10000], Loss: 0.1573\n",
      "Epoch [9680/10000], Loss: 0.1550\n",
      "Epoch [9690/10000], Loss: 0.1564\n",
      "Epoch [9700/10000], Loss: 0.1550\n",
      "Epoch [9710/10000], Loss: 0.1565\n",
      "Epoch [9720/10000], Loss: 0.1554\n",
      "Epoch [9730/10000], Loss: 0.1553\n",
      "Epoch [9740/10000], Loss: 0.1551\n",
      "Epoch [9750/10000], Loss: 0.1551\n",
      "Epoch [9760/10000], Loss: 0.1552\n",
      "Epoch [9770/10000], Loss: 0.1549\n",
      "Epoch [9780/10000], Loss: 0.1560\n",
      "Epoch [9790/10000], Loss: 0.1553\n",
      "Epoch [9800/10000], Loss: 0.1558\n",
      "Epoch [9810/10000], Loss: 0.1548\n",
      "Epoch [9820/10000], Loss: 0.1555\n",
      "Epoch [9830/10000], Loss: 0.1550\n",
      "Epoch [9840/10000], Loss: 0.1555\n",
      "Epoch [9850/10000], Loss: 0.1560\n",
      "Epoch [9860/10000], Loss: 0.1555\n",
      "Epoch [9870/10000], Loss: 0.1552\n",
      "Epoch [9880/10000], Loss: 0.1549\n",
      "Epoch [9890/10000], Loss: 0.1546\n",
      "Epoch [9900/10000], Loss: 0.1553\n",
      "Epoch [9910/10000], Loss: 0.1577\n",
      "Epoch [9920/10000], Loss: 0.1557\n",
      "Epoch [9930/10000], Loss: 0.1552\n",
      "Epoch [9940/10000], Loss: 0.1546\n",
      "Epoch [9950/10000], Loss: 0.1547\n",
      "Epoch [9960/10000], Loss: 0.1550\n",
      "Epoch [9970/10000], Loss: 0.1555\n",
      "Epoch [9980/10000], Loss: 0.1549\n",
      "Epoch [9990/10000], Loss: 0.1554\n",
      "Epoch [10000/10000], Loss: 0.1553\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성 및 학습 설정\n",
    "ssae_model = DenoisingSSAE(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ssae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(n_epochs_ssae):\n",
    "    ssae_model.train()\n",
    "    inputs = torch.FloatTensor(X_train_noisy_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "    targets = torch.FloatTensor(X_train_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = ssae_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs_ssae}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNxDAlFBKApN"
   },
   "source": [
    "## 모델 평가함수 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yXpRxDP8W6Zo"
   },
   "outputs": [],
   "source": [
    "# 평가함수 정의\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "        \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "81YDhprUKApO"
   },
   "outputs": [],
   "source": [
    "# Shapley Value 계산, 시각화 함수 정의\n",
    "def evaluate_models_shap1(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    masker = shap.maskers.Independent(X_train_encoded_ssae)\n",
    "    explainer = shap.LinearExplainer(model, masker=masker)\n",
    "    shap_values = explainer(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n",
    "\n",
    "def evaluate_models_shap2(model, X_test_encoded_ssae):\n",
    "    # TreeExplainer를 사용한 Shapley Value 계산\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "D6GIT1bg23Ny"
   },
   "outputs": [],
   "source": [
    "# ShapleyValue 계산, 시각화함수 정의 (TabNet 전용)\n",
    "def evaluate_models_shap_tabnet(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    # DeepExplainer를 사용한 Shapley Value 계산 (TabNet 전용)\n",
    "    explainer = DeepExplainer(model, X_train_encoded_ssae)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증(CV)을 위한 함수 정의\n",
    "def cross_validate_model(model, X, y, cv=cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_list = [f'e{i}' for i in range(latent_size)]\n",
    "X_train_encoded = pd.concat(\n",
    "                    [X_train[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_train[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_train.index, columns=encoded_list)],\n",
    "                    axis=1)\n",
    "\n",
    "X_test_encoded = pd.concat(\n",
    "                    [X_test[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_test[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_test.index, columns=encoded_list)],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scalerenc = StandardScaler()\n",
    "X_train_encoded[encoded_list] = scalerenc.fit_transform(X_train_encoded[encoded_list])\n",
    "X_test_encoded[encoded_list] = scalerenc.transform(X_test_encoded[encoded_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Logistic Regression model:\n",
      "  F1 Score: 0.3391\n",
      "  Accuracy: 0.5811\n",
      "  Precision: 0.2313\n",
      "  Recall: 0.6355\n",
      "  Best Parameters: {'C': 0.001, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "\n",
      "Best XGBoost model:\n",
      "  F1 Score: 0.3459\n",
      "  Accuracy: 0.5959\n",
      "  Precision: 0.2382\n",
      "  Recall: 0.6319\n",
      "  Best Parameters: {'learning_rate': 0.01, 'n_estimators': 1000, 'reg_alpha': 0.001, 'reg_lambda': 10.0}\n",
      "\n",
      "\n",
      "Best LightGBM model:\n",
      "  F1 Score: 0.3466\n",
      "  Accuracy: 0.5955\n",
      "  Precision: 0.2385\n",
      "  Recall: 0.6346\n",
      "  Best Parameters: {'learning_rate': 0.01, 'n_estimators': 1000, 'reg_alpha': 0.1, 'reg_lambda': 0.001}\n",
      "\n",
      "\n",
      "Best RandomForest model:\n",
      "  F1 Score: 0.3412\n",
      "  Accuracy: 0.5828\n",
      "  Precision: 0.2328\n",
      "  Recall: 0.6388\n",
      "  Best Parameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "\n",
      "Best SVM model:\n",
      "  F1 Score: 0.2722\n",
      "  Accuracy: 0.4889\n",
      "  Precision: 0.1793\n",
      "  Recall: 0.5652\n",
      "  Best Parameters: {'C': 0.001, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000}\n",
      "\n",
      "\n",
      "Best overall model: LightGBM Classifier\n",
      "  F1 Score: 0.3466\n",
      "  Accuracy: 0.5955\n",
      "  Precision: 0.2385\n",
      "  Recall: 0.6346\n",
      "  Best Parameters: {'learning_rate': 0.01, 'n_estimators': 1000, 'reg_alpha': 0.1, 'reg_lambda': 0.001}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Train and evaluate the model with a given parameter set\n",
    "def evaluate_model(model_name, model, param_set):\n",
    "    try:\n",
    "        model.set_params(**param_set)\n",
    "        model.fit(X_train_encoded.drop(encoded_list, axis=1), y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_encoded.drop(encoded_list, axis=1))\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracy, precision, recall, f1, confusion_matrix(y_test, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name} with params: {param_set}. Error: {str(e)}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Define parameter grids for each model\n",
    "logistic_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 3),  # Regularization strength with 10 values\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],  # Fast solver for logistic regression with L2 regularization\n",
    "    'max_iter': [1000]  # Very high iteration count\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': np.logspace(-4, 0, 3),\n",
    "    'reg_alpha': np.logspace(-3, 1, 3),\n",
    "    'reg_lambda': np.logspace(-3, 1, 3),\n",
    "    'n_estimators': [1000]\n",
    "}\n",
    "\n",
    "lgb_param_grid = {\n",
    "    'learning_rate': np.logspace(-4, 0, 3),\n",
    "    'reg_alpha': np.logspace(-3, 1, 3),\n",
    "    'reg_lambda': np.logspace(-3, 1, 3),\n",
    "    'n_estimators': [1000]\n",
    "}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [1000],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node\n",
    "    'bootstrap': [True, False]  # Whether bootstrap samples are used when building trees\n",
    "}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 3),  # Regularization strength with 10 values\n",
    "    'kernel': ['rbf'],  # RBF kernel is commonly used\n",
    "    'gamma': ['scale'],  # Standard option for gamma\n",
    "    'max_iter': [1000]  # Very high iteration count for convergence\n",
    "}\n",
    "\n",
    "# Evaluate models with parameter grids\n",
    "logistic_model = LogisticRegression(n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(n_jobs=-1, verbosity=0)  # Disable XGBoost output\n",
    "lgb_model = lgb.LGBMClassifier(n_jobs=-1, verbose=-1)  # Disable LightGBM output\n",
    "rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "svm_model = SVC()\n",
    "\n",
    "# Dictionary to store best results for each model\n",
    "best_results = {}\n",
    "\n",
    "# Logistic Regression with F1 optimization\n",
    "for params in ParameterGrid(logistic_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('Logistic Regression', logistic_model, params)\n",
    "    if accuracy is not None and ('Logistic Regression' not in best_results or f1 > best_results['Logistic Regression']['f1_score']):\n",
    "        best_results['Logistic Regression'] = {\n",
    "            'model': logistic_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest Logistic Regression model:\")\n",
    "if 'Logistic Regression' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['Logistic Regression']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['Logistic Regression']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['Logistic Regression']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['Logistic Regression']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['Logistic Regression']['params']}\\n\")\n",
    "\n",
    "# XGBoost with F1 optimization\n",
    "for params in ParameterGrid(xgb_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('XGBoost Classifier', xgb_model, params)\n",
    "    if accuracy is not None and ('XGBoost Classifier' not in best_results or f1 > best_results['XGBoost Classifier']['f1_score']):\n",
    "        best_results['XGBoost Classifier'] = {\n",
    "            'model': xgb_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest XGBoost model:\")\n",
    "if 'XGBoost Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['XGBoost Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['XGBoost Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['XGBoost Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['XGBoost Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['XGBoost Classifier']['params']}\\n\")\n",
    "\n",
    "# LightGBM with F1 optimization\n",
    "for params in ParameterGrid(lgb_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('LightGBM Classifier', lgb_model, params)\n",
    "    if accuracy is not None and ('LightGBM Classifier' not in best_results or f1 > best_results['LightGBM Classifier']['f1_score']):\n",
    "        best_results['LightGBM Classifier'] = {\n",
    "            'model': lgb_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest LightGBM model:\")\n",
    "if 'LightGBM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['LightGBM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['LightGBM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['LightGBM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['LightGBM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['LightGBM Classifier']['params']}\\n\")\n",
    "\n",
    "# RandomForest with F1 optimization\n",
    "for params in ParameterGrid(rf_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('RandomForest Classifier', rf_model, params)\n",
    "    if accuracy is not None and ('RandomForest Classifier' not in best_results or f1 > best_results['RandomForest Classifier']['f1_score']):\n",
    "        best_results['RandomForest Classifier'] = {\n",
    "            'model': rf_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest RandomForest model:\")\n",
    "if 'RandomForest Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['RandomForest Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['RandomForest Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['RandomForest Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['RandomForest Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['RandomForest Classifier']['params']}\\n\")\n",
    "\n",
    "# SVM with F1 optimization\n",
    "for params in ParameterGrid(svm_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('SVM Classifier', svm_model, params)\n",
    "    if accuracy is not None and ('SVM Classifier' not in best_results or f1 > best_results['SVM Classifier']['f1_score']):\n",
    "        best_results['SVM Classifier'] = {\n",
    "            'model': svm_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest SVM model:\")\n",
    "if 'SVM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['SVM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['SVM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['SVM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['SVM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['SVM Classifier']['params']}\\n\")\n",
    "\n",
    "# Find the best overall model based on F1 score\n",
    "best_model_name = max(best_results, key=lambda x: best_results[x]['f1_score'])\n",
    "best_model_results = best_results[best_model_name]\n",
    "\n",
    "# Print the best overall model\n",
    "print(f\"\\nBest overall model: {best_model_name}\")\n",
    "print(f\"  F1 Score: {best_model_results['f1_score']:.4f}\")\n",
    "print(f\"  Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_model_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_model_results['recall']:.4f}\")\n",
    "print(f\"  Best Parameters: {best_model_results['params']}\")\n",
    "\n",
    "# Plot confusion matrix for the best overall model\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=best_model_results['confusion_matrix'])\n",
    "disp.plot()\n",
    "plt.title(f\"Confusion Matrix for {best_model_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP 내용 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP\n",
    "class ClsModel:\n",
    "    def __init__(self, reg_model, ssae_model, feature_list, encoding_target, reg_columns, device='cpu'):\n",
    "        self.reg_model = reg_model\n",
    "        self.ssae_model = ssae_model\n",
    "        self.feature_list = feature_list\n",
    "        self.encoding_target = encoding_target\n",
    "        self.reg_columns = reg_columns\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns = self.reg_columns)\n",
    "        \n",
    "        X_org = X[self.feature_list]\n",
    "        X_encoded = X[self.encoding_target]\n",
    "        X_encoded = pd.DataFrame(self.ssae_model.encoder(torch.FloatTensor(X_encoded.to_numpy()).to(device)).detach().numpy(),\n",
    "                        index=X_encoded.index, columns=[f'e{i}' for i in range(latent_size)])\n",
    "        X_con = pd.concat([X_org, X_encoded], axis=1)\n",
    "        \n",
    "        return self.reg_model.predict(X_con)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "\n",
    "cls_model = ClsModel(best_lgb_model, ssae_model, feature_list, encoding_target, X_test.columns)  # 첫번째 모델 변경 필요, 원본 X_test로 계산해야함\n",
    "explainer = shap.KernelExplainer(cls_model, X_test[0:30])\n",
    "shap_values = explainer.shap_values(X_test[0:30])\n",
    "shap.summary_plot(shap_values, X_test[0:30])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
