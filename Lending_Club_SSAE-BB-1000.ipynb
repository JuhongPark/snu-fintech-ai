{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q35P2ummXim"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/JuhongPark/snu-fintech-ai/blob/main/Lending_Club_SSAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WyoEfeVoyh-"
   },
   "source": [
    "# 데이터 가져오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brnach 지정\n",
    "branch = 'develop'\n",
    "\n",
    "# 라이브러리 설치 (2분 가량 소요됨)\n",
    "#!pip install shap -q\n",
    "#!pip install pytorch_tabnet -q\n",
    "\n",
    "# 필요 파일 다운로드\n",
    "#!wget https://raw.githubusercontent.com/JuhongPark/snu-fintech-ai/{branch}/LC_Data_Cleaned_0829.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # seed 값 설정\n",
    "is_test = True  # 테스트런 설정\n",
    "\n",
    "# 입력 변수 파라미터\n",
    "target = 'loan_status_encoded'\n",
    "drop_list = ['id', 'int_rate', 'installment', 'sub_grade', 'grade', 'tbond_int', 'year', 'term', 'loan_status', 'zip_code']\n",
    "feature_list = ['loan_amnt', 'emp_length', 'revol_util', 'pub_rec', 'fico_range_high', 'fico_range_low', 'percent_bc_gt_75', 'annual_inc',\n",
    "                'dti', 'delinq_2yrs', 'open_acc', 'revol_bal', 'total_acc', 'inq_last_6mths'] # 'fico_range_low'제외\n",
    "cat_list = ['purpose', 'addr_state', 'initial_list_status', 'home_ownership']\n",
    "\n",
    "# 대출 이후 변수\n",
    "post_list = ['Funded_amnt', 'funded_amnt_inv', 'collection_recovery_fee', 'collections_12_mths_ex_med', 'last_credit_pull_d', 'last_pymnt_amnt', 'last_pymnt_d',\n",
    " 'mths_since_last_major_derog', 'next_pymnt_d', 'out_prncp', 'out_prncp_inv', 'recoveries', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee',\n",
    " 'total_rec_prncp', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date',\n",
    " 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount',\n",
    " 'hardship_last_payment_amount', 'debt_settlement_flag', 'last_fico_range_high', 'last_fico_range_low']\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "# For model\n",
    "learning_rate = 0.01\n",
    "\n",
    "# For training\n",
    "#n_epochs = 10000\n",
    "\n",
    "# For CV\n",
    "cv = 10\n",
    "\n",
    "# For SSAE\n",
    "n_epochs_ssae = 10000\n",
    "latent_size = 8\n",
    "\n",
    "# 테스트 런일 경우, 에포크 수를 줄임\n",
    "if is_test:\n",
    "    n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATZ3E7NEKApJ",
    "outputId": "653a45fc-17ab-47e4-b4ae-5360f61fcec2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed 설정\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed) # 파이썬 표준 라이브러리\n",
    "    np.random.seed(seed) # numpy의 random 모듈에서 사용하는 seed\n",
    "    torch.manual_seed(seed) # pytorch에서 사용하는 seed\n",
    "    if torch.cuda.is_available(): # GPU에서 실행되는 PyTorch 연산의 무작위성을 제어\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# seed 값 설정\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ans_U0Bqoyh-"
   },
   "source": [
    "## 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mn3BhYVohcPI"
   },
   "outputs": [],
   "source": [
    "# 모든 행이 화면에 표시되도록 설정합니다.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 파일 로드\n",
    "file_path = 'LC_Data_Cleaned_0829.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 테스트 런일 경우, 데이터 크기 줄이기\n",
    "if is_test:\n",
    "    df = df.sample(frac=0.01, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0DpvFHJoyh-"
   },
   "source": [
    "## 데이터 구조 훑어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "eIAt4-Y_oyh-",
    "outputId": "537e79ec-50cb-4a7f-d6f3-e25d5f9b6667"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>...</th>\n",
       "      <th>percent_bc_gt_75</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>loan_status_encoded</th>\n",
       "      <th>year</th>\n",
       "      <th>tbond_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>178221</th>\n",
       "      <td>22250.0</td>\n",
       "      <td>22250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>891.38</td>\n",
       "      <td>F</td>\n",
       "      <td>F5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>110000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114548.0</td>\n",
       "      <td>66485.0</td>\n",
       "      <td>38500.0</td>\n",
       "      <td>74848.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109102</th>\n",
       "      <td>4200.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>143.51</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>407056.0</td>\n",
       "      <td>64902.0</td>\n",
       "      <td>43100.0</td>\n",
       "      <td>51956.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90322</th>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>281.62</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>67000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36656.0</td>\n",
       "      <td>25908.0</td>\n",
       "      <td>13700.0</td>\n",
       "      <td>22456.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210783</th>\n",
       "      <td>5350.0</td>\n",
       "      <td>5350.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>175.13</td>\n",
       "      <td>B</td>\n",
       "      <td>B3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>41683.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185699.0</td>\n",
       "      <td>35220.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208963</th>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>523.75</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>89000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>334807.0</td>\n",
       "      <td>92785.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>70998.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.001312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt  funded_amnt  term  int_rate  installment grade sub_grade  \\\n",
       "178221    22250.0      22250.0     0    0.2557       891.38     F        F5   \n",
       "109102     4200.0       4200.0     0    0.1398       143.51     C        C1   \n",
       "90322      9000.0       9000.0     0    0.0790       281.62     A        A4   \n",
       "210783     5350.0       5350.0     0    0.1099       175.13     B        B3   \n",
       "208963    16000.0      16000.0     0    0.1099       523.75     B        B2   \n",
       "\n",
       "        emp_length  home_ownership  annual_inc  ...  percent_bc_gt_75  \\\n",
       "178221         0.0               1    110000.0  ...               0.0   \n",
       "109102        10.0               0     80000.0  ...              25.0   \n",
       "90322          2.0               1     67000.0  ...              20.0   \n",
       "210783         3.0               1     41683.0  ...               0.0   \n",
       "208963        10.0               0     89000.0  ...              50.0   \n",
       "\n",
       "       pub_rec_bankruptcies tax_liens tot_hi_cred_lim total_bal_ex_mort  \\\n",
       "178221                  1.0       0.0        114548.0           66485.0   \n",
       "109102                  0.0       0.0        407056.0           64902.0   \n",
       "90322                   0.0       0.0         36656.0           25908.0   \n",
       "210783                  0.0       0.0        185699.0           35220.0   \n",
       "208963                  0.0       0.0        334807.0           92785.0   \n",
       "\n",
       "        total_bc_limit  total_il_high_credit_limit  loan_status_encoded  year  \\\n",
       "178221         38500.0                     74848.0                    0  2013   \n",
       "109102         43100.0                     51956.0                    0  2013   \n",
       "90322          13700.0                     22456.0                    0  2014   \n",
       "210783          3200.0                     30000.0                    0  2014   \n",
       "208963         12500.0                     70998.0                    0  2013   \n",
       "\n",
       "        tbond_int  \n",
       "178221   0.001312  \n",
       "109102   0.001312  \n",
       "90322    0.001211  \n",
       "210783   0.001211  \n",
       "208963   0.001312  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxjdTP4gKApM"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "L42IMY2hKSEO",
    "outputId": "c428f136-6ace-4ed8-f5fe-b90c4949fd50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2220, 59)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 df 에 있는 칼럼 중에서, drop_list 및 post_list의 칼럼 제거\n",
    "drop_list = list(set(df.columns) & set(drop_list + post_list))\n",
    "\n",
    "# 불필요한 변수 Drop\n",
    "df = df.drop(columns = drop_list)\n",
    "\n",
    "# 결측치 처리\n",
    "df = df.fillna(0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWIX34Exoyim"
   },
   "source": [
    "# 모델 선택과 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTRHX9M5oyiA"
   },
   "source": [
    "## 테스트 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "PrhJ2zzMKApN"
   },
   "outputs": [],
   "source": [
    "# Torch 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if not is_test else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train distribution: loan_status_encoded\n",
      "0    1481\n",
      "1     295\n",
      "Name: count, dtype: int64\n",
      "1776 444 1776 444\n",
      "Resampled y_train distribution: loan_status_encoded\n",
      "0    295\n",
      "1    295\n",
      "Name: count, dtype: int64\n",
      "590 444 590 444\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 언더샘플링 객체 생성\n",
    "print(f\"Original y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# 트레인 데이터셋에 대해 언더샘플링 수행\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Resampled y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수 선정\n",
    "num_list = list(set(X_train.columns) ^ set(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder 생성 및 학습 데이터에 적합\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oneHotEncoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "oneHotEncoder.fit(X_train[cat_list])\n",
    "\n",
    "# 학습 데이터에 인코딩 적용\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_train[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# 테스트 데이터에 인코딩 적용\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_test[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 원래 데이터프레임과 병합\n",
    "X_train = X_train.drop(cat_list, axis=1).join(X_train_encoded)\n",
    "X_test = X_test.drop(cat_list, axis=1).join(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 수치형 변수 찾기\n",
    "X_train[num_list] = scaler.fit_transform(X_train[num_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test[num_list] = scaler.transform(X_test[num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 113)\n",
      "(590,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvlhFSTgKApN"
   },
   "source": [
    "## SSAE 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GhT0RT1sKApN"
   },
   "outputs": [],
   "source": [
    "# SSAE 모델 정의\n",
    "class DenoisingSSAE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DenoisingSSAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # input_dim이 실제 데이터의 feature 수와 일치해야 함\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)  # output_dim도 input_dim과 일치해야 함\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "# Noise 추가 함수\n",
    "def add_noise(data, noise_factor=0.2):\n",
    "    noise = noise_factor * np.random.randn(*data.shape)\n",
    "    noisy_data = data + noise\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.)\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# Encoding 목표 설정\n",
    "encoding_target = list(set(X_train.columns) ^ set(feature_list))\n",
    "\n",
    "# 노이즈가 추가\n",
    "X_train_noisy_np = add_noise(X_train[encoding_target]).to_numpy()\n",
    "X_train_np = X_train[encoding_target].to_numpy()\n",
    "          \n",
    "input_dim = X_train_np.shape[1]  # X_train의 feature 수\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 0.4507\n",
      "Epoch [10/10000], Loss: 0.3856\n",
      "Epoch [20/10000], Loss: 0.3575\n",
      "Epoch [30/10000], Loss: 0.3319\n",
      "Epoch [40/10000], Loss: 0.3127\n",
      "Epoch [50/10000], Loss: 0.2990\n",
      "Epoch [60/10000], Loss: 0.2866\n",
      "Epoch [70/10000], Loss: 0.2700\n",
      "Epoch [80/10000], Loss: 0.2592\n",
      "Epoch [90/10000], Loss: 0.2444\n",
      "Epoch [100/10000], Loss: 0.2333\n",
      "Epoch [110/10000], Loss: 0.2246\n",
      "Epoch [120/10000], Loss: 0.2174\n",
      "Epoch [130/10000], Loss: 0.2123\n",
      "Epoch [140/10000], Loss: 0.2046\n",
      "Epoch [150/10000], Loss: 0.1975\n",
      "Epoch [160/10000], Loss: 0.1925\n",
      "Epoch [170/10000], Loss: 0.1894\n",
      "Epoch [180/10000], Loss: 0.1857\n",
      "Epoch [190/10000], Loss: 0.1797\n",
      "Epoch [200/10000], Loss: 0.1787\n",
      "Epoch [210/10000], Loss: 0.1744\n",
      "Epoch [220/10000], Loss: 0.1716\n",
      "Epoch [230/10000], Loss: 0.1676\n",
      "Epoch [240/10000], Loss: 0.1650\n",
      "Epoch [250/10000], Loss: 0.1634\n",
      "Epoch [260/10000], Loss: 0.1628\n",
      "Epoch [270/10000], Loss: 0.1600\n",
      "Epoch [280/10000], Loss: 0.1621\n",
      "Epoch [290/10000], Loss: 0.1567\n",
      "Epoch [300/10000], Loss: 0.1537\n",
      "Epoch [310/10000], Loss: 0.1529\n",
      "Epoch [320/10000], Loss: 0.1537\n",
      "Epoch [330/10000], Loss: 0.1494\n",
      "Epoch [340/10000], Loss: 0.1504\n",
      "Epoch [350/10000], Loss: 0.1485\n",
      "Epoch [360/10000], Loss: 0.1467\n",
      "Epoch [370/10000], Loss: 0.1461\n",
      "Epoch [380/10000], Loss: 0.1474\n",
      "Epoch [390/10000], Loss: 0.1449\n",
      "Epoch [400/10000], Loss: 0.1437\n",
      "Epoch [410/10000], Loss: 0.1436\n",
      "Epoch [420/10000], Loss: 0.1414\n",
      "Epoch [430/10000], Loss: 0.1451\n",
      "Epoch [440/10000], Loss: 0.1418\n",
      "Epoch [450/10000], Loss: 0.1403\n",
      "Epoch [460/10000], Loss: 0.1379\n",
      "Epoch [470/10000], Loss: 0.1374\n",
      "Epoch [480/10000], Loss: 0.1366\n",
      "Epoch [490/10000], Loss: 0.1381\n",
      "Epoch [500/10000], Loss: 0.1350\n",
      "Epoch [510/10000], Loss: 0.1355\n",
      "Epoch [520/10000], Loss: 0.1344\n",
      "Epoch [530/10000], Loss: 0.1338\n",
      "Epoch [540/10000], Loss: 0.1327\n",
      "Epoch [550/10000], Loss: 0.1350\n",
      "Epoch [560/10000], Loss: 0.1324\n",
      "Epoch [570/10000], Loss: 0.1312\n",
      "Epoch [580/10000], Loss: 0.1315\n",
      "Epoch [590/10000], Loss: 0.1314\n",
      "Epoch [600/10000], Loss: 0.1305\n",
      "Epoch [610/10000], Loss: 0.1287\n",
      "Epoch [620/10000], Loss: 0.1290\n",
      "Epoch [630/10000], Loss: 0.1333\n",
      "Epoch [640/10000], Loss: 0.1293\n",
      "Epoch [650/10000], Loss: 0.1272\n",
      "Epoch [660/10000], Loss: 0.1267\n",
      "Epoch [670/10000], Loss: 0.1285\n",
      "Epoch [680/10000], Loss: 0.1281\n",
      "Epoch [690/10000], Loss: 0.1264\n",
      "Epoch [700/10000], Loss: 0.1267\n",
      "Epoch [710/10000], Loss: 0.1266\n",
      "Epoch [720/10000], Loss: 0.1245\n",
      "Epoch [730/10000], Loss: 0.1261\n",
      "Epoch [740/10000], Loss: 0.1257\n",
      "Epoch [750/10000], Loss: 0.1243\n",
      "Epoch [760/10000], Loss: 0.1239\n",
      "Epoch [770/10000], Loss: 0.1244\n",
      "Epoch [780/10000], Loss: 0.1249\n",
      "Epoch [790/10000], Loss: 0.1226\n",
      "Epoch [800/10000], Loss: 0.1241\n",
      "Epoch [810/10000], Loss: 0.1224\n",
      "Epoch [820/10000], Loss: 0.1224\n",
      "Epoch [830/10000], Loss: 0.1236\n",
      "Epoch [840/10000], Loss: 0.1235\n",
      "Epoch [850/10000], Loss: 0.1218\n",
      "Epoch [860/10000], Loss: 0.1241\n",
      "Epoch [870/10000], Loss: 0.1217\n",
      "Epoch [880/10000], Loss: 0.1200\n",
      "Epoch [890/10000], Loss: 0.1228\n",
      "Epoch [900/10000], Loss: 0.1229\n",
      "Epoch [910/10000], Loss: 0.1209\n",
      "Epoch [920/10000], Loss: 0.1218\n",
      "Epoch [930/10000], Loss: 0.1201\n",
      "Epoch [940/10000], Loss: 0.1214\n",
      "Epoch [950/10000], Loss: 0.1212\n",
      "Epoch [960/10000], Loss: 0.1223\n",
      "Epoch [970/10000], Loss: 0.1187\n",
      "Epoch [980/10000], Loss: 0.1189\n",
      "Epoch [990/10000], Loss: 0.1181\n",
      "Epoch [1000/10000], Loss: 0.1178\n",
      "Epoch [1010/10000], Loss: 0.1183\n",
      "Epoch [1020/10000], Loss: 0.1177\n",
      "Epoch [1030/10000], Loss: 0.1185\n",
      "Epoch [1040/10000], Loss: 0.1187\n",
      "Epoch [1050/10000], Loss: 0.1191\n",
      "Epoch [1060/10000], Loss: 0.1202\n",
      "Epoch [1070/10000], Loss: 0.1183\n",
      "Epoch [1080/10000], Loss: 0.1195\n",
      "Epoch [1090/10000], Loss: 0.1188\n",
      "Epoch [1100/10000], Loss: 0.1173\n",
      "Epoch [1110/10000], Loss: 0.1197\n",
      "Epoch [1120/10000], Loss: 0.1171\n",
      "Epoch [1130/10000], Loss: 0.1173\n",
      "Epoch [1140/10000], Loss: 0.1238\n",
      "Epoch [1150/10000], Loss: 0.1170\n",
      "Epoch [1160/10000], Loss: 0.1167\n",
      "Epoch [1170/10000], Loss: 0.1172\n",
      "Epoch [1180/10000], Loss: 0.1166\n",
      "Epoch [1190/10000], Loss: 0.1184\n",
      "Epoch [1200/10000], Loss: 0.1175\n",
      "Epoch [1210/10000], Loss: 0.1166\n",
      "Epoch [1220/10000], Loss: 0.1159\n",
      "Epoch [1230/10000], Loss: 0.1162\n",
      "Epoch [1240/10000], Loss: 0.1172\n",
      "Epoch [1250/10000], Loss: 0.1155\n",
      "Epoch [1260/10000], Loss: 0.1163\n",
      "Epoch [1270/10000], Loss: 0.1151\n",
      "Epoch [1280/10000], Loss: 0.1141\n",
      "Epoch [1290/10000], Loss: 0.1175\n",
      "Epoch [1300/10000], Loss: 0.1153\n",
      "Epoch [1310/10000], Loss: 0.1143\n",
      "Epoch [1320/10000], Loss: 0.1170\n",
      "Epoch [1330/10000], Loss: 0.1138\n",
      "Epoch [1340/10000], Loss: 0.1177\n",
      "Epoch [1350/10000], Loss: 0.1171\n",
      "Epoch [1360/10000], Loss: 0.1152\n",
      "Epoch [1370/10000], Loss: 0.1131\n",
      "Epoch [1380/10000], Loss: 0.1149\n",
      "Epoch [1390/10000], Loss: 0.1144\n",
      "Epoch [1400/10000], Loss: 0.1171\n",
      "Epoch [1410/10000], Loss: 0.1139\n",
      "Epoch [1420/10000], Loss: 0.1131\n",
      "Epoch [1430/10000], Loss: 0.1138\n",
      "Epoch [1440/10000], Loss: 0.1146\n",
      "Epoch [1450/10000], Loss: 0.1147\n",
      "Epoch [1460/10000], Loss: 0.1151\n",
      "Epoch [1470/10000], Loss: 0.1146\n",
      "Epoch [1480/10000], Loss: 0.1130\n",
      "Epoch [1490/10000], Loss: 0.1170\n",
      "Epoch [1500/10000], Loss: 0.1131\n",
      "Epoch [1510/10000], Loss: 0.1136\n",
      "Epoch [1520/10000], Loss: 0.1143\n",
      "Epoch [1530/10000], Loss: 0.1142\n",
      "Epoch [1540/10000], Loss: 0.1126\n",
      "Epoch [1550/10000], Loss: 0.1152\n",
      "Epoch [1560/10000], Loss: 0.1130\n",
      "Epoch [1570/10000], Loss: 0.1134\n",
      "Epoch [1580/10000], Loss: 0.1133\n",
      "Epoch [1590/10000], Loss: 0.1124\n",
      "Epoch [1600/10000], Loss: 0.1117\n",
      "Epoch [1610/10000], Loss: 0.1124\n",
      "Epoch [1620/10000], Loss: 0.1124\n",
      "Epoch [1630/10000], Loss: 0.1115\n",
      "Epoch [1640/10000], Loss: 0.1123\n",
      "Epoch [1650/10000], Loss: 0.1140\n",
      "Epoch [1660/10000], Loss: 0.1139\n",
      "Epoch [1670/10000], Loss: 0.1120\n",
      "Epoch [1680/10000], Loss: 0.1117\n",
      "Epoch [1690/10000], Loss: 0.1111\n",
      "Epoch [1700/10000], Loss: 0.1137\n",
      "Epoch [1710/10000], Loss: 0.1109\n",
      "Epoch [1720/10000], Loss: 0.1118\n",
      "Epoch [1730/10000], Loss: 0.1135\n",
      "Epoch [1740/10000], Loss: 0.1127\n",
      "Epoch [1750/10000], Loss: 0.1120\n",
      "Epoch [1760/10000], Loss: 0.1103\n",
      "Epoch [1770/10000], Loss: 0.1111\n",
      "Epoch [1780/10000], Loss: 0.1111\n",
      "Epoch [1790/10000], Loss: 0.1140\n",
      "Epoch [1800/10000], Loss: 0.1116\n",
      "Epoch [1810/10000], Loss: 0.1106\n",
      "Epoch [1820/10000], Loss: 0.1133\n",
      "Epoch [1830/10000], Loss: 0.1109\n",
      "Epoch [1840/10000], Loss: 0.1112\n",
      "Epoch [1850/10000], Loss: 0.1111\n",
      "Epoch [1860/10000], Loss: 0.1108\n",
      "Epoch [1870/10000], Loss: 0.1108\n",
      "Epoch [1880/10000], Loss: 0.1119\n",
      "Epoch [1890/10000], Loss: 0.1121\n",
      "Epoch [1900/10000], Loss: 0.1120\n",
      "Epoch [1910/10000], Loss: 0.1109\n",
      "Epoch [1920/10000], Loss: 0.1101\n",
      "Epoch [1930/10000], Loss: 0.1105\n",
      "Epoch [1940/10000], Loss: 0.1128\n",
      "Epoch [1950/10000], Loss: 0.1110\n",
      "Epoch [1960/10000], Loss: 0.1149\n",
      "Epoch [1970/10000], Loss: 0.1109\n",
      "Epoch [1980/10000], Loss: 0.1089\n",
      "Epoch [1990/10000], Loss: 0.1096\n",
      "Epoch [2000/10000], Loss: 0.1093\n",
      "Epoch [2010/10000], Loss: 0.1115\n",
      "Epoch [2020/10000], Loss: 0.1101\n",
      "Epoch [2030/10000], Loss: 0.1101\n",
      "Epoch [2040/10000], Loss: 0.1100\n",
      "Epoch [2050/10000], Loss: 0.1090\n",
      "Epoch [2060/10000], Loss: 0.1102\n",
      "Epoch [2070/10000], Loss: 0.1108\n",
      "Epoch [2080/10000], Loss: 0.1097\n",
      "Epoch [2090/10000], Loss: 0.1097\n",
      "Epoch [2100/10000], Loss: 0.1101\n",
      "Epoch [2110/10000], Loss: 0.1102\n",
      "Epoch [2120/10000], Loss: 0.1099\n",
      "Epoch [2130/10000], Loss: 0.1128\n",
      "Epoch [2140/10000], Loss: 0.1105\n",
      "Epoch [2150/10000], Loss: 0.1092\n",
      "Epoch [2160/10000], Loss: 0.1092\n",
      "Epoch [2170/10000], Loss: 0.1082\n",
      "Epoch [2180/10000], Loss: 0.1128\n",
      "Epoch [2190/10000], Loss: 0.1093\n",
      "Epoch [2200/10000], Loss: 0.1126\n",
      "Epoch [2210/10000], Loss: 0.1110\n",
      "Epoch [2220/10000], Loss: 0.1093\n",
      "Epoch [2230/10000], Loss: 0.1125\n",
      "Epoch [2240/10000], Loss: 0.1111\n",
      "Epoch [2250/10000], Loss: 0.1095\n",
      "Epoch [2260/10000], Loss: 0.1096\n",
      "Epoch [2270/10000], Loss: 0.1082\n",
      "Epoch [2280/10000], Loss: 0.1082\n",
      "Epoch [2290/10000], Loss: 0.1098\n",
      "Epoch [2300/10000], Loss: 0.1080\n",
      "Epoch [2310/10000], Loss: 0.1080\n",
      "Epoch [2320/10000], Loss: 0.1090\n",
      "Epoch [2330/10000], Loss: 0.1091\n",
      "Epoch [2340/10000], Loss: 0.1101\n",
      "Epoch [2350/10000], Loss: 0.1081\n",
      "Epoch [2360/10000], Loss: 0.1089\n",
      "Epoch [2370/10000], Loss: 0.1084\n",
      "Epoch [2380/10000], Loss: 0.1080\n",
      "Epoch [2390/10000], Loss: 0.1087\n",
      "Epoch [2400/10000], Loss: 0.1088\n",
      "Epoch [2410/10000], Loss: 0.1103\n",
      "Epoch [2420/10000], Loss: 0.1078\n",
      "Epoch [2430/10000], Loss: 0.1087\n",
      "Epoch [2440/10000], Loss: 0.1070\n",
      "Epoch [2450/10000], Loss: 0.1103\n",
      "Epoch [2460/10000], Loss: 0.1085\n",
      "Epoch [2470/10000], Loss: 0.1115\n",
      "Epoch [2480/10000], Loss: 0.1074\n",
      "Epoch [2490/10000], Loss: 0.1082\n",
      "Epoch [2500/10000], Loss: 0.1110\n",
      "Epoch [2510/10000], Loss: 0.1076\n",
      "Epoch [2520/10000], Loss: 0.1077\n",
      "Epoch [2530/10000], Loss: 0.1081\n",
      "Epoch [2540/10000], Loss: 0.1087\n",
      "Epoch [2550/10000], Loss: 0.1086\n",
      "Epoch [2560/10000], Loss: 0.1078\n",
      "Epoch [2570/10000], Loss: 0.1075\n",
      "Epoch [2580/10000], Loss: 0.1072\n",
      "Epoch [2590/10000], Loss: 0.1086\n",
      "Epoch [2600/10000], Loss: 0.1078\n",
      "Epoch [2610/10000], Loss: 0.1075\n",
      "Epoch [2620/10000], Loss: 0.1121\n",
      "Epoch [2630/10000], Loss: 0.1069\n",
      "Epoch [2640/10000], Loss: 0.1066\n",
      "Epoch [2650/10000], Loss: 0.1083\n",
      "Epoch [2660/10000], Loss: 0.1070\n",
      "Epoch [2670/10000], Loss: 0.1091\n",
      "Epoch [2680/10000], Loss: 0.1070\n",
      "Epoch [2690/10000], Loss: 0.1072\n",
      "Epoch [2700/10000], Loss: 0.1070\n",
      "Epoch [2710/10000], Loss: 0.1067\n",
      "Epoch [2720/10000], Loss: 0.1072\n",
      "Epoch [2730/10000], Loss: 0.1079\n",
      "Epoch [2740/10000], Loss: 0.1072\n",
      "Epoch [2750/10000], Loss: 0.1092\n",
      "Epoch [2760/10000], Loss: 0.1073\n",
      "Epoch [2770/10000], Loss: 0.1067\n",
      "Epoch [2780/10000], Loss: 0.1060\n",
      "Epoch [2790/10000], Loss: 0.1067\n",
      "Epoch [2800/10000], Loss: 0.1070\n",
      "Epoch [2810/10000], Loss: 0.1066\n",
      "Epoch [2820/10000], Loss: 0.1071\n",
      "Epoch [2830/10000], Loss: 0.1078\n",
      "Epoch [2840/10000], Loss: 0.1080\n",
      "Epoch [2850/10000], Loss: 0.1065\n",
      "Epoch [2860/10000], Loss: 0.1076\n",
      "Epoch [2870/10000], Loss: 0.1107\n",
      "Epoch [2880/10000], Loss: 0.1064\n",
      "Epoch [2890/10000], Loss: 0.1060\n",
      "Epoch [2900/10000], Loss: 0.1065\n",
      "Epoch [2910/10000], Loss: 0.1062\n",
      "Epoch [2920/10000], Loss: 0.1065\n",
      "Epoch [2930/10000], Loss: 0.1068\n",
      "Epoch [2940/10000], Loss: 0.1075\n",
      "Epoch [2950/10000], Loss: 0.1062\n",
      "Epoch [2960/10000], Loss: 0.1071\n",
      "Epoch [2970/10000], Loss: 0.1077\n",
      "Epoch [2980/10000], Loss: 0.1066\n",
      "Epoch [2990/10000], Loss: 0.1068\n",
      "Epoch [3000/10000], Loss: 0.1054\n",
      "Epoch [3010/10000], Loss: 0.1078\n",
      "Epoch [3020/10000], Loss: 0.1068\n",
      "Epoch [3030/10000], Loss: 0.1058\n",
      "Epoch [3040/10000], Loss: 0.1062\n",
      "Epoch [3050/10000], Loss: 0.1056\n",
      "Epoch [3060/10000], Loss: 0.1085\n",
      "Epoch [3070/10000], Loss: 0.1074\n",
      "Epoch [3080/10000], Loss: 0.1078\n",
      "Epoch [3090/10000], Loss: 0.1053\n",
      "Epoch [3100/10000], Loss: 0.1066\n",
      "Epoch [3110/10000], Loss: 0.1079\n",
      "Epoch [3120/10000], Loss: 0.1062\n",
      "Epoch [3130/10000], Loss: 0.1068\n",
      "Epoch [3140/10000], Loss: 0.1047\n",
      "Epoch [3150/10000], Loss: 0.1079\n",
      "Epoch [3160/10000], Loss: 0.1083\n",
      "Epoch [3170/10000], Loss: 0.1062\n",
      "Epoch [3180/10000], Loss: 0.1069\n",
      "Epoch [3190/10000], Loss: 0.1083\n",
      "Epoch [3200/10000], Loss: 0.1059\n",
      "Epoch [3210/10000], Loss: 0.1056\n",
      "Epoch [3220/10000], Loss: 0.1097\n",
      "Epoch [3230/10000], Loss: 0.1055\n",
      "Epoch [3240/10000], Loss: 0.1059\n",
      "Epoch [3250/10000], Loss: 0.1051\n",
      "Epoch [3260/10000], Loss: 0.1040\n",
      "Epoch [3270/10000], Loss: 0.1048\n",
      "Epoch [3280/10000], Loss: 0.1046\n",
      "Epoch [3290/10000], Loss: 0.1075\n",
      "Epoch [3300/10000], Loss: 0.1070\n",
      "Epoch [3310/10000], Loss: 0.1054\n",
      "Epoch [3320/10000], Loss: 0.1041\n",
      "Epoch [3330/10000], Loss: 0.1065\n",
      "Epoch [3340/10000], Loss: 0.1040\n",
      "Epoch [3350/10000], Loss: 0.1064\n",
      "Epoch [3360/10000], Loss: 0.1056\n",
      "Epoch [3370/10000], Loss: 0.1050\n",
      "Epoch [3380/10000], Loss: 0.1068\n",
      "Epoch [3390/10000], Loss: 0.1050\n",
      "Epoch [3400/10000], Loss: 0.1066\n",
      "Epoch [3410/10000], Loss: 0.1050\n",
      "Epoch [3420/10000], Loss: 0.1043\n",
      "Epoch [3430/10000], Loss: 0.1046\n",
      "Epoch [3440/10000], Loss: 0.1058\n",
      "Epoch [3450/10000], Loss: 0.1056\n",
      "Epoch [3460/10000], Loss: 0.1045\n",
      "Epoch [3470/10000], Loss: 0.1037\n",
      "Epoch [3480/10000], Loss: 0.1044\n",
      "Epoch [3490/10000], Loss: 0.1053\n",
      "Epoch [3500/10000], Loss: 0.1034\n",
      "Epoch [3510/10000], Loss: 0.1053\n",
      "Epoch [3520/10000], Loss: 0.1057\n",
      "Epoch [3530/10000], Loss: 0.1061\n",
      "Epoch [3540/10000], Loss: 0.1055\n",
      "Epoch [3550/10000], Loss: 0.1038\n",
      "Epoch [3560/10000], Loss: 0.1061\n",
      "Epoch [3570/10000], Loss: 0.1054\n",
      "Epoch [3580/10000], Loss: 0.1036\n",
      "Epoch [3590/10000], Loss: 0.1066\n",
      "Epoch [3600/10000], Loss: 0.1038\n",
      "Epoch [3610/10000], Loss: 0.1050\n",
      "Epoch [3620/10000], Loss: 0.1037\n",
      "Epoch [3630/10000], Loss: 0.1043\n",
      "Epoch [3640/10000], Loss: 0.1039\n",
      "Epoch [3650/10000], Loss: 0.1042\n",
      "Epoch [3660/10000], Loss: 0.1043\n",
      "Epoch [3670/10000], Loss: 0.1042\n",
      "Epoch [3680/10000], Loss: 0.1032\n",
      "Epoch [3690/10000], Loss: 0.1044\n",
      "Epoch [3700/10000], Loss: 0.1058\n",
      "Epoch [3710/10000], Loss: 0.1051\n",
      "Epoch [3720/10000], Loss: 0.1033\n",
      "Epoch [3730/10000], Loss: 0.1047\n",
      "Epoch [3740/10000], Loss: 0.1045\n",
      "Epoch [3750/10000], Loss: 0.1051\n",
      "Epoch [3760/10000], Loss: 0.1042\n",
      "Epoch [3770/10000], Loss: 0.1035\n",
      "Epoch [3780/10000], Loss: 0.1036\n",
      "Epoch [3790/10000], Loss: 0.1024\n",
      "Epoch [3800/10000], Loss: 0.1034\n",
      "Epoch [3810/10000], Loss: 0.1043\n",
      "Epoch [3820/10000], Loss: 0.1044\n",
      "Epoch [3830/10000], Loss: 0.1037\n",
      "Epoch [3840/10000], Loss: 0.1034\n",
      "Epoch [3850/10000], Loss: 0.1046\n",
      "Epoch [3860/10000], Loss: 0.1032\n",
      "Epoch [3870/10000], Loss: 0.1047\n",
      "Epoch [3880/10000], Loss: 0.1029\n",
      "Epoch [3890/10000], Loss: 0.1056\n",
      "Epoch [3900/10000], Loss: 0.1038\n",
      "Epoch [3910/10000], Loss: 0.1028\n",
      "Epoch [3920/10000], Loss: 0.1034\n",
      "Epoch [3930/10000], Loss: 0.1047\n",
      "Epoch [3940/10000], Loss: 0.1068\n",
      "Epoch [3950/10000], Loss: 0.1026\n",
      "Epoch [3960/10000], Loss: 0.1038\n",
      "Epoch [3970/10000], Loss: 0.1042\n",
      "Epoch [3980/10000], Loss: 0.1027\n",
      "Epoch [3990/10000], Loss: 0.1044\n",
      "Epoch [4000/10000], Loss: 0.1039\n",
      "Epoch [4010/10000], Loss: 0.1032\n",
      "Epoch [4020/10000], Loss: 0.1034\n",
      "Epoch [4030/10000], Loss: 0.1043\n",
      "Epoch [4040/10000], Loss: 0.1030\n",
      "Epoch [4050/10000], Loss: 0.1036\n",
      "Epoch [4060/10000], Loss: 0.1038\n",
      "Epoch [4070/10000], Loss: 0.1046\n",
      "Epoch [4080/10000], Loss: 0.1026\n",
      "Epoch [4090/10000], Loss: 0.1030\n",
      "Epoch [4100/10000], Loss: 0.1035\n",
      "Epoch [4110/10000], Loss: 0.1033\n",
      "Epoch [4120/10000], Loss: 0.1032\n",
      "Epoch [4130/10000], Loss: 0.1042\n",
      "Epoch [4140/10000], Loss: 0.1041\n",
      "Epoch [4150/10000], Loss: 0.1027\n",
      "Epoch [4160/10000], Loss: 0.1024\n",
      "Epoch [4170/10000], Loss: 0.1021\n",
      "Epoch [4180/10000], Loss: 0.1049\n",
      "Epoch [4190/10000], Loss: 0.1020\n",
      "Epoch [4200/10000], Loss: 0.1036\n",
      "Epoch [4210/10000], Loss: 0.1034\n",
      "Epoch [4220/10000], Loss: 0.1037\n",
      "Epoch [4230/10000], Loss: 0.1032\n",
      "Epoch [4240/10000], Loss: 0.1040\n",
      "Epoch [4250/10000], Loss: 0.1023\n",
      "Epoch [4260/10000], Loss: 0.1030\n",
      "Epoch [4270/10000], Loss: 0.1034\n",
      "Epoch [4280/10000], Loss: 0.1038\n",
      "Epoch [4290/10000], Loss: 0.1031\n",
      "Epoch [4300/10000], Loss: 0.1027\n",
      "Epoch [4310/10000], Loss: 0.1032\n",
      "Epoch [4320/10000], Loss: 0.1043\n",
      "Epoch [4330/10000], Loss: 0.1053\n",
      "Epoch [4340/10000], Loss: 0.1030\n",
      "Epoch [4350/10000], Loss: 0.1022\n",
      "Epoch [4360/10000], Loss: 0.1049\n",
      "Epoch [4370/10000], Loss: 0.1016\n",
      "Epoch [4380/10000], Loss: 0.1048\n",
      "Epoch [4390/10000], Loss: 0.1023\n",
      "Epoch [4400/10000], Loss: 0.1030\n",
      "Epoch [4410/10000], Loss: 0.1029\n",
      "Epoch [4420/10000], Loss: 0.1018\n",
      "Epoch [4430/10000], Loss: 0.1040\n",
      "Epoch [4440/10000], Loss: 0.1028\n",
      "Epoch [4450/10000], Loss: 0.1055\n",
      "Epoch [4460/10000], Loss: 0.1046\n",
      "Epoch [4470/10000], Loss: 0.1019\n",
      "Epoch [4480/10000], Loss: 0.1031\n",
      "Epoch [4490/10000], Loss: 0.1026\n",
      "Epoch [4500/10000], Loss: 0.1029\n",
      "Epoch [4510/10000], Loss: 0.1029\n",
      "Epoch [4520/10000], Loss: 0.1022\n",
      "Epoch [4530/10000], Loss: 0.1029\n",
      "Epoch [4540/10000], Loss: 0.1066\n",
      "Epoch [4550/10000], Loss: 0.1023\n",
      "Epoch [4560/10000], Loss: 0.1024\n",
      "Epoch [4570/10000], Loss: 0.1041\n",
      "Epoch [4580/10000], Loss: 0.1029\n",
      "Epoch [4590/10000], Loss: 0.1016\n",
      "Epoch [4600/10000], Loss: 0.1034\n",
      "Epoch [4610/10000], Loss: 0.1017\n",
      "Epoch [4620/10000], Loss: 0.1025\n",
      "Epoch [4630/10000], Loss: 0.1029\n",
      "Epoch [4640/10000], Loss: 0.1019\n",
      "Epoch [4650/10000], Loss: 0.1044\n",
      "Epoch [4660/10000], Loss: 0.1024\n",
      "Epoch [4670/10000], Loss: 0.1026\n",
      "Epoch [4680/10000], Loss: 0.1015\n",
      "Epoch [4690/10000], Loss: 0.1025\n",
      "Epoch [4700/10000], Loss: 0.1018\n",
      "Epoch [4710/10000], Loss: 0.1050\n",
      "Epoch [4720/10000], Loss: 0.1034\n",
      "Epoch [4730/10000], Loss: 0.1028\n",
      "Epoch [4740/10000], Loss: 0.1013\n",
      "Epoch [4750/10000], Loss: 0.1065\n",
      "Epoch [4760/10000], Loss: 0.1017\n",
      "Epoch [4770/10000], Loss: 0.1012\n",
      "Epoch [4780/10000], Loss: 0.1031\n",
      "Epoch [4790/10000], Loss: 0.1014\n",
      "Epoch [4800/10000], Loss: 0.1051\n",
      "Epoch [4810/10000], Loss: 0.1031\n",
      "Epoch [4820/10000], Loss: 0.1014\n",
      "Epoch [4830/10000], Loss: 0.1027\n",
      "Epoch [4840/10000], Loss: 0.1038\n",
      "Epoch [4850/10000], Loss: 0.1022\n",
      "Epoch [4860/10000], Loss: 0.1026\n",
      "Epoch [4870/10000], Loss: 0.1021\n",
      "Epoch [4880/10000], Loss: 0.1033\n",
      "Epoch [4890/10000], Loss: 0.1025\n",
      "Epoch [4900/10000], Loss: 0.1023\n",
      "Epoch [4910/10000], Loss: 0.1018\n",
      "Epoch [4920/10000], Loss: 0.1052\n",
      "Epoch [4930/10000], Loss: 0.1020\n",
      "Epoch [4940/10000], Loss: 0.1012\n",
      "Epoch [4950/10000], Loss: 0.1017\n",
      "Epoch [4960/10000], Loss: 0.1037\n",
      "Epoch [4970/10000], Loss: 0.1026\n",
      "Epoch [4980/10000], Loss: 0.1027\n",
      "Epoch [4990/10000], Loss: 0.1024\n",
      "Epoch [5000/10000], Loss: 0.1013\n",
      "Epoch [5010/10000], Loss: 0.1021\n",
      "Epoch [5020/10000], Loss: 0.1005\n",
      "Epoch [5030/10000], Loss: 0.1026\n",
      "Epoch [5040/10000], Loss: 0.1037\n",
      "Epoch [5050/10000], Loss: 0.1017\n",
      "Epoch [5060/10000], Loss: 0.1031\n",
      "Epoch [5070/10000], Loss: 0.1024\n",
      "Epoch [5080/10000], Loss: 0.1012\n",
      "Epoch [5090/10000], Loss: 0.1024\n",
      "Epoch [5100/10000], Loss: 0.1023\n",
      "Epoch [5110/10000], Loss: 0.1022\n",
      "Epoch [5120/10000], Loss: 0.1016\n",
      "Epoch [5130/10000], Loss: 0.1005\n",
      "Epoch [5140/10000], Loss: 0.1014\n",
      "Epoch [5150/10000], Loss: 0.1028\n",
      "Epoch [5160/10000], Loss: 0.1035\n",
      "Epoch [5170/10000], Loss: 0.1018\n",
      "Epoch [5180/10000], Loss: 0.1008\n",
      "Epoch [5190/10000], Loss: 0.1005\n",
      "Epoch [5200/10000], Loss: 0.1020\n",
      "Epoch [5210/10000], Loss: 0.1018\n",
      "Epoch [5220/10000], Loss: 0.1032\n",
      "Epoch [5230/10000], Loss: 0.1019\n",
      "Epoch [5240/10000], Loss: 0.1020\n",
      "Epoch [5250/10000], Loss: 0.1003\n",
      "Epoch [5260/10000], Loss: 0.1021\n",
      "Epoch [5270/10000], Loss: 0.1003\n",
      "Epoch [5280/10000], Loss: 0.1031\n",
      "Epoch [5290/10000], Loss: 0.1038\n",
      "Epoch [5300/10000], Loss: 0.1037\n",
      "Epoch [5310/10000], Loss: 0.1022\n",
      "Epoch [5320/10000], Loss: 0.1009\n",
      "Epoch [5330/10000], Loss: 0.1007\n",
      "Epoch [5340/10000], Loss: 0.1039\n",
      "Epoch [5350/10000], Loss: 0.1015\n",
      "Epoch [5360/10000], Loss: 0.1010\n",
      "Epoch [5370/10000], Loss: 0.1042\n",
      "Epoch [5380/10000], Loss: 0.1010\n",
      "Epoch [5390/10000], Loss: 0.1034\n",
      "Epoch [5400/10000], Loss: 0.1020\n",
      "Epoch [5410/10000], Loss: 0.1026\n",
      "Epoch [5420/10000], Loss: 0.1017\n",
      "Epoch [5430/10000], Loss: 0.1034\n",
      "Epoch [5440/10000], Loss: 0.1011\n",
      "Epoch [5450/10000], Loss: 0.1025\n",
      "Epoch [5460/10000], Loss: 0.1018\n",
      "Epoch [5470/10000], Loss: 0.1019\n",
      "Epoch [5480/10000], Loss: 0.1023\n",
      "Epoch [5490/10000], Loss: 0.1012\n",
      "Epoch [5500/10000], Loss: 0.1003\n",
      "Epoch [5510/10000], Loss: 0.1031\n",
      "Epoch [5520/10000], Loss: 0.1027\n",
      "Epoch [5530/10000], Loss: 0.1017\n",
      "Epoch [5540/10000], Loss: 0.1015\n",
      "Epoch [5550/10000], Loss: 0.1017\n",
      "Epoch [5560/10000], Loss: 0.1015\n",
      "Epoch [5570/10000], Loss: 0.1007\n",
      "Epoch [5580/10000], Loss: 0.1057\n",
      "Epoch [5590/10000], Loss: 0.1015\n",
      "Epoch [5600/10000], Loss: 0.1012\n",
      "Epoch [5610/10000], Loss: 0.1034\n",
      "Epoch [5620/10000], Loss: 0.1012\n",
      "Epoch [5630/10000], Loss: 0.1018\n",
      "Epoch [5640/10000], Loss: 0.1012\n",
      "Epoch [5650/10000], Loss: 0.1005\n",
      "Epoch [5660/10000], Loss: 0.1028\n",
      "Epoch [5670/10000], Loss: 0.1031\n",
      "Epoch [5680/10000], Loss: 0.1000\n",
      "Epoch [5690/10000], Loss: 0.1003\n",
      "Epoch [5700/10000], Loss: 0.1028\n",
      "Epoch [5710/10000], Loss: 0.1009\n",
      "Epoch [5720/10000], Loss: 0.1013\n",
      "Epoch [5730/10000], Loss: 0.1005\n",
      "Epoch [5740/10000], Loss: 0.1032\n",
      "Epoch [5750/10000], Loss: 0.1014\n",
      "Epoch [5760/10000], Loss: 0.0997\n",
      "Epoch [5770/10000], Loss: 0.1016\n",
      "Epoch [5780/10000], Loss: 0.1013\n",
      "Epoch [5790/10000], Loss: 0.1012\n",
      "Epoch [5800/10000], Loss: 0.1036\n",
      "Epoch [5810/10000], Loss: 0.1044\n",
      "Epoch [5820/10000], Loss: 0.1009\n",
      "Epoch [5830/10000], Loss: 0.1023\n",
      "Epoch [5840/10000], Loss: 0.1011\n",
      "Epoch [5850/10000], Loss: 0.1031\n",
      "Epoch [5860/10000], Loss: 0.1003\n",
      "Epoch [5870/10000], Loss: 0.1002\n",
      "Epoch [5880/10000], Loss: 0.1010\n",
      "Epoch [5890/10000], Loss: 0.1019\n",
      "Epoch [5900/10000], Loss: 0.1025\n",
      "Epoch [5910/10000], Loss: 0.1002\n",
      "Epoch [5920/10000], Loss: 0.1021\n",
      "Epoch [5930/10000], Loss: 0.0997\n",
      "Epoch [5940/10000], Loss: 0.1004\n",
      "Epoch [5950/10000], Loss: 0.1005\n",
      "Epoch [5960/10000], Loss: 0.0998\n",
      "Epoch [5970/10000], Loss: 0.0990\n",
      "Epoch [5980/10000], Loss: 0.1007\n",
      "Epoch [5990/10000], Loss: 0.1004\n",
      "Epoch [6000/10000], Loss: 0.1012\n",
      "Epoch [6010/10000], Loss: 0.1012\n",
      "Epoch [6020/10000], Loss: 0.0994\n",
      "Epoch [6030/10000], Loss: 0.1021\n",
      "Epoch [6040/10000], Loss: 0.1013\n",
      "Epoch [6050/10000], Loss: 0.1003\n",
      "Epoch [6060/10000], Loss: 0.1022\n",
      "Epoch [6070/10000], Loss: 0.1010\n",
      "Epoch [6080/10000], Loss: 0.1022\n",
      "Epoch [6090/10000], Loss: 0.1005\n",
      "Epoch [6100/10000], Loss: 0.1015\n",
      "Epoch [6110/10000], Loss: 0.1000\n",
      "Epoch [6120/10000], Loss: 0.0997\n",
      "Epoch [6130/10000], Loss: 0.0998\n",
      "Epoch [6140/10000], Loss: 0.1005\n",
      "Epoch [6150/10000], Loss: 0.1008\n",
      "Epoch [6160/10000], Loss: 0.1014\n",
      "Epoch [6170/10000], Loss: 0.0996\n",
      "Epoch [6180/10000], Loss: 0.0997\n",
      "Epoch [6190/10000], Loss: 0.1023\n",
      "Epoch [6200/10000], Loss: 0.0999\n",
      "Epoch [6210/10000], Loss: 0.1018\n",
      "Epoch [6220/10000], Loss: 0.1001\n",
      "Epoch [6230/10000], Loss: 0.1025\n",
      "Epoch [6240/10000], Loss: 0.1006\n",
      "Epoch [6250/10000], Loss: 0.1024\n",
      "Epoch [6260/10000], Loss: 0.0992\n",
      "Epoch [6270/10000], Loss: 0.1006\n",
      "Epoch [6280/10000], Loss: 0.1011\n",
      "Epoch [6290/10000], Loss: 0.1030\n",
      "Epoch [6300/10000], Loss: 0.1002\n",
      "Epoch [6310/10000], Loss: 0.0993\n",
      "Epoch [6320/10000], Loss: 0.1033\n",
      "Epoch [6330/10000], Loss: 0.1001\n",
      "Epoch [6340/10000], Loss: 0.1026\n",
      "Epoch [6350/10000], Loss: 0.1008\n",
      "Epoch [6360/10000], Loss: 0.0999\n",
      "Epoch [6370/10000], Loss: 0.0999\n",
      "Epoch [6380/10000], Loss: 0.1026\n",
      "Epoch [6390/10000], Loss: 0.0998\n",
      "Epoch [6400/10000], Loss: 0.1000\n",
      "Epoch [6410/10000], Loss: 0.1012\n",
      "Epoch [6420/10000], Loss: 0.1003\n",
      "Epoch [6430/10000], Loss: 0.1007\n",
      "Epoch [6440/10000], Loss: 0.1012\n",
      "Epoch [6450/10000], Loss: 0.0997\n",
      "Epoch [6460/10000], Loss: 0.1032\n",
      "Epoch [6470/10000], Loss: 0.0991\n",
      "Epoch [6480/10000], Loss: 0.1002\n",
      "Epoch [6490/10000], Loss: 0.1025\n",
      "Epoch [6500/10000], Loss: 0.1006\n",
      "Epoch [6510/10000], Loss: 0.1013\n",
      "Epoch [6520/10000], Loss: 0.1015\n",
      "Epoch [6530/10000], Loss: 0.0998\n",
      "Epoch [6540/10000], Loss: 0.1018\n",
      "Epoch [6550/10000], Loss: 0.0999\n",
      "Epoch [6560/10000], Loss: 0.0993\n",
      "Epoch [6570/10000], Loss: 0.0995\n",
      "Epoch [6580/10000], Loss: 0.0987\n",
      "Epoch [6590/10000], Loss: 0.1040\n",
      "Epoch [6600/10000], Loss: 0.1003\n",
      "Epoch [6610/10000], Loss: 0.1030\n",
      "Epoch [6620/10000], Loss: 0.1009\n",
      "Epoch [6630/10000], Loss: 0.1002\n",
      "Epoch [6640/10000], Loss: 0.1006\n",
      "Epoch [6650/10000], Loss: 0.0998\n",
      "Epoch [6660/10000], Loss: 0.1015\n",
      "Epoch [6670/10000], Loss: 0.1005\n",
      "Epoch [6680/10000], Loss: 0.1002\n",
      "Epoch [6690/10000], Loss: 0.0997\n",
      "Epoch [6700/10000], Loss: 0.0998\n",
      "Epoch [6710/10000], Loss: 0.1000\n",
      "Epoch [6720/10000], Loss: 0.0989\n",
      "Epoch [6730/10000], Loss: 0.0999\n",
      "Epoch [6740/10000], Loss: 0.0996\n",
      "Epoch [6750/10000], Loss: 0.0996\n",
      "Epoch [6760/10000], Loss: 0.1003\n",
      "Epoch [6770/10000], Loss: 0.1003\n",
      "Epoch [6780/10000], Loss: 0.1003\n",
      "Epoch [6790/10000], Loss: 0.1010\n",
      "Epoch [6800/10000], Loss: 0.0995\n",
      "Epoch [6810/10000], Loss: 0.0994\n",
      "Epoch [6820/10000], Loss: 0.1026\n",
      "Epoch [6830/10000], Loss: 0.0992\n",
      "Epoch [6840/10000], Loss: 0.1006\n",
      "Epoch [6850/10000], Loss: 0.0991\n",
      "Epoch [6860/10000], Loss: 0.0997\n",
      "Epoch [6870/10000], Loss: 0.1000\n",
      "Epoch [6880/10000], Loss: 0.1018\n",
      "Epoch [6890/10000], Loss: 0.0992\n",
      "Epoch [6900/10000], Loss: 0.1016\n",
      "Epoch [6910/10000], Loss: 0.0994\n",
      "Epoch [6920/10000], Loss: 0.0988\n",
      "Epoch [6930/10000], Loss: 0.1014\n",
      "Epoch [6940/10000], Loss: 0.0993\n",
      "Epoch [6950/10000], Loss: 0.1021\n",
      "Epoch [6960/10000], Loss: 0.1014\n",
      "Epoch [6970/10000], Loss: 0.0986\n",
      "Epoch [6980/10000], Loss: 0.0998\n",
      "Epoch [6990/10000], Loss: 0.0998\n",
      "Epoch [7000/10000], Loss: 0.1026\n",
      "Epoch [7010/10000], Loss: 0.0994\n",
      "Epoch [7020/10000], Loss: 0.1015\n",
      "Epoch [7030/10000], Loss: 0.0989\n",
      "Epoch [7040/10000], Loss: 0.1011\n",
      "Epoch [7050/10000], Loss: 0.1008\n",
      "Epoch [7060/10000], Loss: 0.1001\n",
      "Epoch [7070/10000], Loss: 0.1007\n",
      "Epoch [7080/10000], Loss: 0.0999\n",
      "Epoch [7090/10000], Loss: 0.0993\n",
      "Epoch [7100/10000], Loss: 0.0993\n",
      "Epoch [7110/10000], Loss: 0.1009\n",
      "Epoch [7120/10000], Loss: 0.1005\n",
      "Epoch [7130/10000], Loss: 0.1003\n",
      "Epoch [7140/10000], Loss: 0.0994\n",
      "Epoch [7150/10000], Loss: 0.1007\n",
      "Epoch [7160/10000], Loss: 0.0990\n",
      "Epoch [7170/10000], Loss: 0.1002\n",
      "Epoch [7180/10000], Loss: 0.1004\n",
      "Epoch [7190/10000], Loss: 0.0999\n",
      "Epoch [7200/10000], Loss: 0.1006\n",
      "Epoch [7210/10000], Loss: 0.1004\n",
      "Epoch [7220/10000], Loss: 0.0993\n",
      "Epoch [7230/10000], Loss: 0.0997\n",
      "Epoch [7240/10000], Loss: 0.1008\n",
      "Epoch [7250/10000], Loss: 0.1017\n",
      "Epoch [7260/10000], Loss: 0.1014\n",
      "Epoch [7270/10000], Loss: 0.0993\n",
      "Epoch [7280/10000], Loss: 0.1018\n",
      "Epoch [7290/10000], Loss: 0.1003\n",
      "Epoch [7300/10000], Loss: 0.0998\n",
      "Epoch [7310/10000], Loss: 0.0990\n",
      "Epoch [7320/10000], Loss: 0.1016\n",
      "Epoch [7330/10000], Loss: 0.0991\n",
      "Epoch [7340/10000], Loss: 0.0983\n",
      "Epoch [7350/10000], Loss: 0.0990\n",
      "Epoch [7360/10000], Loss: 0.0994\n",
      "Epoch [7370/10000], Loss: 0.1007\n",
      "Epoch [7380/10000], Loss: 0.0989\n",
      "Epoch [7390/10000], Loss: 0.1015\n",
      "Epoch [7400/10000], Loss: 0.1000\n",
      "Epoch [7410/10000], Loss: 0.0982\n",
      "Epoch [7420/10000], Loss: 0.1004\n",
      "Epoch [7430/10000], Loss: 0.0982\n",
      "Epoch [7440/10000], Loss: 0.0996\n",
      "Epoch [7450/10000], Loss: 0.1006\n",
      "Epoch [7460/10000], Loss: 0.0984\n",
      "Epoch [7470/10000], Loss: 0.1007\n",
      "Epoch [7480/10000], Loss: 0.0994\n",
      "Epoch [7490/10000], Loss: 0.0999\n",
      "Epoch [7500/10000], Loss: 0.0996\n",
      "Epoch [7510/10000], Loss: 0.0997\n",
      "Epoch [7520/10000], Loss: 0.0999\n",
      "Epoch [7530/10000], Loss: 0.1000\n",
      "Epoch [7540/10000], Loss: 0.0989\n",
      "Epoch [7550/10000], Loss: 0.0987\n",
      "Epoch [7560/10000], Loss: 0.0985\n",
      "Epoch [7570/10000], Loss: 0.0997\n",
      "Epoch [7580/10000], Loss: 0.1003\n",
      "Epoch [7590/10000], Loss: 0.0992\n",
      "Epoch [7600/10000], Loss: 0.0987\n",
      "Epoch [7610/10000], Loss: 0.1027\n",
      "Epoch [7620/10000], Loss: 0.0989\n",
      "Epoch [7630/10000], Loss: 0.0993\n",
      "Epoch [7640/10000], Loss: 0.0977\n",
      "Epoch [7650/10000], Loss: 0.1001\n",
      "Epoch [7660/10000], Loss: 0.0990\n",
      "Epoch [7670/10000], Loss: 0.1004\n",
      "Epoch [7680/10000], Loss: 0.0986\n",
      "Epoch [7690/10000], Loss: 0.0987\n",
      "Epoch [7700/10000], Loss: 0.1014\n",
      "Epoch [7710/10000], Loss: 0.0992\n",
      "Epoch [7720/10000], Loss: 0.0980\n",
      "Epoch [7730/10000], Loss: 0.0999\n",
      "Epoch [7740/10000], Loss: 0.0993\n",
      "Epoch [7750/10000], Loss: 0.1008\n",
      "Epoch [7760/10000], Loss: 0.1031\n",
      "Epoch [7770/10000], Loss: 0.0978\n",
      "Epoch [7780/10000], Loss: 0.0998\n",
      "Epoch [7790/10000], Loss: 0.1009\n",
      "Epoch [7800/10000], Loss: 0.0994\n",
      "Epoch [7810/10000], Loss: 0.0989\n",
      "Epoch [7820/10000], Loss: 0.0994\n",
      "Epoch [7830/10000], Loss: 0.0987\n",
      "Epoch [7840/10000], Loss: 0.0989\n",
      "Epoch [7850/10000], Loss: 0.0997\n",
      "Epoch [7860/10000], Loss: 0.0985\n",
      "Epoch [7870/10000], Loss: 0.1001\n",
      "Epoch [7880/10000], Loss: 0.0984\n",
      "Epoch [7890/10000], Loss: 0.0986\n",
      "Epoch [7900/10000], Loss: 0.1006\n",
      "Epoch [7910/10000], Loss: 0.1021\n",
      "Epoch [7920/10000], Loss: 0.1003\n",
      "Epoch [7930/10000], Loss: 0.0988\n",
      "Epoch [7940/10000], Loss: 0.0984\n",
      "Epoch [7950/10000], Loss: 0.1011\n",
      "Epoch [7960/10000], Loss: 0.1007\n",
      "Epoch [7970/10000], Loss: 0.0982\n",
      "Epoch [7980/10000], Loss: 0.0987\n",
      "Epoch [7990/10000], Loss: 0.0996\n",
      "Epoch [8000/10000], Loss: 0.1028\n",
      "Epoch [8010/10000], Loss: 0.0987\n",
      "Epoch [8020/10000], Loss: 0.0989\n",
      "Epoch [8030/10000], Loss: 0.0998\n",
      "Epoch [8040/10000], Loss: 0.0999\n",
      "Epoch [8050/10000], Loss: 0.0995\n",
      "Epoch [8060/10000], Loss: 0.0994\n",
      "Epoch [8070/10000], Loss: 0.0981\n",
      "Epoch [8080/10000], Loss: 0.0994\n",
      "Epoch [8090/10000], Loss: 0.1021\n",
      "Epoch [8100/10000], Loss: 0.1017\n",
      "Epoch [8110/10000], Loss: 0.0981\n",
      "Epoch [8120/10000], Loss: 0.0988\n",
      "Epoch [8130/10000], Loss: 0.0986\n",
      "Epoch [8140/10000], Loss: 0.1016\n",
      "Epoch [8150/10000], Loss: 0.0996\n",
      "Epoch [8160/10000], Loss: 0.1003\n",
      "Epoch [8170/10000], Loss: 0.0995\n",
      "Epoch [8180/10000], Loss: 0.0983\n",
      "Epoch [8190/10000], Loss: 0.0986\n",
      "Epoch [8200/10000], Loss: 0.0989\n",
      "Epoch [8210/10000], Loss: 0.0996\n",
      "Epoch [8220/10000], Loss: 0.0987\n",
      "Epoch [8230/10000], Loss: 0.1044\n",
      "Epoch [8240/10000], Loss: 0.0998\n",
      "Epoch [8250/10000], Loss: 0.0998\n",
      "Epoch [8260/10000], Loss: 0.0985\n",
      "Epoch [8270/10000], Loss: 0.0978\n",
      "Epoch [8280/10000], Loss: 0.1009\n",
      "Epoch [8290/10000], Loss: 0.0989\n",
      "Epoch [8300/10000], Loss: 0.0979\n",
      "Epoch [8310/10000], Loss: 0.1002\n",
      "Epoch [8320/10000], Loss: 0.0984\n",
      "Epoch [8330/10000], Loss: 0.0991\n",
      "Epoch [8340/10000], Loss: 0.0992\n",
      "Epoch [8350/10000], Loss: 0.0991\n",
      "Epoch [8360/10000], Loss: 0.0996\n",
      "Epoch [8370/10000], Loss: 0.0989\n",
      "Epoch [8380/10000], Loss: 0.0993\n",
      "Epoch [8390/10000], Loss: 0.0986\n",
      "Epoch [8400/10000], Loss: 0.1006\n",
      "Epoch [8410/10000], Loss: 0.1013\n",
      "Epoch [8420/10000], Loss: 0.0990\n",
      "Epoch [8430/10000], Loss: 0.0973\n",
      "Epoch [8440/10000], Loss: 0.0979\n",
      "Epoch [8450/10000], Loss: 0.0992\n",
      "Epoch [8460/10000], Loss: 0.0997\n",
      "Epoch [8470/10000], Loss: 0.0987\n",
      "Epoch [8480/10000], Loss: 0.0999\n",
      "Epoch [8490/10000], Loss: 0.0996\n",
      "Epoch [8500/10000], Loss: 0.0982\n",
      "Epoch [8510/10000], Loss: 0.1004\n",
      "Epoch [8520/10000], Loss: 0.1011\n",
      "Epoch [8530/10000], Loss: 0.0986\n",
      "Epoch [8540/10000], Loss: 0.0980\n",
      "Epoch [8550/10000], Loss: 0.0979\n",
      "Epoch [8560/10000], Loss: 0.0989\n",
      "Epoch [8570/10000], Loss: 0.1023\n",
      "Epoch [8580/10000], Loss: 0.0979\n",
      "Epoch [8590/10000], Loss: 0.0975\n",
      "Epoch [8600/10000], Loss: 0.0987\n",
      "Epoch [8610/10000], Loss: 0.0989\n",
      "Epoch [8620/10000], Loss: 0.1017\n",
      "Epoch [8630/10000], Loss: 0.0992\n",
      "Epoch [8640/10000], Loss: 0.0987\n",
      "Epoch [8650/10000], Loss: 0.0982\n",
      "Epoch [8660/10000], Loss: 0.0997\n",
      "Epoch [8670/10000], Loss: 0.0990\n",
      "Epoch [8680/10000], Loss: 0.0983\n",
      "Epoch [8690/10000], Loss: 0.0985\n",
      "Epoch [8700/10000], Loss: 0.0980\n",
      "Epoch [8710/10000], Loss: 0.0993\n",
      "Epoch [8720/10000], Loss: 0.1006\n",
      "Epoch [8730/10000], Loss: 0.0981\n",
      "Epoch [8740/10000], Loss: 0.0992\n",
      "Epoch [8750/10000], Loss: 0.0999\n",
      "Epoch [8760/10000], Loss: 0.0987\n",
      "Epoch [8770/10000], Loss: 0.0988\n",
      "Epoch [8780/10000], Loss: 0.0994\n",
      "Epoch [8790/10000], Loss: 0.0987\n",
      "Epoch [8800/10000], Loss: 0.0990\n",
      "Epoch [8810/10000], Loss: 0.0981\n",
      "Epoch [8820/10000], Loss: 0.0988\n",
      "Epoch [8830/10000], Loss: 0.0975\n",
      "Epoch [8840/10000], Loss: 0.0981\n",
      "Epoch [8850/10000], Loss: 0.0995\n",
      "Epoch [8860/10000], Loss: 0.0983\n",
      "Epoch [8870/10000], Loss: 0.0992\n",
      "Epoch [8880/10000], Loss: 0.0968\n",
      "Epoch [8890/10000], Loss: 0.1038\n",
      "Epoch [8900/10000], Loss: 0.1007\n",
      "Epoch [8910/10000], Loss: 0.0979\n",
      "Epoch [8920/10000], Loss: 0.0983\n",
      "Epoch [8930/10000], Loss: 0.0993\n",
      "Epoch [8940/10000], Loss: 0.0976\n",
      "Epoch [8950/10000], Loss: 0.1002\n",
      "Epoch [8960/10000], Loss: 0.0982\n",
      "Epoch [8970/10000], Loss: 0.0983\n",
      "Epoch [8980/10000], Loss: 0.1019\n",
      "Epoch [8990/10000], Loss: 0.0984\n",
      "Epoch [9000/10000], Loss: 0.0984\n",
      "Epoch [9010/10000], Loss: 0.0996\n",
      "Epoch [9020/10000], Loss: 0.0979\n",
      "Epoch [9030/10000], Loss: 0.0981\n",
      "Epoch [9040/10000], Loss: 0.0986\n",
      "Epoch [9050/10000], Loss: 0.0987\n",
      "Epoch [9060/10000], Loss: 0.0989\n",
      "Epoch [9070/10000], Loss: 0.0983\n",
      "Epoch [9080/10000], Loss: 0.0999\n",
      "Epoch [9090/10000], Loss: 0.0994\n",
      "Epoch [9100/10000], Loss: 0.0984\n",
      "Epoch [9110/10000], Loss: 0.0975\n",
      "Epoch [9120/10000], Loss: 0.0987\n",
      "Epoch [9130/10000], Loss: 0.0991\n",
      "Epoch [9140/10000], Loss: 0.0986\n",
      "Epoch [9150/10000], Loss: 0.0971\n",
      "Epoch [9160/10000], Loss: 0.0983\n",
      "Epoch [9170/10000], Loss: 0.0984\n",
      "Epoch [9180/10000], Loss: 0.0989\n",
      "Epoch [9190/10000], Loss: 0.0986\n",
      "Epoch [9200/10000], Loss: 0.0998\n",
      "Epoch [9210/10000], Loss: 0.0986\n",
      "Epoch [9220/10000], Loss: 0.1008\n",
      "Epoch [9230/10000], Loss: 0.0987\n",
      "Epoch [9240/10000], Loss: 0.0983\n",
      "Epoch [9250/10000], Loss: 0.0985\n",
      "Epoch [9260/10000], Loss: 0.0999\n",
      "Epoch [9270/10000], Loss: 0.0976\n",
      "Epoch [9280/10000], Loss: 0.0985\n",
      "Epoch [9290/10000], Loss: 0.0988\n",
      "Epoch [9300/10000], Loss: 0.0998\n",
      "Epoch [9310/10000], Loss: 0.0998\n",
      "Epoch [9320/10000], Loss: 0.0975\n",
      "Epoch [9330/10000], Loss: 0.0989\n",
      "Epoch [9340/10000], Loss: 0.0985\n",
      "Epoch [9350/10000], Loss: 0.0973\n",
      "Epoch [9360/10000], Loss: 0.1022\n",
      "Epoch [9370/10000], Loss: 0.0987\n",
      "Epoch [9380/10000], Loss: 0.0989\n",
      "Epoch [9390/10000], Loss: 0.0979\n",
      "Epoch [9400/10000], Loss: 0.0992\n",
      "Epoch [9410/10000], Loss: 0.0974\n",
      "Epoch [9420/10000], Loss: 0.0988\n",
      "Epoch [9430/10000], Loss: 0.0986\n",
      "Epoch [9440/10000], Loss: 0.0980\n",
      "Epoch [9450/10000], Loss: 0.0985\n",
      "Epoch [9460/10000], Loss: 0.0973\n",
      "Epoch [9470/10000], Loss: 0.0998\n",
      "Epoch [9480/10000], Loss: 0.0992\n",
      "Epoch [9490/10000], Loss: 0.0971\n",
      "Epoch [9500/10000], Loss: 0.0971\n",
      "Epoch [9510/10000], Loss: 0.1024\n",
      "Epoch [9520/10000], Loss: 0.0992\n",
      "Epoch [9530/10000], Loss: 0.0966\n",
      "Epoch [9540/10000], Loss: 0.0972\n",
      "Epoch [9550/10000], Loss: 0.0981\n",
      "Epoch [9560/10000], Loss: 0.1002\n",
      "Epoch [9570/10000], Loss: 0.0986\n",
      "Epoch [9580/10000], Loss: 0.0979\n",
      "Epoch [9590/10000], Loss: 0.0992\n",
      "Epoch [9600/10000], Loss: 0.0996\n",
      "Epoch [9610/10000], Loss: 0.0975\n",
      "Epoch [9620/10000], Loss: 0.0999\n",
      "Epoch [9630/10000], Loss: 0.1006\n",
      "Epoch [9640/10000], Loss: 0.0983\n",
      "Epoch [9650/10000], Loss: 0.0974\n",
      "Epoch [9660/10000], Loss: 0.0978\n",
      "Epoch [9670/10000], Loss: 0.1010\n",
      "Epoch [9680/10000], Loss: 0.0988\n",
      "Epoch [9690/10000], Loss: 0.0974\n",
      "Epoch [9700/10000], Loss: 0.0992\n",
      "Epoch [9710/10000], Loss: 0.0968\n",
      "Epoch [9720/10000], Loss: 0.1024\n",
      "Epoch [9730/10000], Loss: 0.0970\n",
      "Epoch [9740/10000], Loss: 0.0974\n",
      "Epoch [9750/10000], Loss: 0.0996\n",
      "Epoch [9760/10000], Loss: 0.0973\n",
      "Epoch [9770/10000], Loss: 0.1006\n",
      "Epoch [9780/10000], Loss: 0.0993\n",
      "Epoch [9790/10000], Loss: 0.0980\n",
      "Epoch [9800/10000], Loss: 0.0987\n",
      "Epoch [9810/10000], Loss: 0.0977\n",
      "Epoch [9820/10000], Loss: 0.0985\n",
      "Epoch [9830/10000], Loss: 0.0995\n",
      "Epoch [9840/10000], Loss: 0.0989\n",
      "Epoch [9850/10000], Loss: 0.0980\n",
      "Epoch [9860/10000], Loss: 0.0978\n",
      "Epoch [9870/10000], Loss: 0.0975\n",
      "Epoch [9880/10000], Loss: 0.0974\n",
      "Epoch [9890/10000], Loss: 0.0979\n",
      "Epoch [9900/10000], Loss: 0.0991\n",
      "Epoch [9910/10000], Loss: 0.0978\n",
      "Epoch [9920/10000], Loss: 0.0968\n",
      "Epoch [9930/10000], Loss: 0.0996\n",
      "Epoch [9940/10000], Loss: 0.1014\n",
      "Epoch [9950/10000], Loss: 0.0982\n",
      "Epoch [9960/10000], Loss: 0.0970\n",
      "Epoch [9970/10000], Loss: 0.0977\n",
      "Epoch [9980/10000], Loss: 0.0971\n",
      "Epoch [9990/10000], Loss: 0.0982\n",
      "Epoch [10000/10000], Loss: 0.0979\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성 및 학습 설정\n",
    "ssae_model = DenoisingSSAE(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ssae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(n_epochs_ssae):\n",
    "    ssae_model.train()\n",
    "    inputs = torch.FloatTensor(X_train_noisy_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "    targets = torch.FloatTensor(X_train_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = ssae_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs_ssae}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNxDAlFBKApN"
   },
   "source": [
    "## 모델 평가함수 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yXpRxDP8W6Zo"
   },
   "outputs": [],
   "source": [
    "# 평가함수 정의\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "        \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "81YDhprUKApO"
   },
   "outputs": [],
   "source": [
    "# Shapley Value 계산, 시각화 함수 정의\n",
    "def evaluate_models_shap1(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    masker = shap.maskers.Independent(X_train_encoded_ssae)\n",
    "    explainer = shap.LinearExplainer(model, masker=masker)\n",
    "    shap_values = explainer(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n",
    "\n",
    "def evaluate_models_shap2(model, X_test_encoded_ssae):\n",
    "    # TreeExplainer를 사용한 Shapley Value 계산\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "D6GIT1bg23Ny"
   },
   "outputs": [],
   "source": [
    "# ShapleyValue 계산, 시각화함수 정의 (TabNet 전용)\n",
    "def evaluate_models_shap_tabnet(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    # DeepExplainer를 사용한 Shapley Value 계산 (TabNet 전용)\n",
    "    explainer = DeepExplainer(model, X_train_encoded_ssae)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증(CV)을 위한 함수 정의\n",
    "def cross_validate_model(model, X, y, cv=cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_list = [f'e{i}' for i in range(latent_size)]\n",
    "X_train_encoded = pd.concat(\n",
    "                    [X_train[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_train[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_train.index, columns=encoded_list)],\n",
    "                    axis=1)\n",
    "\n",
    "X_test_encoded = pd.concat(\n",
    "                    [X_test[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_test[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_test.index, columns=encoded_list)],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scalerenc = StandardScaler()\n",
    "X_train_encoded[encoded_list] = scalerenc.fit_transform(X_train_encoded[encoded_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test_encoded[encoded_list] = scalerenc.transform(X_test_encoded[encoded_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Logistic Regression model:\n",
      "  F1 Score: 0.3426\n",
      "  Accuracy: 0.5862\n",
      "  Precision: 0.2342\n",
      "  Recall: 0.6375\n",
      "  Best Parameters: {'C': 0.004641588833612777, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 124\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# XGBoost with F1 optimization\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# for params in ParameterGrid(common_param_grid):\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#     accuracy, precision, recall, f1, cm = evaluate_model('XGBoost Classifier', xgb_model, params)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# LightGBM with F1 optimization\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m ParameterGrid(common_param_grid):\n\u001b[0;32m--> 124\u001b[0m     accuracy, precision, recall, f1, cm \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLightGBM Classifier\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM Classifier\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m best_results \u001b[38;5;129;01mor\u001b[39;00m f1 \u001b[38;5;241m>\u001b[39m best_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM Classifier\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    126\u001b[0m         best_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM Classifier\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: lgb_model,\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfusion_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m: cm\n\u001b[1;32m    134\u001b[0m         }\n",
      "Cell \u001b[0;32mIn[50], line 31\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model_name, model, param_set)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam_set)\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_encoded)\n\u001b[1;32m     35\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[0;32m~/miniconda/envs/torch/lib/python3.9/site-packages/lightgbm/sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/torch/lib/python3.9/site-packages/lightgbm/sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[0;32m~/miniconda/envs/torch/lib/python3.9/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/torch/lib/python3.9/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression  # 수정된 부분\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 모든 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Stopped training because there are no more leaves\")\n",
    "\n",
    "# Train and evaluate the model with a given parameter set\n",
    "def evaluate_model(model_name, model, param_set):\n",
    "    try:\n",
    "        model.set_params(**param_set)\n",
    "        model.fit(X_train_encoded, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracy, precision, recall, f1, confusion_matrix(y_test, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name} with params: {param_set}. Error: {str(e)}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Define parameter grids for each model\n",
    "logistic_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Regularization strength with 10 values\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],  # Fast solver for logistic regression with L2 regularization\n",
    "    'max_iter': [1000]  # Very high iteration count\n",
    "}\n",
    "\n",
    "# Common parameter grid for XGBoost, LightGBM, and RandomForest\n",
    "common_param_grid = {\n",
    "    'learning_rate': np.logspace(-4, 0, 10),  # Learning rate with 10 values\n",
    "    'reg_alpha': np.logspace(-3, 1, 10),  # L1 regularization with 10 values\n",
    "    'reg_lambda': np.logspace(-3, 1, 10),  # L2 regularization with 10 values\n",
    "    'n_estimators': [1000]  # Very high number of trees\n",
    "}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 10),  # Regularization strength with 10 values\n",
    "    'kernel': ['rbf'],  # RBF kernel is commonly used\n",
    "    'gamma': ['scale'],  # Standard option for gamma\n",
    "    'max_iter': [1000]  # Very high iteration count for convergence\n",
    "}\n",
    "\n",
    "# Evaluate models with parameter grids\n",
    "logistic_model = LogisticRegression(n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(n_jobs=-1, verbosity=0)  # Disable XGBoost output\n",
    "lgb_model = lgb.LGBMClassifier(n_jobs=-1, verbose=-1)  # Disable LightGBM output\n",
    "rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "svm_model = SVC()\n",
    "\n",
    "# Dictionary to store best results for each model\n",
    "best_results = {}\n",
    "\n",
    "# Logistic Regression with F1 optimization\n",
    "for params in ParameterGrid(logistic_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('Logistic Regression', logistic_model, params)\n",
    "    if accuracy is not None and ('Logistic Regression' not in best_results or f1 > best_results['Logistic Regression']['f1_score']):\n",
    "        best_results['Logistic Regression'] = {\n",
    "            'model': logistic_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest Logistic Regression model:\")\n",
    "if 'Logistic Regression' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['Logistic Regression']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['Logistic Regression']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['Logistic Regression']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['Logistic Regression']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['Logistic Regression']['params']}\\n\")\n",
    "\n",
    "# XGBoost with F1 optimization\n",
    "# for params in ParameterGrid(common_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('XGBoost Classifier', xgb_model, params)\n",
    "#     if accuracy is not None and ('XGBoost Classifier' not in best_results or f1 > best_results['XGBoost Classifier']['f1_score']):\n",
    "#         best_results['XGBoost Classifier'] = {\n",
    "#             'model': xgb_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest XGBoost model:\")\n",
    "# if 'XGBoost Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['XGBoost Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['XGBoost Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['XGBoost Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['XGBoost Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['XGBoost Classifier']['params']}\\n\")\n",
    "\n",
    "# LightGBM with F1 optimization\n",
    "for params in ParameterGrid(common_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('LightGBM Classifier', lgb_model, params)\n",
    "    if accuracy is not None and ('LightGBM Classifier' not in best_results or f1 > best_results['LightGBM Classifier']['f1_score']):\n",
    "        best_results['LightGBM Classifier'] = {\n",
    "            'model': lgb_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest LightGBM model:\")\n",
    "if 'LightGBM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['LightGBM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['LightGBM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['LightGBM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['LightGBM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['LightGBM Classifier']['params']}\\n\")\n",
    "\n",
    "# RandomForest with F1 optimization\n",
    "# for params in ParameterGrid(common_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('RandomForest Classifier', rf_model, params)\n",
    "#     if accuracy is not None and ('RandomForest Classifier' not in best_results or f1 > best_results['RandomForest Classifier']['f1_score']):\n",
    "#         best_results['RandomForest Classifier'] = {\n",
    "#             'model': rf_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest RandomForest model:\")\n",
    "# if 'RandomForest Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['RandomForest Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['RandomForest Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['RandomForest Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['RandomForest Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['RandomForest Classifier']['params']}\\n\")\n",
    "\n",
    "# # SVM with F1 optimization\n",
    "# for params in ParameterGrid(svm_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('SVM Classifier', svm_model, params)\n",
    "#     if accuracy is not None and ('SVM Classifier' not in best_results or f1 > best_results['SVM Classifier']['f1_score']):\n",
    "#         best_results['SVM Classifier'] = {\n",
    "#             'model': svm_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest SVM model:\")\n",
    "# if 'SVM Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['SVM Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['SVM Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['SVM Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['SVM Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['SVM Classifier']['params']}\\n\")\n",
    "\n",
    "# # Find the best overall model based on F1 score\n",
    "# best_model_name = max(best_results, key=lambda x: best_results[x]['f1_score'])\n",
    "# best_model_results = best_results[best_model_name]\n",
    "\n",
    "# # Print the best overall model\n",
    "# print(f\"\\nBest overall model: {best_model_name}\")\n",
    "# print(f\"  F1 Score: {best_model_results['f1_score']:.4f}\")\n",
    "# print(f\"  Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "# print(f\"  Precision: {best_model_results['precision']:.4f}\")\n",
    "# print(f\"  Recall: {best_model_results['recall']:.4f}\")\n",
    "# print(f\"  Best Parameters: {best_model_results['params']}\")\n",
    "\n",
    "# # Plot confusion matrix for the best overall model\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=best_model_results['confusion_matrix'])\n",
    "# disp.plot()\n",
    "# plt.title(f\"Confusion Matrix for {best_model_name}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_encoded), len(X_test_encoded)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
