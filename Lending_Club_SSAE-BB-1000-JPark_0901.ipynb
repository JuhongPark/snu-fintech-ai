{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Q35P2ummXim"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/JuhongPark/snu-fintech-ai/blob/main/Lending_Club_SSAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WyoEfeVoyh-"
   },
   "source": [
    "# 데이터 가져오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brnach 지정\n",
    "branch = 'develop'\n",
    "\n",
    "# 라이브러리 설치 (2분 가량 소요됨)\n",
    "#!pip install shap -q\n",
    "#!pip install pytorch_tabnet -q\n",
    "\n",
    "# 필요 파일 다운로드\n",
    "#!wget https://raw.githubusercontent.com/JuhongPark/snu-fintech-ai/{branch}/LC_Data_Cleaned_0829.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # seed 값 설정\n",
    "is_test = False  # 테스트런 설정\n",
    "\n",
    "# 입력 변수 파라미터\n",
    "target = 'loan_status_encoded'\n",
    "drop_list = ['id', 'int_rate', 'installment', 'sub_grade', 'grade', 'tbond_int', 'year', 'term', 'loan_status', 'zip_code']\n",
    "feature_list = ['loan_amnt', 'emp_length', 'revol_util', 'pub_rec', 'fico_range_high', 'fico_range_low', 'percent_bc_gt_75', 'annual_inc',\n",
    "                'dti', 'delinq_2yrs', 'open_acc', 'revol_bal', 'total_acc', 'inq_last_6mths'] # 'fico_range_low'제외\n",
    "cat_list = ['purpose', 'addr_state', 'initial_list_status', 'home_ownership']\n",
    "\n",
    "# 대출 이후 변수\n",
    "post_list = ['Funded_amnt', 'funded_amnt_inv', 'collection_recovery_fee', 'collections_12_mths_ex_med', 'last_credit_pull_d', 'last_pymnt_amnt', 'last_pymnt_d',\n",
    " 'mths_since_last_major_derog', 'next_pymnt_d', 'out_prncp', 'out_prncp_inv', 'recoveries', 'total_pymnt', 'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee',\n",
    " 'total_rec_prncp', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date',\n",
    " 'payment_plan_start_date', 'hardship_length', 'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount',\n",
    " 'hardship_last_payment_amount', 'debt_settlement_flag', 'last_fico_range_high', 'last_fico_range_low']\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "# For model\n",
    "learning_rate = 0.01\n",
    "\n",
    "# For training\n",
    "#n_epochs = 10000\n",
    "\n",
    "# For CV\n",
    "cv = 10\n",
    "\n",
    "# For SSAE\n",
    "n_epochs_ssae = 10000\n",
    "latent_size = 8\n",
    "\n",
    "# 테스트 런일 경우, 에포크 수를 줄임\n",
    "if is_test:\n",
    "    n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATZ3E7NEKApJ",
    "outputId": "653a45fc-17ab-47e4-b4ae-5360f61fcec2"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed 설정\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed) # 파이썬 표준 라이브러리\n",
    "    np.random.seed(seed) # numpy의 random 모듈에서 사용하는 seed\n",
    "    torch.manual_seed(seed) # pytorch에서 사용하는 seed\n",
    "    if torch.cuda.is_available(): # GPU에서 실행되는 PyTorch 연산의 무작위성을 제어\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# seed 값 설정\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ans_U0Bqoyh-"
   },
   "source": [
    "## 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn3BhYVohcPI"
   },
   "outputs": [],
   "source": [
    "# 모든 행이 화면에 표시되도록 설정합니다.\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 파일 로드\n",
    "file_path = 'LC_Data_Cleaned_0829.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 테스트 런일 경우, 데이터 크기 줄이기\n",
    "if is_test:\n",
    "    df = df.sample(frac=0.01, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0DpvFHJoyh-"
   },
   "source": [
    "## 데이터 구조 훑어 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "eIAt4-Y_oyh-",
    "outputId": "537e79ec-50cb-4a7f-d6f3-e25d5f9b6667"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxjdTP4gKApM"
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "L42IMY2hKSEO",
    "outputId": "c428f136-6ace-4ed8-f5fe-b90c4949fd50"
   },
   "outputs": [],
   "source": [
    "# 기존 df 에 있는 칼럼 중에서, drop_list 및 post_list의 칼럼 제거\n",
    "drop_list = list(set(df.columns) & set(drop_list + post_list))\n",
    "\n",
    "# 불필요한 변수 Drop\n",
    "df = df.drop(columns = drop_list)\n",
    "\n",
    "# 결측치 처리\n",
    "df = df.fillna(0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[target])\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWIX34Exoyim"
   },
   "source": [
    "# 모델 선택과 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTRHX9M5oyiA"
   },
   "source": [
    "## 테스트 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrhJ2zzMKApN"
   },
   "outputs": [],
   "source": [
    "# Torch 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if not is_test else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [],
   "source": [
    "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 언더샘플링 객체 생성\n",
    "print(f\"Original y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "# 트레인 데이터셋에 대해 언더샘플링 수행\n",
    "X_train, y_train = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"Resampled y_train distribution: {y_train.value_counts()}\")\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 변수 선정\n",
    "num_list = list(set(X_train.columns) ^ set(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder 생성 및 학습 데이터에 적합\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oneHotEncoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "oneHotEncoder.fit(X_train[cat_list])\n",
    "\n",
    "# 학습 데이터에 인코딩 적용\n",
    "X_train_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_train[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# 테스트 데이터에 인코딩 적용\n",
    "X_test_encoded = pd.DataFrame(\n",
    "    oneHotEncoder.transform(X_test[cat_list]), \n",
    "    columns=oneHotEncoder.get_feature_names_out(cat_list), \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# 원래 데이터프레임과 병합\n",
    "X_train = X_train.drop(cat_list, axis=1).join(X_train_encoded)\n",
    "X_test = X_test.drop(cat_list, axis=1).join(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLnzlvjchMLK"
   },
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 수치형 변수 찾기\n",
    "X_train[num_list] = scaler.fit_transform(X_train[num_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test[num_list] = scaler.transform(X_test[num_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvlhFSTgKApN"
   },
   "source": [
    "## SSAE 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhT0RT1sKApN"
   },
   "outputs": [],
   "source": [
    "# SSAE 모델 정의\n",
    "class DenoisingSSAE(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DenoisingSSAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  # input_dim이 실제 데이터의 feature 수와 일치해야 함\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)  # output_dim도 input_dim과 일치해야 함\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "# Noise 추가 함수\n",
    "def add_noise(data, noise_factor=0.2):\n",
    "    noise = noise_factor * np.random.randn(*data.shape)\n",
    "    noisy_data = data + noise\n",
    "    noisy_data = np.clip(noisy_data, 0., 1.)\n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding 목표 설정\n",
    "encoding_target = list(set(X_train.columns) ^ set(feature_list))\n",
    "\n",
    "# 노이즈가 추가\n",
    "X_train_noisy_np = add_noise(X_train[encoding_target]).to_numpy()\n",
    "X_train_np = X_train[encoding_target].to_numpy()\n",
    "          \n",
    "input_dim = X_train_np.shape[1]  # X_train의 feature 수\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 생성 및 학습 설정\n",
    "ssae_model = DenoisingSSAE(input_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ssae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(n_epochs_ssae):\n",
    "    ssae_model.train()\n",
    "    inputs = torch.FloatTensor(X_train_noisy_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "    targets = torch.FloatTensor(X_train_np).to(device)  # NumPy 배열 -> PyTorch 텐서\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = ssae_model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs_ssae}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNxDAlFBKApN"
   },
   "source": [
    "## 모델 평가함수 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXpRxDP8W6Zo"
   },
   "outputs": [],
   "source": [
    "# 평가함수 정의\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "        \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81YDhprUKApO"
   },
   "outputs": [],
   "source": [
    "# Shapley Value 계산, 시각화 함수 정의\n",
    "def evaluate_models_shap1(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    masker = shap.maskers.Independent(X_train_encoded_ssae)\n",
    "    explainer = shap.LinearExplainer(model, masker=masker)\n",
    "    shap_values = explainer(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n",
    "\n",
    "def evaluate_models_shap2(model, X_test_encoded_ssae):\n",
    "    # TreeExplainer를 사용한 Shapley Value 계산\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6GIT1bg23Ny"
   },
   "outputs": [],
   "source": [
    "# ShapleyValue 계산, 시각화함수 정의 (TabNet 전용)\n",
    "def evaluate_models_shap_tabnet(model, X_train_encoded_ssae, X_test_encoded_ssae):\n",
    "    # DeepExplainer를 사용한 Shapley Value 계산 (TabNet 전용)\n",
    "    explainer = DeepExplainer(model, X_train_encoded_ssae)\n",
    "    shap_values = explainer.shap_values(X_test_encoded_ssae)\n",
    "\n",
    "    # summary_plot을 사용한 Shapley Value 시각화\n",
    "    shap.summary_plot(shap_values, X_test_encoded_ssae, feature_names=[f\"Encoded_{i}\" for i in range(X_test_encoded_ssae.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증(CV)을 위한 함수 정의\n",
    "def cross_validate_model(model, X, y, cv=cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-validation scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_list = [f'e{i}' for i in range(latent_size)]\n",
    "X_train_encoded = pd.concat(\n",
    "                    [X_train[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_train[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_train.index, columns=encoded_list)],\n",
    "                    axis=1)\n",
    "\n",
    "X_test_encoded = pd.concat(\n",
    "                    [X_test[feature_list],\n",
    "                    pd.DataFrame(ssae_model.encoder(torch.FloatTensor(X_test[encoding_target].to_numpy()).to(device)).detach().numpy(),\n",
    "                                 index=X_test.index, columns=encoded_list)],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화 (훈련 데이터에 fit_transform, 테스트 데이터에 transform)\n",
    "scalerenc = StandardScaler()\n",
    "X_train_encoded[encoded_list] = scalerenc.fit_transform(X_train_encoded[encoded_list])  # (TODO): 더미 변수 표준화에서 제외해야함\n",
    "X_test_encoded[encoded_list] = scalerenc.transform(X_test_encoded[encoded_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression  # 수정된 부분\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# 모든 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No further splits with positive gain\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Stopped training because there are no more leaves\")\n",
    "\n",
    "# Train and evaluate the model with a given parameter set\n",
    "def evaluate_model(model_name, model, param_set):\n",
    "    try:\n",
    "        model.set_params(**param_set)\n",
    "        model.fit(X_train_encoded, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracy, precision, recall, f1, confusion_matrix(y_test, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name} with params: {param_set}. Error: {str(e)}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Define parameter grids for each model\n",
    "logistic_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 3),  # Regularization strength with 10 values\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs'],  # Fast solver for logistic regression with L2 regularization\n",
    "    'max_iter': [1000]  # Very high iteration count\n",
    "}\n",
    "\n",
    "# Common parameter grid for XGBoost, LightGBM, and RandomForest\n",
    "common_param_grid = {\n",
    "    'learning_rate': np.logspace(-4, 0, 3),  # Learning rate with 10 values\n",
    "    'reg_alpha': np.logspace(-3, 1, 3),  # L1 regularization with 10 values\n",
    "    'reg_lambda': np.logspace(-3, 1, 3),  # L2 regularization with 10 values\n",
    "    'n_estimators': [1000]  # Very high number of trees\n",
    "}\n",
    "\n",
    "svm_param_grid = {\n",
    "    'C': np.logspace(-3, 3, 3),  # Regularization strength with 10 values\n",
    "    'kernel': ['rbf'],  # RBF kernel is commonly used\n",
    "    'gamma': ['scale'],  # Standard option for gamma\n",
    "    'max_iter': [1000]  # Very high iteration count for convergence\n",
    "}\n",
    "\n",
    "# Evaluate models with parameter grids\n",
    "logistic_model = LogisticRegression(n_jobs=-1)\n",
    "xgb_model = xgb.XGBClassifier(n_jobs=-1, verbosity=0)  # Disable XGBoost output\n",
    "lgb_model = lgb.LGBMClassifier(n_jobs=-1, verbose=-1)  # Disable LightGBM output\n",
    "rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "svm_model = SVC()\n",
    "\n",
    "# Dictionary to store best results for each model\n",
    "best_results = {}\n",
    "\n",
    "# Logistic Regression with F1 optimization\n",
    "for params in ParameterGrid(logistic_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('Logistic Regression', logistic_model, params)\n",
    "    if accuracy is not None and ('Logistic Regression' not in best_results or f1 > best_results['Logistic Regression']['f1_score']):\n",
    "        best_results['Logistic Regression'] = {\n",
    "            'model': logistic_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest Logistic Regression model:\")\n",
    "if 'Logistic Regression' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['Logistic Regression']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['Logistic Regression']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['Logistic Regression']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['Logistic Regression']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['Logistic Regression']['params']}\\n\")\n",
    "\n",
    "# XGBoost with F1 optimization\n",
    "# for params in ParameterGrid(common_param_grid):\n",
    "#     accuracy, precision, recall, f1, cm = evaluate_model('XGBoost Classifier', xgb_model, params)\n",
    "#     if accuracy is not None and ('XGBoost Classifier' not in best_results or f1 > best_results['XGBoost Classifier']['f1_score']):\n",
    "#         best_results['XGBoost Classifier'] = {\n",
    "#             'model': xgb_model,\n",
    "#             'params': params,\n",
    "#             'accuracy': accuracy,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1_score': f1,\n",
    "#             'confusion_matrix': cm\n",
    "#         }\n",
    "\n",
    "# print(f\"\\nBest XGBoost model:\")\n",
    "# if 'XGBoost Classifier' in best_results:\n",
    "#     print(f\"  F1 Score: {best_results['XGBoost Classifier']['f1_score']:.4f}\")\n",
    "#     print(f\"  Accuracy: {best_results['XGBoost Classifier']['accuracy']:.4f}\")\n",
    "#     print(f\"  Precision: {best_results['XGBoost Classifier']['precision']:.4f}\")\n",
    "#     print(f\"  Recall: {best_results['XGBoost Classifier']['recall']:.4f}\")\n",
    "#     print(f\"  Best Parameters: {best_results['XGBoost Classifier']['params']}\\n\")\n",
    "\n",
    "# LightGBM with F1 optimization\n",
    "for params in ParameterGrid(common_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('LightGBM Classifier', lgb_model, params)\n",
    "    if accuracy is not None and ('LightGBM Classifier' not in best_results or f1 > best_results['LightGBM Classifier']['f1_score']):\n",
    "        best_results['LightGBM Classifier'] = {\n",
    "            'model': lgb_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest LightGBM model:\")\n",
    "if 'LightGBM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['LightGBM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['LightGBM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['LightGBM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['LightGBM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['LightGBM Classifier']['params']}\\n\")\n",
    "\n",
    "RandomForest with F1 optimization\n",
    "for params in ParameterGrid(common_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('RandomForest Classifier', rf_model, params)\n",
    "    if accuracy is not None and ('RandomForest Classifier' not in best_results or f1 > best_results['RandomForest Classifier']['f1_score']):\n",
    "        best_results['RandomForest Classifier'] = {\n",
    "            'model': rf_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest RandomForest model:\")\n",
    "if 'RandomForest Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['RandomForest Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['RandomForest Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['RandomForest Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['RandomForest Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['RandomForest Classifier']['params']}\\n\")\n",
    "\n",
    "# SVM with F1 optimization\n",
    "for params in ParameterGrid(svm_param_grid):\n",
    "    accuracy, precision, recall, f1, cm = evaluate_model('SVM Classifier', svm_model, params)\n",
    "    if accuracy is not None and ('SVM Classifier' not in best_results or f1 > best_results['SVM Classifier']['f1_score']):\n",
    "        best_results['SVM Classifier'] = {\n",
    "            'model': svm_model,\n",
    "            'params': params,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "print(f\"\\nBest SVM model:\")\n",
    "if 'SVM Classifier' in best_results:\n",
    "    print(f\"  F1 Score: {best_results['SVM Classifier']['f1_score']:.4f}\")\n",
    "    print(f\"  Accuracy: {best_results['SVM Classifier']['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {best_results['SVM Classifier']['precision']:.4f}\")\n",
    "    print(f\"  Recall: {best_results['SVM Classifier']['recall']:.4f}\")\n",
    "    print(f\"  Best Parameters: {best_results['SVM Classifier']['params']}\\n\")\n",
    "\n",
    "# Find the best overall model based on F1 score\n",
    "best_model_name = max(best_results, key=lambda x: best_results[x]['f1_score'])\n",
    "best_model_results = best_results[best_model_name]\n",
    "\n",
    "# Print the best overall model\n",
    "print(f\"\\nBest overall model: {best_model_name}\")\n",
    "print(f\"  F1 Score: {best_model_results['f1_score']:.4f}\")\n",
    "print(f\"  Accuracy: {best_model_results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {best_model_results['precision']:.4f}\")\n",
    "print(f\"  Recall: {best_model_results['recall']:.4f}\")\n",
    "print(f\"  Best Parameters: {best_model_results['params']}\")\n",
    "\n",
    "# Plot confusion matrix for the best overall model\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=best_model_results['confusion_matrix'])\n",
    "disp.plot()\n",
    "plt.title(f\"Confusion Matrix for {best_model_name}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_encoded), len(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
